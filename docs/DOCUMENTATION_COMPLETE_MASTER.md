# ğŸ“š DOCUMENTATION COMPLÃˆTE MASTER - Cluster HPC
## Guide Exhaustif : Technologies, Logiciels, Architecture et Choix de Conception

**Classification**: Documentation Technique ComplÃ¨te  
**Public**: Tous Niveaux (DÃ©butants Ã  Experts)  
**Version**: 1.0  
**Date**: 2024

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Introduction et Vue d'Ensemble](#1-introduction-et-vue-densemble)
2. [Architecture ComplÃ¨te du Cluster](#2-architecture-complÃ¨te-du-cluster)
3. [Technologies de Base - Explications DÃ©taillÃ©es](#3-technologies-de-base)
4. [Logiciels et Applications - Guide Complet](#4-logiciels-et-applications)
5. [Choix de Conception et Justifications](#5-choix-de-conception)
6. [Structure du Projet - Organisation](#6-structure-du-projet)
7. [Comment Tout Fonctionne Ensemble](#7-fonctionnement-global)
8. [Guide d'Utilisation de Chaque Logiciel](#8-guide-utilisation-logiciels)
9. [Maintenance et OpÃ©rations](#9-maintenance)
10. [Monitoring et Dashboards](#10-monitoring)

---

## 1. Introduction et Vue d'Ensemble

### 1.1 Qu'est-ce qu'un Cluster HPC ?

Un **Cluster HPC (High-Performance Computing)** est un ensemble de serveurs interconnectÃ©s qui travaillent ensemble pour rÃ©soudre des problÃ¨mes de calcul complexes nÃ©cessitant une grande puissance de traitement.

**CaractÃ©ristiques** :
- **ParallÃ©lisme** : Calculs distribuÃ©s sur plusieurs nÅ“uds
- **Haute Performance** : OptimisÃ© pour calculs intensifs
- **ScalabilitÃ©** : Peut s'Ã©tendre Ã  des milliers de nÅ“uds
- **FiabilitÃ©** : Redondance et haute disponibilitÃ©

### 1.2 Objectif de ce Projet

**Mission** : CrÃ©er un cluster HPC complet, **100% open-source**, portable via Docker, prÃªt pour dÃ©ploiement sur **SUSE 15 SP4**, avec toutes les fonctionnalitÃ©s nÃ©cessaires pour un environnement de production.

**Contraintes** :
- âœ… **100% Open-Source** : Aucune licence commerciale
- âœ… **Portable** : Docker, fonctionne partout
- âœ… **Production-Ready** : SÃ©curitÃ©, monitoring, maintenance
- âœ… **Complet** : Tous les composants nÃ©cessaires

---

## 2. Architecture ComplÃ¨te du Cluster

### 2.1 Architecture GÃ©nÃ©rale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COUCHE MANAGEMENT                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  LDAP    â”‚  â”‚ Kerberos â”‚  â”‚Prometheusâ”‚  â”‚  Grafana â”‚         â”‚
â”‚  â”‚ (389DS)  â”‚  â”‚   KDC    â”‚  â”‚          â”‚  â”‚          â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Slurm  â”‚  â”‚  BeeGFS  â”‚  â”‚  Nexus   â”‚  â”‚  X2Go    â”‚         â”‚
â”‚  â”‚  CTLD   â”‚  â”‚   MGMtd  â”‚  â”‚ (PyPI)   â”‚  â”‚ (Remote) â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Compute-01  â”‚  â”‚  Compute-02  â”‚  â”‚  Compute-06  â”‚
â”‚  (SlurmD)    â”‚  â”‚  (SlurmD)    â”‚  â”‚  (SlurmD)    â”‚
â”‚  + BeeGFS    â”‚  â”‚  + BeeGFS    â”‚  â”‚  + BeeGFS    â”‚
â”‚  + Spack     â”‚  â”‚  + Spack     â”‚  â”‚  + Spack     â”‚
â”‚  + Apps      â”‚  â”‚  + Apps      â”‚  â”‚  + Apps      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Composants Principaux

#### NÅ“uds Frontaux (2)
- **frontal-01** : Services principaux (LDAP, Kerberos, SlurmCTLD, BeeGFS MGMtd)
- **frontal-02** : Services secondaires (rÃ©plication, haute disponibilitÃ©)

#### NÅ“uds de Calcul (6)
- **compute-01 Ã  compute-06** : ExÃ©cution des jobs HPC

#### RÃ©seaux (4)
- **Management (172.20.0.0/24)** : SSH, LDAP, Kerberos, Slurm
- **Cluster (10.0.0.0/24)** : Communication MPI
- **Storage (10.10.10.0/24)** : BeeGFS, stockage haute performance
- **Monitoring (192.168.200.0/24)** : Prometheus, Grafana

---

## 3. Technologies de Base - Explications DÃ©taillÃ©es

### 3.1 Docker et Docker Compose

#### Qu'est-ce que c'est ?

**Docker** est une plateforme de conteneurisation qui permet d'empaqueter une application et ses dÃ©pendances dans un conteneur isolÃ©.

**Docker Compose** est un outil pour dÃ©finir et orchestrer plusieurs conteneurs Docker.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **PortabilitÃ©** : Fonctionne identiquement sur n'importe quel systÃ¨me
- âœ… **Isolation** : Chaque service dans son conteneur
- âœ… **ReproductibilitÃ©** : MÃªme environnement partout
- âœ… **FacilitÃ©** : Un seul fichier `docker-compose.yml` pour tout
- âœ… **Air-gapped** : Peut fonctionner sans Internet (images exportÃ©es)

**Alternatives considÃ©rÃ©es** :
- âŒ **VMware/KVM** : Trop lourd, nÃ©cessite hyperviseur
- âŒ **LXC** : Moins standardisÃ©, moins d'outils
- âŒ **Installation native** : Difficile Ã  reproduire, dÃ©pendances complexes

**Pourquoi Docker a Ã©tÃ© choisi** :
- Standard de l'industrie
- Large Ã©cosystÃ¨me
- Facile Ã  apprendre
- Compatible SUSE 15 SP4

#### Comment Ã§a fonctionne ?

```
Docker Engine
    â”‚
    â”œâ”€â–º Conteneur frontal-01
    â”‚   â”œâ”€â–º openSUSE Leap 15.4
    â”‚   â”œâ”€â–º SlurmCTLD
    â”‚   â”œâ”€â–º LDAP
    â”‚   â””â”€â–º Services systÃ¨me
    â”‚
    â”œâ”€â–º Conteneur compute-01
    â”‚   â”œâ”€â–º openSUSE Leap 15.4
    â”‚   â”œâ”€â–º SlurmD
    â”‚   â”œâ”€â–º BeeGFS Client
    â”‚   â””â”€â–º Applications HPC
    â”‚
    â””â”€â–º Conteneur Prometheus
        â”œâ”€â–º Image Prometheus
        â””â”€â–º Configuration
```

**Fichiers** :
- `docker/frontal/Dockerfile` : Image pour nÅ“uds frontaux
- `docker/client/Dockerfile` : Image pour nÅ“uds de calcul
- `docker/docker-compose-opensource.yml` : Orchestration complÃ¨te

**Utilisation** :
```bash
# Build
cd docker/
docker-compose -f docker-compose-opensource.yml build

# DÃ©marrage
docker-compose -f docker-compose-opensource.yml up -d

# ArrÃªt
docker-compose -f docker-compose-opensource.yml down
```

---

### 3.2 Slurm (Workload Manager)

#### Qu'est-ce que c'est ?

**Slurm** (Simple Linux Utility for Resource Management) est un gestionnaire de jobs et de ressources pour clusters HPC.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Standard HPC** : UtilisÃ© par 60%+ des clusters mondiaux
- âœ… **Gestion ressources** : CPU, mÃ©moire, GPU, partitions
- âœ… **File d'attente intelligente** : PrioritÃ©, backfill, preemption
- âœ… **Multi-utilisateurs** : Partage Ã©quitable des ressources
- âœ… **Open-source** : Gratuit, communautÃ© active

**Alternatives considÃ©rÃ©es** :
- âŒ **PBS/Torque** : Moins moderne, moins de fonctionnalitÃ©s
- âŒ **LSF** : Commercial, licence coÃ»teuse
- âŒ **SGE** : Moins maintenu, communautÃ© rÃ©duite

**Pourquoi Slurm a Ã©tÃ© choisi** :
- Standard de facto pour HPC
- TrÃ¨s bien documentÃ©
- Performant et fiable
- Supporte toutes les fonctionnalitÃ©s nÃ©cessaires

#### Comment Ã§a fonctionne ?

```
Utilisateur soumet un job
    â”‚
    â”œâ”€â–º sbatch job.sh
    â”‚
    â–¼
SlurmCTLD (Controller)
    â”‚
    â”œâ”€â–º VÃ©rifie ressources disponibles
    â”œâ”€â–º Place dans file d'attente
    â”œâ”€â–º Calcule prioritÃ©
    â””â”€â–º Lance sur nÅ“uds disponibles
        â”‚
        â–¼
    SlurmD (Daemon sur chaque nÅ“ud)
        â”‚
        â”œâ”€â–º Alloue ressources (CPU, RAM)
        â”œâ”€â–º Lance le job
        â””â”€â–º Collecte mÃ©triques
```

**Composants** :
- **SlurmCTLD** : ContrÃ´leur (frontal-01)
- **SlurmDBD** : Base de donnÃ©es des jobs
- **SlurmD** : Daemon sur chaque nÅ“ud de calcul

**Fichiers de configuration** :
- `configs/slurm/slurm.conf` : Configuration principale
- `configs/slurm/cgroup.conf` : Gestion des ressources (cgroups)

**Utilisation** :
```bash
# Soumettre un job
sbatch job.sh

# Voir les jobs
squeue

# Voir les nÅ“uds
sinfo

# Annuler un job
scancel JOBID
```

---

### 3.3 BeeGFS (SystÃ¨me de Fichiers ParallÃ¨le)

#### Qu'est-ce que c'est ?

**BeeGFS** (formerly FhGFS) est un systÃ¨me de fichiers parallÃ¨le open-source optimisÃ© pour HPC.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Performance** : TrÃ¨s rapide pour HPC (I/O parallÃ¨le)
- âœ… **ScalabilitÃ©** : Supporte des milliers de nÅ“uds
- âœ… **Open-source** : Gratuit, pas de licence
- âœ… **FacilitÃ©** : Plus simple que Lustre
- âœ… **FlexibilitÃ©** : Peut dÃ©marrer petit et grandir

**Alternatives considÃ©rÃ©es** :
- âŒ **GPFS (IBM Spectrum Scale)** : Commercial, licence coÃ»teuse
- âŒ **Lustre** : Plus complexe, nÃ©cessite plus de ressources
- âŒ **GlusterFS** : Moins performant pour HPC
- âŒ **NFS** : Trop lent pour HPC

**Pourquoi BeeGFS a Ã©tÃ© choisi** :
- Meilleur compromis performance/facilitÃ©
- Open-source pur
- Bien documentÃ©
- CommunautÃ© active

#### Comment Ã§a fonctionne ?

```
Application (sur compute-01)
    â”‚
    â”œâ”€â–º Ã‰criture fichier
    â”‚
    â–¼
BeeGFS Client
    â”‚
    â”œâ”€â–º Contacte MGMtd (mÃ©tadonnÃ©es)
    â”‚   â””â”€â–º Obtient localisation
    â”‚
    â”œâ”€â–º Ã‰crit directement sur Storage Targets
    â”‚   â”œâ”€â–º Target 1 (frontal-01)
    â”‚   â”œâ”€â–º Target 2 (frontal-02)
    â”‚   â””â”€â–º Stripe parallÃ¨le
    â”‚
    â””â”€â–º Performance Ã©levÃ©e (I/O parallÃ¨le)
```

**Composants** :
- **MGMtd** : Gestionnaire de mÃ©tadonnÃ©es (frontal-01)
- **Storage Targets** : Stockage des donnÃ©es (frontal-01, frontal-02)
- **Client** : Sur chaque nÅ“ud (compute-01 Ã  compute-06)

**Installation** :
```bash
./scripts/storage/install-beegfs.sh
```

**Utilisation** :
```bash
# Monter BeeGFS
mount -t beegfs /mnt/beegfs

# VÃ©rifier
df -h /mnt/beegfs
```

---

### 3.4 LDAP (389 Directory Server)

#### Qu'est-ce que c'est ?

**LDAP** (Lightweight Directory Access Protocol) est un protocole d'accÃ¨s Ã  un annuaire distribuÃ©. **389 Directory Server** est l'implÃ©mentation open-source.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Authentification centralisÃ©e** : Un seul point d'authentification
- âœ… **Gestion utilisateurs** : CrÃ©ation/modification centralisÃ©e
- âœ… **Standards** : Protocole standardisÃ© (RFC 4510)
- âœ… **IntÃ©gration** : Compatible avec tout (SSH, Slurm, etc.)

**Alternatives considÃ©rÃ©es** :
- âŒ **Fichiers locaux (/etc/passwd)** : Non scalable, pas centralisÃ©
- âŒ **Active Directory** : Commercial, Windows-centric
- âŒ **FreeIPA** : Plus complexe, peut Ãªtre overkill

**Pourquoi LDAP a Ã©tÃ© choisi** :
- Standard de l'industrie
- Simple et efficace
- Compatible avec tout
- Open-source

#### Comment Ã§a fonctionne ?

```
Client (SSH, Slurm, etc.)
    â”‚
    â”‚ RequÃªte LDAP (port 389)
    â”‚ "VÃ©rifier utilisateur jdoe / password"
    â–¼
389 Directory Server
    â”‚
    â”‚ Recherche dans base LDAP
    â”‚ dc=cluster,dc=local
    â”‚   â””â”€â–º ou=users
    â”‚       â””â”€â–º uid=jdoe
    â”‚
    â”œâ”€â–º VÃ©rifie password (hash)
    â”‚
    â””â”€â–º Retourne : OK / NOK
```

**Structure LDAP** :
```
dc=cluster,dc=local
â”œâ”€â”€ ou=users
â”‚   â”œâ”€â”€ uid=jdoe,ou=users,dc=cluster,dc=local
â”‚   â”‚   â”œâ”€â”€ cn: John Doe
â”‚   â”‚   â”œâ”€â”€ userPassword: {SSHA}...
â”‚   â”‚   â”œâ”€â”€ uidNumber: 1001
â”‚   â”‚   â””â”€â”€ homeDirectory: /home/jdoe
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ou=groups
â”‚   â”œâ”€â”€ cn=hpc-users,ou=groups,dc=cluster,dc=local
â”‚   â””â”€â”€ cn=admins,ou=groups,dc=cluster,dc=local
â””â”€â”€ ou=computers
    â””â”€â”€ cn=node-01,ou=computers,dc=cluster,dc=local
```

**Installation** :
```bash
./scripts/install-ldap-kerberos.sh
```

**Utilisation** :
```bash
# CrÃ©er un utilisateur
ldapadd -x -D "cn=Directory Manager" -w "password" <<EOF
dn: uid=jdoe,ou=users,dc=cluster,dc=local
objectClass: inetOrgPerson
objectClass: posixAccount
uid: jdoe
cn: John Doe
userPassword: {SSHA}encrypted
uidNumber: 1001
gidNumber: 1001
homeDirectory: /home/jdoe
EOF

# Rechercher
ldapsearch -x -b "dc=cluster,dc=local" "(uid=jdoe)"
```

---

### 3.5 Kerberos

#### Qu'est-ce que c'est ?

**Kerberos** est un protocole d'authentification rÃ©seau sÃ©curisÃ© basÃ© sur des tickets cryptographiques.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **SÃ©curitÃ©** : Pas de mots de passe en clair sur le rÃ©seau
- âœ… **SSO** : Authentification unique (Single Sign-On)
- âœ… **Standards** : Protocole standardisÃ© (RFC 4120)
- âœ… **IntÃ©gration** : Compatible avec LDAP, SSH, NFS

**Alternatives considÃ©rÃ©es** :
- âŒ **Mots de passe en clair** : Non sÃ©curisÃ©
- âŒ **Certificats uniquement** : Plus complexe Ã  gÃ©rer
- âŒ **OAuth/SAML** : Overkill pour HPC

**Pourquoi Kerberos a Ã©tÃ© choisi** :
- Standard pour environnements enterprise
- SÃ©curisÃ© par dÃ©faut
- SSO intÃ©grÃ©
- Compatible avec tout

#### Comment Ã§a fonctionne ?

```
1. Client demande un ticket
   Client â”€â”€â–º KDC (Key Distribution Center)
              Port 88
   
2. KDC vÃ©rifie l'identitÃ© (via LDAP)
   KDC â”€â”€â–º LDAP (vÃ©rification)
   
3. KDC Ã©met un TGT (Ticket Granting Ticket)
   KDC â”€â”€â–º Client (TGT chiffrÃ©)
   
4. Client utilise le TGT pour obtenir un service ticket
   Client â”€â”€â–º KDC (avec TGT)
   KDC â”€â”€â–º Client (Service Ticket)
   
5. Client prÃ©sente le service ticket au service
   Client â”€â”€â–º SSH/NFS/etc. (avec Service Ticket)
   Service â”€â”€â–º OK (authentifiÃ©)
```

**Composants** :
- **KDC** : Key Distribution Center (frontal-01)
- **Realm** : Domaine Kerberos (CLUSTER.LOCAL)
- **Principal** : IdentitÃ© (jdoe@CLUSTER.LOCAL)
- **Ticket** : Token d'authentification temporaire (24h par dÃ©faut)

**Installation** :
```bash
./scripts/install-ldap-kerberos.sh
```

**Utilisation** :
```bash
# Obtenir un ticket
kinit jdoe@CLUSTER.LOCAL
# Entrer le mot de passe

# VÃ©rifier
klist

# SSH sans mot de passe (si ticket valide)
ssh jdoe@node-01
```

---

### 3.6 Prometheus (Monitoring)

#### Qu'est-ce que c'est ?

**Prometheus** est un systÃ¨me de monitoring et d'alerting open-source.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Pull-based** : Collecte active des mÃ©triques
- âœ… **Time-series** : Base de donnÃ©es optimisÃ©e pour sÃ©ries temporelles
- âœ… **PromQL** : Langage de requÃªte puissant
- âœ… **Alerting** : SystÃ¨me d'alertes intÃ©grÃ©
- âœ… **Open-source** : Gratuit, communautÃ© Ã©norme

**Alternatives considÃ©rÃ©es** :
- âŒ **Nagios** : Plus ancien, moins moderne
- âŒ **Zabbix** : Plus complexe, interface moins intuitive
- âŒ **Datadog/New Relic** : Commercial, coÃ»teux

**Pourquoi Prometheus a Ã©tÃ© choisi** :
- Standard de facto pour monitoring moderne
- TrÃ¨s performant
- Excellent pour mÃ©triques time-series
- Ã‰cosystÃ¨me Ã©norme (exporters)

#### Comment Ã§a fonctionne ?

```
Prometheus Server
    â”‚
    â”œâ”€â–º Scrape (toutes les 15s)
    â”‚   â”œâ”€â–º Node Exporter (port 9100)
    â”‚   â”‚   â””â”€â–º MÃ©triques systÃ¨me (CPU, RAM, Disk)
    â”‚   â”‚
    â”‚   â”œâ”€â–º Telegraf (port 9273)
    â”‚   â”‚   â””â”€â–º MÃ©triques applicatives
    â”‚   â”‚
    â”‚   â””â”€â–º Slurm Exporter
    â”‚       â””â”€â–º MÃ©triques Slurm (jobs, partitions)
    â”‚
    â”œâ”€â–º Stockage (TSDB)
    â”‚   â””â”€â–º RÃ©tention 30 jours
    â”‚
    â””â”€â–º Alerting
        â””â”€â–º RÃ¨gles d'alerte (CPU > 80%, etc.)
```

**Configuration** :
- `configs/prometheus/prometheus.yml` : Configuration scraping
- `configs/prometheus/alerts.yml` : RÃ¨gles d'alertes

**Utilisation** :
```bash
# AccÃ¨s web
http://localhost:9090

# RequÃªte PromQL
up{job="frontal-01-node"}
node_cpu_seconds_total{mode="idle"}
```

---

### 3.7 Grafana (Visualisation)

#### Qu'est-ce que c'est ?

**Grafana** est une plateforme de visualisation et d'analyse de mÃ©triques.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Dashboards** : Visualisation graphique des mÃ©triques
- âœ… **Multi-sources** : Prometheus, InfluxDB, etc.
- âœ… **Alerting** : Alertes visuelles
- âœ… **Templates** : Dashboards prÃ©-configurÃ©s
- âœ… **Open-source** : Gratuit

**Alternatives considÃ©rÃ©es** :
- âŒ **Kibana** : Principalement pour logs (ELK)
- âŒ **Custom HTML** : Trop de travail
- âŒ **Tableaux de bord propriÃ©taires** : CoÃ»teux

**Pourquoi Grafana a Ã©tÃ© choisi** :
- Standard pour visualisation mÃ©triques
- Interface intuitive
- Large communautÃ© de dashboards
- IntÃ©gration native avec Prometheus

#### Comment Ã§a fonctionne ?

```
Grafana Server
    â”‚
    â”œâ”€â–º Datasource : Prometheus
    â”‚   â””â”€â–º http://172.20.0.10:9090
    â”‚
    â”œâ”€â–º Dashboards
    â”‚   â”œâ”€â–º HPC Cluster Overview
    â”‚   â”œâ”€â–º CPU/Memory by Node
    â”‚   â”œâ”€â–º Network I/O
    â”‚   â”œâ”€â–º Slurm Jobs
    â”‚   â””â”€â–º 50+ autres dashboards
    â”‚
    â””â”€â–º Alerting
        â””â”€â–º Notifications (email, webhook)
```

**Dashboards** :
- `grafana-dashboards/` : 54+ dashboards JSON
- Auto-provisioning via `configs/grafana/provisioning/`

**Utilisation** :
```bash
# AccÃ¨s web
http://localhost:3000
# Login: admin / admin

# Dashboards disponibles
# - HPC Cluster Overview
# - CPU/Memory by Node
# - Network I/O
# - Slurm Jobs
# - Et 50+ autres...
```

---

### 3.8 Telegraf (Collecte de MÃ©triques)

#### Qu'est-ce que c'est ?

**Telegraf** est un agent de collecte de mÃ©triques Ã©crit en Go.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **LÃ©ger** : Faible consommation ressources
- âœ… **Plugins** : 200+ plugins disponibles
- âœ… **Flexible** : Inputs et outputs multiples
- âœ… **Prometheus** : Exporte en format Prometheus

**Alternatives considÃ©rÃ©es** :
- âŒ **Collectd** : Moins moderne, moins de plugins
- âŒ **StatsD** : Uniquement UDP, moins flexible
- âŒ **Custom scripts** : Trop de maintenance

**Pourquoi Telegraf a Ã©tÃ© choisi** :
- Standard moderne
- TrÃ¨s performant
- Large Ã©cosystÃ¨me de plugins
- Export Prometheus natif

#### Comment Ã§a fonctionne ?

```
Telegraf Agent (sur chaque nÅ“ud)
    â”‚
    â”œâ”€â–º Inputs (collecte)
    â”‚   â”œâ”€â–º CPU
    â”‚   â”œâ”€â–º Memory
    â”‚   â”œâ”€â–º Disk
    â”‚   â”œâ”€â–º Network
    â”‚   â””â”€â–º Processes
    â”‚
    â”œâ”€â–º Processors (optionnel)
    â”‚   â””â”€â–º Tags, transformations
    â”‚
    â””â”€â–º Outputs (exposition)
        â””â”€â–º Prometheus Client (port 9273)
            â””â”€â–º ScrapÃ© par Prometheus
```

**Configuration** :
- `configs/telegraf/telegraf-frontal.conf` : Pour nÅ“uds frontaux
- `configs/telegraf/telegraf-slave.conf` : Pour nÅ“uds de calcul

**Utilisation** :
```bash
# VÃ©rifier mÃ©triques
curl http://172.20.0.101:9273/metrics

# Logs
journalctl -u telegraf -f
```

---

## 4. Logiciels et Applications - Guide Complet

### 4.1 Applications Scientifiques

#### GROMACS

**Qu'est-ce que c'est ?**
- Simulation molÃ©culaire
- Dynamique molÃ©culaire
- UtilisÃ© pour protÃ©ines, membranes, etc.

**Pourquoi l'utiliser ?**
- âœ… Standard pour simulation molÃ©culaire
- âœ… TrÃ¨s performant (optimisÃ© MPI)
- âœ… Open-source
- âœ… Large communautÃ©

**Installation** :
```bash
./scripts/applications/install-gromacs.sh
```

**Utilisation** :
```bash
# Job Slurm
#!/bin/bash
#SBATCH --job-name=gromacs
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=16

gmx mdrun -s topol.tpr -v
```

---

#### OpenFOAM

**Qu'est-ce que c'est ?**
- CFD (Computational Fluid Dynamics)
- Simulation de fluides
- UtilisÃ© pour aÃ©rodynamique, etc.

**Pourquoi l'utiliser ?**
- âœ… Standard pour CFD
- âœ… Open-source
- âœ… TrÃ¨s performant
- âœ… Large communautÃ©

**Installation** :
```bash
./scripts/applications/install-openfoam.sh
```

**Utilisation** :
```bash
# Job Slurm
#!/bin/bash
#SBATCH --job-name=openfoam
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=8

mpirun -np 32 simpleFoam -parallel
```

---

#### Quantum ESPRESSO

**Qu'est-ce que c'est ?**
- Calculs quantiques (DFT)
- Ã‰lectronique de structure
- UtilisÃ© pour matÃ©riaux, etc.

**Pourquoi l'utiliser ?**
- âœ… Standard pour calculs quantiques
- âœ… Open-source
- âœ… TrÃ¨s performant
- âœ… Large communautÃ©

**Installation** :
```bash
./scripts/applications/install-quantum-espresso.sh
```

**Utilisation** :
```bash
# Job Slurm
#!/bin/bash
#SBATCH --job-name=qe
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=16

mpirun -np 32 pw.x < input.in > output.out
```

---

#### ParaView

**Qu'est-ce que c'est ?**
- Visualisation scientifique
- Visualisation de donnÃ©es HPC
- UtilisÃ© pour visualiser rÃ©sultats

**Pourquoi l'utiliser ?**
- âœ… Standard pour visualisation HPC
- âœ… Open-source
- âœ… TrÃ¨s performant (parallÃ¨le)
- âœ… Large communautÃ©

**Installation** :
```bash
./scripts/applications/install-paraview.sh
```

**Utilisation** :
```bash
# Via X2Go (remote graphics)
paraview

# Ou batch
pvpython script.py
```

---

### 4.2 Bases de DonnÃ©es

#### PostgreSQL

**Qu'est-ce que c'est ?**
- Base de donnÃ©es relationnelle
- ACID compliant
- UtilisÃ© pour SlurmDBD, applications

**Pourquoi l'utiliser ?**
- âœ… Standard SQL
- âœ… TrÃ¨s performant
- âœ… Open-source
- âœ… Extensible

**Installation** :
```bash
./scripts/database/install-postgresql.sh
```

**Utilisation** :
```bash
# Connexion
psql -U slurm -d slurm_acct_db

# RequÃªtes
SELECT * FROM job_table;
```

---

#### MongoDB

**Qu'est-ce que c'est ?**
- Base de donnÃ©es NoSQL
- Document-oriented
- UtilisÃ© pour donnÃ©es non-structurÃ©es

**Pourquoi l'utiliser ?**
- âœ… Flexible (schÃ©ma dynamique)
- âœ… Scalable
- âœ… Open-source
- âœ… Performant

**Installation** :
```bash
./scripts/database/install-mongodb.sh
```

**Utilisation** :
```bash
# Connexion
mongo

# RequÃªtes
db.collection.find()
```

---

#### InfluxDB

**Qu'est-ce que c'est ?**
- Base de donnÃ©es time-series
- OptimisÃ©e pour mÃ©triques
- UtilisÃ© pour monitoring

**Pourquoi l'utiliser ?**
- âœ… OptimisÃ© time-series
- âœ… TrÃ¨s performant
- âœ… Open-source
- âœ… Compatible Telegraf

**Installation** :
```bash
./scripts/database/install-influxdb.sh
```

**Utilisation** :
```bash
# AccÃ¨s web
http://localhost:8086

# RequÃªtes InfluxQL
SELECT * FROM cpu WHERE time > now() - 1h
```

---

### 4.3 Messaging et Streaming

#### RabbitMQ

**Qu'est-ce que c'est ?**
- Message broker
- AMQP protocol
- UtilisÃ© pour communication asynchrone

**Pourquoi l'utiliser ?**
- âœ… Standard AMQP
- âœ… TrÃ¨s performant
- âœ… Open-source
- âœ… Haute disponibilitÃ©

**Installation** :
```bash
./scripts/messaging/install-rabbitmq-complete.sh
```

**Utilisation** :
```bash
# AccÃ¨s web
http://localhost:15672
# Login: guest / guest

# Publier message
rabbitmqadmin publish exchange=amq.default routing_key=test payload="Hello"
```

---

#### Kafka

**Qu'est-ce que c'est ?**
- Streaming platform
- Event streaming
- UtilisÃ© pour Big Data

**Pourquoi l'utiliser ?**
- âœ… TrÃ¨s performant (millions messages/s)
- âœ… Scalable
- âœ… Open-source
- âœ… Standard Big Data

**Installation** :
```bash
./scripts/streaming/install-kafka-complete.sh
```

**Utilisation** :
```bash
# CrÃ©er topic
kafka-topics.sh --create --topic test --bootstrap-server localhost:9092

# Publier
kafka-console-producer.sh --topic test --bootstrap-server localhost:9092

# Consommer
kafka-console-consumer.sh --topic test --bootstrap-server localhost:9092
```

---

### 4.4 Big Data et Machine Learning

#### Apache Spark

**Qu'est-ce que c'est ?**
- Processing distribuÃ©
- Big Data analytics
- UtilisÃ© pour traitement de donnÃ©es massives

**Pourquoi l'utiliser ?**
- âœ… TrÃ¨s performant (in-memory)
- âœ… Scalable
- âœ… Open-source
- âœ… Standard Big Data

**Installation** :
```bash
./scripts/bigdata/install-spark.sh
```

**Utilisation** :
```bash
# Spark Shell
spark-shell

# Job Spark
spark-submit --class MyApp --master yarn myapp.jar
```

---

#### TensorFlow

**Qu'est-ce que c'est ?**
- Framework deep learning
- Machine learning
- UtilisÃ© pour IA

**Pourquoi l'utiliser ?**
- âœ… Standard deep learning
- âœ… TrÃ¨s performant (GPU)
- âœ… Open-source
- âœ… Large communautÃ©

**Installation** :
```bash
./scripts/ml/install-tensorflow.sh
```

**Utilisation** :
```python
import tensorflow as tf

# ModÃ¨le
model = tf.keras.Sequential([...])
model.fit(x_train, y_train)
```

---

#### PyTorch

**Qu'est-ce que c'est ?**
- Framework deep learning
- Alternative Ã  TensorFlow
- UtilisÃ© pour recherche IA

**Pourquoi l'utiliser ?**
- âœ… Plus flexible que TensorFlow
- âœ… TrÃ¨s performant (GPU)
- âœ… Open-source
- âœ… Popular en recherche

**Installation** :
```bash
./scripts/ml/install-pytorch.sh
```

**Utilisation** :
```python
import torch

# ModÃ¨le
model = torch.nn.Sequential(...)
loss = criterion(output, target)
```

---

## 5. Choix de Conception et Justifications

### 5.1 Pourquoi Docker au lieu de VMs ?

**Docker** :
- âœ… Plus lÃ©ger (partage kernel)
- âœ… DÃ©marrage plus rapide
- âœ… Moins de ressources
- âœ… Plus portable

**VMs** :
- âŒ Plus lourd (OS complet)
- âŒ DÃ©marrage lent
- âŒ Plus de ressources
- âŒ Moins portable

**DÃ©cision** : Docker pour portabilitÃ© et lÃ©gÃ¨retÃ©.

---

### 5.2 Pourquoi BeeGFS au lieu de GPFS ?

**BeeGFS** :
- âœ… Open-source (gratuit)
- âœ… Plus simple Ã  configurer
- âœ… Performance excellente
- âœ… CommunautÃ© active

**GPFS** :
- âŒ Commercial (licence coÃ»teuse)
- âŒ Plus complexe
- âŒ NÃ©cessite support IBM

**DÃ©cision** : BeeGFS pour open-source et simplicitÃ©.

---

### 5.3 Pourquoi Slurm au lieu de PBS/Torque ?

**Slurm** :
- âœ… Standard moderne HPC
- âœ… Plus de fonctionnalitÃ©s
- âœ… Meilleure performance
- âœ… CommunautÃ© trÃ¨s active

**PBS/Torque** :
- âŒ Plus ancien
- âŒ Moins de fonctionnalitÃ©s
- âŒ CommunautÃ© rÃ©duite

**DÃ©cision** : Slurm pour standard moderne.

---

### 5.4 Pourquoi Prometheus au lieu de Nagios ?

**Prometheus** :
- âœ… Pull-based (plus efficace)
- âœ… Time-series optimisÃ©
- âœ… PromQL puissant
- âœ… Standard moderne

**Nagios** :
- âŒ Push-based (moins efficace)
- âŒ Plus ancien
- âŒ Interface moins moderne

**DÃ©cision** : Prometheus pour monitoring moderne.

---

### 5.5 Pourquoi LDAP + Kerberos au lieu de FreeIPA ?

**LDAP + Kerberos** :
- âœ… Plus simple
- âœ… Plus de contrÃ´le
- âœ… Composants sÃ©parÃ©s (flexibilitÃ©)
- âœ… Moins de dÃ©pendances

**FreeIPA** :
- âŒ Plus complexe
- âŒ Tout-en-un (moins flexible)
- âŒ Plus de dÃ©pendances

**DÃ©cision** : LDAP + Kerberos pour simplicitÃ© et contrÃ´le.

---

## 6. Structure du Projet - Organisation

### 6.1 Organisation des Dossiers

```
cluster hpc/
â”œâ”€â”€ docker/                    # Configuration Docker
â”‚   â”œâ”€â”€ docker-compose-opensource.yml
â”‚   â”œâ”€â”€ frontal/Dockerfile
â”‚   â”œâ”€â”€ client/Dockerfile
â”‚   â”œâ”€â”€ scripts/               # Entrypoints
â”‚   â””â”€â”€ packages/              # RPMs (GPFS, Telegraf)
â”‚
â”œâ”€â”€ configs/                   # Configurations
â”‚   â”œâ”€â”€ prometheus/
â”‚   â”œâ”€â”€ grafana/
â”‚   â”œâ”€â”€ telegraf/
â”‚   â”œâ”€â”€ slurm/
â”‚   â”œâ”€â”€ loki/
â”‚   â””â”€â”€ jupyterhub/
â”‚
â”œâ”€â”€ scripts/                   # Scripts d'installation
â”‚   â”œâ”€â”€ applications/         # Applications scientifiques
â”‚   â”œâ”€â”€ storage/              # Stockage (BeeGFS, Lustre)
â”‚   â”œâ”€â”€ security/             # SÃ©curitÃ©
â”‚   â”œâ”€â”€ monitoring/           # Monitoring
â”‚   â”œâ”€â”€ database/             # Bases de donnÃ©es
â”‚   â”œâ”€â”€ messaging/            # Messaging
â”‚   â”œâ”€â”€ bigdata/             # Big Data
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ docs/                     # Documentation (85+ guides)
â”‚   â”œâ”€â”€ GUIDE_*.md
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ grafana-dashboards/       # Dashboards Grafana (54+)
â”‚   â””â”€â”€ *.json
â”‚
â””â”€â”€ examples/                 # Exemples
    â”œâ”€â”€ jobs/                 # Exemples jobs Slurm
    â””â”€â”€ jupyter/              # Notebooks Jupyter
```

### 6.2 Pourquoi cette Structure ?

**SÃ©paration des prÃ©occupations** :
- `docker/` : Tout ce qui concerne Docker
- `configs/` : Toutes les configurations
- `scripts/` : Tous les scripts (organisÃ©s par catÃ©gorie)
- `docs/` : Toute la documentation

**Avantages** :
- âœ… Facile Ã  naviguer
- âœ… Logique et intuitive
- âœ… Facile Ã  maintenir
- âœ… Facile Ã  Ã©tendre

---

## 7. Comment Tout Fonctionne Ensemble

### 7.1 Flux d'Authentification

```
1. Utilisateur se connecte
   ssh jdoe@frontal-01
   
2. SSH contacte LDAP
   SSH â”€â”€â–º LDAP (vÃ©rification identitÃ©)
   
3. LDAP vÃ©rifie et retourne OK
   LDAP â”€â”€â–º SSH (OK)
   
4. SSH demande ticket Kerberos
   SSH â”€â”€â–º Kerberos KDC (obtention ticket)
   
5. Kerberos Ã©met ticket
   Kerberos â”€â”€â–º SSH (ticket)
   
6. Utilisateur authentifiÃ©
   SSH â”€â”€â–º Session ouverte
```

### 7.2 Flux de Soumission de Job

```
1. Utilisateur soumet un job
   sbatch job.sh
   
2. SlurmCTLD reÃ§oit la requÃªte
   sbatch â”€â”€â–º SlurmCTLD
   
3. SlurmCTLD vÃ©rifie ressources
   SlurmCTLD â”€â”€â–º VÃ©rifie CPU, RAM disponibles
   
4. SlurmCTLD place dans file d'attente
   SlurmCTLD â”€â”€â–º File d'attente
   
5. Quand ressources disponibles, lance le job
   SlurmCTLD â”€â”€â–º SlurmD (compute-01)
   
6. SlurmD exÃ©cute le job
   SlurmD â”€â”€â–º ExÃ©cution job.sh
   
7. Job accÃ¨de Ã  BeeGFS
   Job â”€â”€â–º BeeGFS Client â”€â”€â–º BeeGFS MGMtd
   
8. MÃ©triques collectÃ©es
   Telegraf â”€â”€â–º Prometheus â”€â”€â–º Grafana
```

### 7.3 Flux de Monitoring

```
1. Telegraf collecte mÃ©triques (toutes les 15s)
   Telegraf â”€â”€â–º CPU, RAM, Disk, Network
   
2. Telegraf expose mÃ©triques
   Telegraf â”€â”€â–º Port 9273 (format Prometheus)
   
3. Prometheus scrape (toutes les 15s)
   Prometheus â”€â”€â–º http://172.20.0.101:9273/metrics
   
4. Prometheus stocke dans TSDB
   Prometheus â”€â”€â–º TSDB (rÃ©tention 30 jours)
   
5. Grafana interroge Prometheus
   Grafana â”€â”€â–º Prometheus (requÃªte PromQL)
   
6. Grafana affiche dans dashboard
   Grafana â”€â”€â–º Dashboard (visualisation)
   
7. Alertes si seuils dÃ©passÃ©s
   Prometheus â”€â”€â–º Alertmanager â”€â”€â–º Notification
```

---

## 8. Guide d'Utilisation de Chaque Logiciel

### 8.1 Slurm - Guide Complet

Voir : `docs/GUIDE_LANCEMENT_JOBS.md`

### 8.2 Applications Scientifiques

Voir : `docs/GUIDE_APPLICATIONS_SCIENTIFIQUES_COMPLET.md`

### 8.3 Monitoring

Voir : `docs/GUIDE_MONITORING_COMPLET.md`

### 8.4 SÃ©curitÃ©

Voir : `docs/GUIDE_SECURITE_AVANCEE.md`

---

## 9. Maintenance et OpÃ©rations

### 9.1 Maintenance Quotidienne

Voir : `docs/GUIDE_MAINTENANCE_COMPLETE.md`

### 9.2 Troubleshooting

Voir : `docs/GUIDE_TROUBLESHOOTING.md`

### 9.3 Mise Ã  Jour

Voir : `docs/GUIDE_MISE_A_JOUR_REPARATION.md`

---

## 10. Monitoring et Dashboards

### 10.1 Dashboards Disponibles

**54+ dashboards Grafana** disponibles dans `grafana-dashboards/` :

- HPC Cluster Overview
- CPU/Memory by Node
- Network I/O
- Slurm Jobs
- Slurm Partitions
- Applications (Redis, RabbitMQ, Kafka, etc.)
- SÃ©curitÃ© (Firewall, IDS, Compliance)
- Et 40+ autres...

### 10.2 Ajout d'Agents de Monitoring

**Sur un nouveau nÅ“ud** :

1. Installer Telegraf :
```bash
./scripts/monitoring/install-telegraf.sh
```

2. Configurer Telegraf :
```bash
cp configs/telegraf/telegraf-slave.conf /etc/telegraf/telegraf.conf
```

3. Ajouter Ã  Prometheus :
```yaml
# configs/prometheus/prometheus.yml
- job_name: 'new-node-telegraf'
  static_configs:
    - targets: ['172.20.0.XXX:9273']
```

4. RedÃ©marrer Prometheus :
```bash
docker-compose -f docker/docker-compose-opensource.yml restart prometheus
```

### 10.3 Monitoring Hardware

**Node Exporter** (dÃ©jÃ  installÃ©) collecte :
- CPU (utilisation, tempÃ©rature)
- MÃ©moire (RAM, swap)
- Disque (I/O, espace)
- RÃ©seau (trafic, erreurs)
- SystÃ¨me (load, uptime)

**AccÃ¨s** : `http://172.20.0.101:9100/metrics`

### 10.4 Monitoring RÃ©seau

**Telegraf** collecte :
- Trafic rÃ©seau (bytes in/out)
- Paquets (in/out, erreurs)
- Connexions (TCP, UDP)

**Dashboard** : "Network I/O" dans Grafana

---

## ğŸ“š Ressources et Documentation ComplÃ©mentaire

### Documentation par CatÃ©gorie

- **Technologies** : `docs/TECHNOLOGIES_CLUSTER.md`
- **Applications** : `docs/GUIDE_APPLICATIONS_SCIENTIFIQUES_COMPLET.md`
- **Architecture** : `docs/ARCHITECTURE.md`
- **Monitoring** : `docs/GUIDE_MONITORING_COMPLET.md`
- **SÃ©curitÃ©** : `docs/GUIDE_SECURITE_AVANCEE.md`
- **Maintenance** : `docs/GUIDE_MAINTENANCE_COMPLETE.md`

### Index Complet

Voir : `DOCUMENTATION_COMPLETE_INDEX_300_ETAPES.md`

---

**Version**: 1.0  
**DerniÃ¨re mise Ã  jour**: 2024
