# Technologies et Outils du Cluster HPC
## Documentation Technique ComplÃ¨te

**Classification**: Documentation Technique  
**Public**: Ã‰tudiants Master / IngÃ©nieurs  
**Version**: 1.0  
**Date**: 2024

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [Stack d'Authentification](#stack-dauthentification)
3. [Gestion des Packages](#gestion-des-packages)
4. [Remote Graphics](#remote-graphics)
5. [Scheduler et Jobs](#scheduler-et-jobs)
6. [Stockage](#stockage)
7. [Monitoring](#monitoring)
8. [Provisioning](#provisioning)

---

## ğŸ¯ Vue d'ensemble

Le cluster HPC utilise une stack complÃ¨te d'outils enterprise pour gÃ©rer un environnement de calcul haute performance sÃ©curisÃ© et maintenable.

### Architecture GÃ©nÃ©rale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NÅ’UDS FRONTAUX                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LDAP    â”‚  â”‚ Kerberos â”‚  â”‚  Nexus   â”‚  â”‚  X2Go    â”‚   â”‚
â”‚  â”‚ (389DS)  â”‚  â”‚   KDC    â”‚  â”‚ (PyPI)   â”‚  â”‚ (Remote) â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Slurm  â”‚  â”‚  BeeGFS  â”‚  â”‚Prometheusâ”‚  â”‚  Grafana â”‚   â”‚
â”‚  â”‚  CTLD   â”‚  â”‚   MGMtd  â”‚  â”‚          â”‚  â”‚          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Compute-01  â”‚  â”‚  Compute-02  â”‚  â”‚  Compute-06  â”‚
â”‚  (SlurmD)    â”‚  â”‚  (SlurmD)    â”‚  â”‚  (SlurmD)    â”‚
â”‚  + BeeGFS    â”‚  â”‚  + BeeGFS    â”‚  â”‚  + BeeGFS    â”‚
â”‚  + Spack     â”‚  â”‚  + Spack     â”‚  â”‚  + Spack     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” Stack d'Authentification

### 1. LDAP (389 Directory Server)

#### Qu'est-ce que c'est ?

**LDAP** (Lightweight Directory Access Protocol) est un protocole d'accÃ¨s Ã  un annuaire distribuÃ©. **389 Directory Server** est l'implÃ©mentation open-source d'IBM (anciennement Red Hat Directory Server).

#### Pourquoi l'utiliser ?

- **Authentification centralisÃ©e** : Un seul point d'authentification pour tous les nÅ“uds
- **Gestion des utilisateurs** : CrÃ©ation/modification/suppression centralisÃ©e
- **IntÃ©gration** : Compatible avec Kerberos, SSH, Slurm, etc.
- **Standards** : Protocole standardisÃ© (RFC 4510)

#### Comment Ã§a fonctionne ?

```
Client (SSH, Slurm, etc.)
    â”‚
    â”‚ RequÃªte LDAP (port 389)
    â–¼
389 Directory Server
    â”‚
    â”‚ VÃ©rification credentials
    â–¼
Base de donnÃ©es LDAP (Berkeley DB)
    â”‚
    â””â”€â–º Retourne : OK / NOK
```

**Structure LDAP** :
```
dc=cluster,dc=local
â”œâ”€â”€ ou=users
â”‚   â”œâ”€â”€ uid=jdoe,ou=users,dc=cluster,dc=local
â”‚   â”œâ”€â”€ uid=asmith,ou=users,dc=cluster,dc=local
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ou=groups
â”‚   â”œâ”€â”€ cn=hpc-users,ou=groups,dc=cluster,dc=local
â”‚   â””â”€â”€ cn=admins,ou=groups,dc=cluster,dc=local
â””â”€â”€ ou=computers
    â”œâ”€â”€ cn=node-01,ou=computers,dc=cluster,dc=local
    â””â”€â”€ ...
```

#### Installation et Configuration

**Installation** :
```bash
zypper install -y 389-ds 389-ds-base
```

**Configuration initiale** :
```bash
setup-ds.pl --silent --file /path/to/inf.conf
```

**CrÃ©ation d'un utilisateur** :
```bash
ldapadd -x -D "cn=Directory Manager" -w "password" <<EOF
dn: uid=jdoe,ou=users,dc=cluster,dc=local
objectClass: inetOrgPerson
objectClass: posixAccount
uid: jdoe
cn: John Doe
sn: Doe
userPassword: {SSHA}encrypted_password
uidNumber: 1001
gidNumber: 1001
homeDirectory: /home/jdoe
loginShell: /bin/bash
EOF
```

#### Maintenance

**VÃ©rification du service** :
```bash
systemctl status dirsrv@cluster
ldapsearch -x -b "dc=cluster,dc=local" -s base
```

**Sauvegarde** :
```bash
# Export LDIF
ldapsearch -x -b "dc=cluster,dc=local" > backup.ldif

# Restauration
ldapadd -x -D "cn=Directory Manager" -w "password" -f backup.ldif
```

**Logs** :
```bash
tail -f /var/log/dirsrv/slapd-cluster/access
tail -f /var/log/dirsrv/slapd-cluster/errors
```

---

### 2. Kerberos

#### Qu'est-ce que c'est ?

**Kerberos** est un protocole d'authentification rÃ©seau sÃ©curisÃ© basÃ© sur des tickets cryptographiques. Il permet l'authentification unique (SSO) sans transmettre de mots de passe en clair.

#### Pourquoi l'utiliser ?

- **SÃ©curitÃ©** : Pas de mots de passe en clair sur le rÃ©seau
- **SSO** : Authentification unique pour tous les services
- **IntÃ©gration** : Compatible avec LDAP, SSH, NFS, etc.
- **Standards** : Protocole standardisÃ© (RFC 4120)

#### Comment Ã§a fonctionne ?

```
1. Client demande un ticket
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Client  â”‚ â”€â”€â–º KDC (Key Distribution Center)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     Port 88
   
2. KDC vÃ©rifie l'identitÃ© et Ã©met un TGT
   (Ticket Granting Ticket)
   
3. Client utilise le TGT pour obtenir un service ticket
   
4. Client prÃ©sente le service ticket au service
   (SSH, NFS, etc.)
```

**Composants** :
- **KDC** (Key Distribution Center) : Serveur d'authentification
- **Realm** : Domaine Kerberos (ex: CLUSTER.LOCAL)
- **Principal** : IdentitÃ© (ex: jdoe@CLUSTER.LOCAL)
- **Ticket** : Token d'authentification temporaire

#### Installation et Configuration

**Installation** :
```bash
zypper install -y krb5 krb5-server krb5-client
```

**Configuration KDC** :
```bash
# /etc/krb5.conf
[libdefaults]
    default_realm = CLUSTER.LOCAL
    ticket_lifetime = 24h
    renew_lifetime = 7d

[realms]
    CLUSTER.LOCAL = {
        kdc = frontal-01.cluster.local:88
        admin_server = frontal-01.cluster.local:749
    }
```

**Initialisation de la base de donnÃ©es** :
```bash
kdb5_util create -s
```

**CrÃ©ation d'un principal** :
```bash
kadmin.local -q "addprinc jdoe@CLUSTER.LOCAL"
```

#### Utilisation

**Obtenir un ticket** :
```bash
kinit jdoe@CLUSTER.LOCAL
# Entrer le mot de passe
```

**VÃ©rifier le ticket** :
```bash
klist
```

**SSH avec Kerberos** :
```bash
# Configuration SSH
# /etc/ssh/sshd_config
GSSAPIAuthentication yes
GSSAPICleanupCredentials yes

# Connexion (sans mot de passe si ticket valide)
ssh jdoe@node-01
```

#### Maintenance

**VÃ©rification du service** :
```bash
systemctl status krb5kdc
kadmin.local -q "listprincs"
```

**Expiration des tickets** :
```bash
# VÃ©rifier l'expiration
klist -v

# Renouveler
kinit -R
```

**Logs** :
```bash
tail -f /var/log/krb5kdc.log
tail -f /var/log/kadmin.log
```

---

### 3. FreeIPA (Alternative)

#### Qu'est-ce que c'est ?

**FreeIPA** (Identity, Policy, and Audit) est une solution intÃ©grÃ©e qui combine :
- LDAP (389 Directory Server)
- Kerberos
- DNS
- PKI (Certificats)
- Gestion des politiques

#### Pourquoi l'utiliser ?

- **Solution unifiÃ©e** : Tout-en-un au lieu de configurer LDAP + Kerberos sÃ©parÃ©ment
- **Interface web** : Administration via interface graphique
- **Enterprise-ready** : Solution robuste pour production
- **Gestion des politiques** : ContrÃ´le d'accÃ¨s centralisÃ©

#### Installation

```bash
# Sur SUSE, utiliser le conteneur FreeIPA
docker run -d --name freeipa \
    -h ipa.cluster.local \
    -v /sys/fs/cgroup:/sys/fs/cgroup:ro \
    --tmpfs /run --tmpfs /tmp \
    -v /var/lib/ipa-data:/data:Z \
    freeipa/freeipa-server:centos-8-stream \
    ipa-server-install -U -r CLUSTER.LOCAL \
    -n cluster.local -p 'AdminPassword' \
    --admin-password 'AdminPassword'
```

---

## ğŸ“¦ Gestion des Packages

### 1. Nexus Repository

#### Qu'est-ce que c'est ?

**Nexus Repository** est un gestionnaire de dÃ©pÃ´ts d'artefacts qui permet de crÃ©er un miroir privÃ© pour :
- Packages Python (PyPI)
- Packages R (CRAN)
- Packages npm
- Packages Maven
- Etc.

#### Pourquoi l'utiliser ?

- **Air-gapped** : Fonctionne en environnement isolÃ©
- **Performance** : Cache local = tÃ©lÃ©chargements plus rapides
- **SÃ©curitÃ©** : ContrÃ´le des packages installÃ©s
- **Audit** : TraÃ§abilitÃ© des packages utilisÃ©s

#### Comment Ã§a fonctionne ?

```
pip install numpy
    â”‚
    â”‚ RequÃªte vers Nexus (port 8081)
    â–¼
Nexus Repository
    â”‚
    â”œâ”€â–º Si package en cache : retourne directement
    â”‚
    â””â”€â–º Si pas en cache : tÃ©lÃ©charge depuis PyPI
        puis met en cache et retourne
```

#### Installation

**Via Docker** :
```bash
docker run -d --name nexus \
    -p 8081:8081 \
    -v nexus-data:/nexus-data \
    sonatype/nexus3
```

**Configuration pip** :
```bash
# ~/.pip/pip.conf
[global]
index-url = http://frontal-01:8081/repository/pypi-group/simple
trusted-host = frontal-01
```

#### Maintenance

**VÃ©rification** :
```bash
curl http://frontal-01:8081/service/rest/v1/status
```

**Nettoyage du cache** :
```bash
# Via interface web : http://frontal-01:8081
# Administration > Repositories > Cleanup policies
```

---

### 2. Spack

#### Qu'est-ce que c'est ?

**Spack** est un gestionnaire de packages scientifique pour HPC qui permet de :
- Compiler des logiciels scientifiques
- GÃ©rer plusieurs versions
- GÃ©rer les dÃ©pendances
- Optimiser pour l'architecture

#### Pourquoi l'utiliser ?

- **Packages scientifiques** : OpenMPI, HDF5, NetCDF, etc.
- **Optimisation** : Compilation optimisÃ©e pour l'architecture
- **Versions multiples** : Plusieurs versions installÃ©es simultanÃ©ment
- **Environnements** : Isolation des environnements

#### Installation

```bash
git clone https://github.com/spack/spack.git /opt/spack
. /opt/spack/share/spack/setup-env.sh
spack compiler find
```

#### Utilisation

**Installer un package** :
```bash
spack install openmpi@4.1.5
spack install hdf5@1.14.0
```

**Charger un package** :
```bash
spack load openmpi
mpirun --version
```

**CrÃ©er un environnement** :
```bash
spack env create myenv
spack env activate myenv
spack add openmpi hdf5
spack install
```

#### Maintenance

**Mise Ã  jour** :
```bash
cd /opt/spack
git pull
spack reindex
```

**Nettoyage** :
```bash
spack clean -a  # Nettoie tout
spack clean -m  # Nettoie les miroirs
```

---

## ğŸ–¥ï¸ Remote Graphics

### X2Go (Open-Source)

#### Qu'est-ce que c'est ?

**X2Go** est une solution de remote graphics open-source qui permet d'exÃ©cuter des applications graphiques sur le cluster et d'afficher l'interface sur un client distant via SSH.

#### Pourquoi l'utiliser ?

- **100% Gratuit** : Open-source, aucune licence requise
- **Applications graphiques** : ParaView, GROMACS, OpenFOAM, Quantum ESPRESSO
- **Performance** : OptimisÃ© via SSH
- **SÃ©curitÃ©** : Chiffrement SSH intÃ©grÃ©
- **Multi-utilisateurs** : Plusieurs sessions simultanÃ©es

#### Comment Ã§a fonctionne ?

```
Client (Windows/Linux)
    â”‚
    â”‚ Connexion SSH avec X11 Forwarding
    â–¼
X2Go Server (frontal-01)
    â”‚
    â”‚ Lance l'application graphique
    â–¼
Application (ParaView, GROMACS, OpenFOAM, Quantum ESPRESSO)
    â”‚
    â”‚ Stream graphique via X11
    â””â”€â–º AffichÃ© sur le client
```

#### Installation

**Sur le serveur** :
```bash
cd cluster\ hpc/scripts/remote-graphics
sudo ./install-x2go.sh
```

**Configuration** :
```bash
# X2Go utilise SSH (port 22)
# Configuration automatique via script
```

#### Utilisation

**Lancer une application** :
```bash
# Sur le client (avec X11 Forwarding)
ssh -X user@frontal-01

# Dans la session SSH
paraview
```

#### Alternative : NoMachine

**NoMachine** est une alternative gratuite Ã©galement disponible :
```bash
cd cluster\ hpc/scripts/remote-graphics
sudo ./install-nomachine.sh
```

**Connexion** : frontal-01:4000

---

## âš¡ Scheduler et Jobs

### Slurm Workload Manager

#### Qu'est-ce que c'est ?

**Slurm** (Simple Linux Utility for Resource Management) est un gestionnaire de jobs et de ressources pour clusters HPC.

#### Pourquoi l'utiliser ?

- **Gestion des ressources** : CPU, mÃ©moire, GPU
- **File d'attente** : Gestion intelligente des jobs
- **Multi-utilisateurs** : Partage Ã©quitable des ressources
- **Standards** : UtilisÃ© par la majoritÃ© des clusters HPC

#### Comment Ã§a fonctionne ?

```
Utilisateur soumet un job
    â”‚
    â–¼
SlurmCTLD (Controller)
    â”‚
    â”œâ”€â–º VÃ©rifie les ressources disponibles
    â”œâ”€â–º Place dans la file d'attente
    â””â”€â–º Lance sur les nÅ“uds disponibles
        â”‚
        â–¼
    SlurmD (Daemon sur chaque nÅ“ud)
        â”‚
        â””â”€â–º ExÃ©cute le job
```

#### Soumission de Jobs

**Job simple** :
```bash
#!/bin/bash
#SBATCH --job-name=myjob
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=4
#SBATCH --time=01:00:00

srun my_program
```

**Soumission** :
```bash
sbatch myjob.sh
```

**VÃ©rification** :
```bash
squeue -u $USER
sinfo
```

#### Maintenance

**VÃ©rification** :
```bash
systemctl status slurmctld
scontrol show nodes
```

**Logs** :
```bash
tail -f /var/log/slurmctld.log
```

---

## ğŸ’¾ Stockage

### BeeGFS (SystÃ¨me de Fichiers ParallÃ¨le Open-Source)

**Qu'est-ce que c'est ?**
- SystÃ¨me de fichiers parallÃ¨le open-source
- OptimisÃ© pour HPC et calcul haute performance

**Pourquoi l'utiliser ?**
- âœ… **Performance** : TrÃ¨s rapide pour HPC (I/O parallÃ¨le)
- âœ… **ScalabilitÃ©** : Supporte des milliers de nÅ“uds
- âœ… **Open-source** : Gratuit, pas de licence
- âœ… **FacilitÃ©** : Plus simple que Lustre

**Installation** :
```bash
./scripts/storage/install-beegfs.sh
```

**Alternative** : Lustre Ã©galement disponible
```bash
./scripts/storage/install-lustre.sh
```

---

## ğŸ“Š Monitoring

### Prometheus + Grafana + Telegraf

Voir documentation dÃ©diÃ©e dans `monitoring/README.md`

---

## ğŸ”§ Provisioning

### TrinityX + Warewulf

Voir documentation dÃ©diÃ©e dans `trinityx/GUIDE_INSTALLATION_TRINITYX.md`

---

## ğŸ“š Ressources

- **LDAP 389DS**: https://directory.fedoraproject.org/
- **Kerberos**: https://web.mit.edu/kerberos/
- **FreeIPA**: https://www.freeipa.org/
- **Nexus**: https://www.sonatype.com/products/nexus-repository
- **Spack**: https://spack.io/
- **X2Go**: https://wiki.x2go.org/
- **NoMachine**: https://www.nomachine.com/
- **Slurm**: https://slurm.schedmd.com/

---

**Version**: 1.0  
**DerniÃ¨re mise Ã  jour**: 2024
