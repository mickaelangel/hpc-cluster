# ğŸ“š GUIDE COMPLET - TOUTES LES TECHNOLOGIES
## Explication DÃ©taillÃ©e de Chaque Technologie, Pourquoi et Comment

**Classification**: Documentation Technique Exhaustive  
**Public**: Tous Niveaux  
**Version**: 1.0  
**Date**: 2024

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Technologies de Base](#1-technologies-de-base)
2. [Authentification et SÃ©curitÃ©](#2-authentification-et-sÃ©curitÃ©)
3. [Scheduler et Gestion des Jobs](#3-scheduler-et-gestion-des-jobs)
4. [Stockage](#4-stockage)
5. [Monitoring et ObservabilitÃ©](#5-monitoring-et-observabilitÃ©)
6. [Applications Scientifiques](#6-applications-scientifiques)
7. [Bases de DonnÃ©es](#7-bases-de-donnÃ©es)
8. [Messaging et Streaming](#8-messaging-et-streaming)
9. [Big Data et Machine Learning](#9-big-data-et-machine-learning)
10. [CI/CD et Automatisation](#10-cicd-et-automatisation)
11. [Orchestration et Conteneurs](#11-orchestration-et-conteneurs)
12. [SÃ©curitÃ© AvancÃ©e](#12-sÃ©curitÃ©-avancÃ©e)
13. [Remote Graphics](#13-remote-graphics)
14. [Gestion de Packages](#14-gestion-de-packages)

---

## 1. Technologies de Base

### 1.1 Docker

#### Qu'est-ce que c'est ?

**Docker** est une plateforme de conteneurisation qui permet d'empaqueter une application et ses dÃ©pendances dans un conteneur isolÃ©, lÃ©ger et portable.

**Concepts clÃ©s** :
- **Image** : Template d'un conteneur (ex: `openSUSE Leap 15.4`)
- **Conteneur** : Instance d'une image en cours d'exÃ©cution
- **Dockerfile** : Instructions pour construire une image
- **Docker Compose** : Orchestration de plusieurs conteneurs

#### Pourquoi l'utiliser dans ce projet ?

**Avantages** :
- âœ… **PortabilitÃ©** : Fonctionne identiquement sur n'importe quel systÃ¨me
- âœ… **Isolation** : Chaque service dans son conteneur (pas de conflits)
- âœ… **ReproductibilitÃ©** : MÃªme environnement partout (dev, test, prod)
- âœ… **FacilitÃ©** : Un seul fichier `docker-compose.yml` pour tout
- âœ… **Air-gapped** : Peut fonctionner sans Internet (images exportÃ©es)
- âœ… **LÃ©ger** : Partage le kernel (vs VMs qui ont leur propre OS)

**Alternatives considÃ©rÃ©es et pourquoi rejetÃ©es** :
- âŒ **VMware/KVM** : Trop lourd, nÃ©cessite hyperviseur, dÃ©marrage lent
- âŒ **LXC** : Moins standardisÃ©, moins d'outils, communautÃ© plus petite
- âŒ **Installation native** : Difficile Ã  reproduire, dÃ©pendances complexes, pas portable

**Pourquoi Docker a Ã©tÃ© choisi** :
- Standard de l'industrie (utilisÃ© partout)
- Large Ã©cosystÃ¨me (millions d'images disponibles)
- Facile Ã  apprendre et utiliser
- Compatible SUSE 15 SP4
- Parfait pour simulation/dÃ©mo de cluster

#### Comment Ã§a fonctionne ?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Docker Engine               â”‚
â”‚  (sur le systÃ¨me hÃ´te)              â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Conteneur frontal-01         â”‚  â”‚
â”‚  â”‚  â”œâ”€â–º openSUSE Leap 15.4       â”‚  â”‚
â”‚  â”‚  â”œâ”€â–º SlurmCTLD                â”‚  â”‚
â”‚  â”‚  â”œâ”€â–º LDAP (389DS)             â”‚  â”‚
â”‚  â”‚  â”œâ”€â–º Kerberos KDC             â”‚  â”‚
â”‚  â”‚  â””â”€â–º Services systÃ¨me          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Conteneur compute-01          â”‚  â”‚
â”‚  â”‚  â”œâ”€â–º openSUSE Leap 15.4       â”‚  â”‚
â”‚  â”‚  â”œâ”€â–º SlurmD                   â”‚  â”‚
â”‚  â”‚  â”œâ”€â–º BeeGFS Client            â”‚  â”‚
â”‚  â”‚  â””â”€â–º Applications HPC         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Conteneur Prometheus          â”‚  â”‚
â”‚  â”‚  â”œâ”€â–º Image Prometheus          â”‚  â”‚
â”‚  â”‚  â””â”€â–º Configuration            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Fichiers** :
- `docker/frontal/Dockerfile` : Image pour nÅ“uds frontaux
- `docker/client/Dockerfile` : Image pour nÅ“uds de calcul
- `docker/docker-compose-opensource.yml` : Orchestration complÃ¨te

**Utilisation** :
```bash
# Build des images
cd docker/
docker-compose -f docker-compose-opensource.yml build

# DÃ©marrage
docker-compose -f docker-compose-opensource.yml up -d

# ArrÃªt
docker-compose -f docker-compose-opensource.yml down

# Logs
docker-compose -f docker-compose-opensource.yml logs -f

# Statut
docker-compose -f docker-compose-opensource.yml ps
```

**Maintenance** :
```bash
# Mise Ã  jour images
docker-compose -f docker-compose-opensource.yml pull
docker-compose -f docker-compose-opensource.yml up -d

# Nettoyage
docker system prune -a
```

---

### 1.2 openSUSE Leap 15.4

#### Qu'est-ce que c'est ?

**openSUSE Leap** est une distribution Linux basÃ©e sur SUSE Linux Enterprise Server (SLES), garantissant la compatibilitÃ© avec SLES.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Compatible SLES** : MÃªme base que SUSE 15 SP4/SP7
- âœ… **Stable** : Version LTS (Long Term Support)
- âœ… **Zypper** : Gestionnaire de packages performant
- âœ… **Enterprise-ready** : UtilisÃ© en production
- âœ… **Open-source** : Gratuit

**Alternatives considÃ©rÃ©es** :
- âŒ **CentOS/RHEL** : Moins compatible avec SUSE
- âŒ **Ubuntu** : DiffÃ©rent de SUSE (apt vs zypper)
- âŒ **Debian** : Moins enterprise-oriented

**Pourquoi openSUSE Leap a Ã©tÃ© choisi** :
- CompatibilitÃ© maximale avec SUSE 15 SP4
- Stable et fiable
- Zypper excellent gestionnaire de packages
- Parfait pour environnement enterprise

---

## 2. Authentification et SÃ©curitÃ©

### 2.1 LDAP (389 Directory Server)

#### Qu'est-ce que c'est ?

**LDAP** (Lightweight Directory Access Protocol) est un protocole d'accÃ¨s Ã  un annuaire distribuÃ©. **389 Directory Server** est l'implÃ©mentation open-source d'IBM (anciennement Red Hat Directory Server).

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Authentification centralisÃ©e** : Un seul point d'authentification pour tous les nÅ“uds
- âœ… **Gestion utilisateurs** : CrÃ©ation/modification/suppression centralisÃ©e
- âœ… **IntÃ©gration** : Compatible avec Kerberos, SSH, Slurm, NFS, etc.
- âœ… **Standards** : Protocole standardisÃ© (RFC 4510)
- âœ… **Scalable** : Supporte des milliers d'utilisateurs
- âœ… **Open-source** : Gratuit

**Alternatives considÃ©rÃ©es** :
- âŒ **Fichiers locaux (/etc/passwd)** : Non scalable, pas centralisÃ©, difficile Ã  maintenir
- âŒ **Active Directory** : Commercial, Windows-centric, licence coÃ»teuse
- âŒ **FreeIPA** : Plus complexe, peut Ãªtre overkill pour certains cas

**Pourquoi LDAP a Ã©tÃ© choisi** :
- Standard de l'industrie pour annuaires
- Simple et efficace
- Compatible avec tout (SSH, Slurm, NFS, etc.)
- Open-source pur
- Bien documentÃ©

#### Comment Ã§a fonctionne ?

```
Client (SSH, Slurm, etc.)
    â”‚
    â”‚ RequÃªte LDAP (port 389)
    â”‚ "VÃ©rifier utilisateur jdoe / password"
    â–¼
389 Directory Server
    â”‚
    â”‚ Recherche dans base LDAP (Berkeley DB)
    â”‚ dc=cluster,dc=local
    â”‚   â””â”€â–º ou=users
    â”‚       â””â”€â–º uid=jdoe
    â”‚
    â”œâ”€â–º VÃ©rifie password (hash SSHA)
    â”‚
    â””â”€â–º Retourne : OK / NOK
```

**Structure LDAP** :
```
dc=cluster,dc=local
â”œâ”€â”€ ou=users
â”‚   â”œâ”€â”€ uid=jdoe,ou=users,dc=cluster,dc=local
â”‚   â”‚   â”œâ”€â”€ cn: John Doe
â”‚   â”‚   â”œâ”€â”€ sn: Doe
â”‚   â”‚   â”œâ”€â”€ userPassword: {SSHA}encrypted_password
â”‚   â”‚   â”œâ”€â”€ uidNumber: 1001
â”‚   â”‚   â”œâ”€â”€ gidNumber: 1001
â”‚   â”‚   â”œâ”€â”€ homeDirectory: /home/jdoe
â”‚   â”‚   â””â”€â”€ loginShell: /bin/bash
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ou=groups
â”‚   â”œâ”€â”€ cn=hpc-users,ou=groups,dc=cluster,dc=local
â”‚   â”‚   â”œâ”€â”€ memberUid: jdoe
â”‚   â”‚   â””â”€â”€ gidNumber: 1001
â”‚   â””â”€â”€ cn=admins,ou=groups,dc=cluster,dc=local
â””â”€â”€ ou=computers
    â”œâ”€â”€ cn=node-01,ou=computers,dc=cluster,dc=local
    â””â”€â”€ ...
```

**Installation** :
```bash
./scripts/install-ldap-kerberos.sh
```

**Configuration** :
```bash
# CrÃ©er un utilisateur
ldapadd -x -D "cn=Directory Manager" -w "password" <<EOF
dn: uid=jdoe,ou=users,dc=cluster,dc=local
objectClass: inetOrgPerson
objectClass: posixAccount
objectClass: shadowAccount
uid: jdoe
cn: John Doe
sn: Doe
userPassword: {SSHA}encrypted_password
uidNumber: 1001
gidNumber: 1001
homeDirectory: /home/jdoe
loginShell: /bin/bash
EOF

# Rechercher
ldapsearch -x -b "dc=cluster,dc=local" "(uid=jdoe)"

# Modifier
ldapmodify -x -D "cn=Directory Manager" -w "password" <<EOF
dn: uid=jdoe,ou=users,dc=cluster,dc=local
changetype: modify
replace: userPassword
userPassword: {SSHA}new_encrypted_password
EOF
```

**Maintenance** :
```bash
# VÃ©rification
systemctl status dirsrv@cluster
ldapsearch -x -b "dc=cluster,dc=local" -s base

# Sauvegarde
ldapsearch -x -b "dc=cluster,dc=local" > backup.ldif

# Restauration
ldapadd -x -D "cn=Directory Manager" -w "password" -f backup.ldif

# Logs
tail -f /var/log/dirsrv/slapd-cluster/access
tail -f /var/log/dirsrv/slapd-cluster/errors
```

---

### 2.2 Kerberos

#### Qu'est-ce que c'est ?

**Kerberos** est un protocole d'authentification rÃ©seau sÃ©curisÃ© basÃ© sur des tickets cryptographiques. Il permet l'authentification unique (SSO) sans transmettre de mots de passe en clair sur le rÃ©seau.

**Concepts clÃ©s** :
- **KDC** (Key Distribution Center) : Serveur d'authentification
- **Realm** : Domaine Kerberos (ex: CLUSTER.LOCAL)
- **Principal** : IdentitÃ© (ex: jdoe@CLUSTER.LOCAL)
- **TGT** (Ticket Granting Ticket) : Ticket pour obtenir d'autres tickets
- **Service Ticket** : Ticket pour accÃ©der Ã  un service spÃ©cifique

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **SÃ©curitÃ©** : Pas de mots de passe en clair sur le rÃ©seau
- âœ… **SSO** : Authentification unique (Single Sign-On)
- âœ… **IntÃ©gration** : Compatible avec LDAP, SSH, NFS, etc.
- âœ… **Standards** : Protocole standardisÃ© (RFC 4120)
- âœ… **Enterprise** : UtilisÃ© par toutes les grandes entreprises

**Alternatives considÃ©rÃ©es** :
- âŒ **Mots de passe en clair** : Non sÃ©curisÃ©, vulnÃ©rable aux attaques
- âŒ **Certificats uniquement** : Plus complexe Ã  gÃ©rer, pas de SSO
- âŒ **OAuth/SAML** : Overkill pour HPC, plus pour web apps

**Pourquoi Kerberos a Ã©tÃ© choisi** :
- Standard pour environnements enterprise
- SÃ©curisÃ© par dÃ©faut (chiffrement)
- SSO intÃ©grÃ© (une authentification pour tout)
- Compatible avec tout (SSH, NFS, Slurm, etc.)
- Bien documentÃ©

#### Comment Ã§a fonctionne ?

```
1. Client demande un TGT
   Client â”€â”€â–º KDC (Key Distribution Center)
              Port 88
              "Je suis jdoe, voici mon password (chiffrÃ©)"
   
2. KDC vÃ©rifie l'identitÃ© (via LDAP)
   KDC â”€â”€â–º LDAP (vÃ©rification jdoe / password)
   LDAP â”€â”€â–º KDC (OK / NOK)
   
3. Si OK, KDC Ã©met un TGT (chiffrÃ© avec password client)
   KDC â”€â”€â–º Client (TGT chiffrÃ©)
   
4. Client utilise le TGT pour obtenir un service ticket
   Client â”€â”€â–º KDC (avec TGT)
              "Je veux accÃ©der Ã  SSH sur node-01"
   KDC â”€â”€â–º Client (Service Ticket pour SSH/node-01)
   
5. Client prÃ©sente le service ticket au service
   Client â”€â”€â–º SSH/node-01 (avec Service Ticket)
   SSH â”€â”€â–º KDC (vÃ©rification ticket)
   KDC â”€â”€â–º SSH (ticket valide)
   SSH â”€â”€â–º Client (OK, authentifiÃ©, session ouverte)
```

**DurÃ©e de vie des tickets** :
- **TGT** : 24 heures (par dÃ©faut)
- **Service Ticket** : 10 heures (par dÃ©faut)
- **Renouvellement** : Possible jusqu'Ã  7 jours

**Installation** :
```bash
./scripts/install-ldap-kerberos.sh
```

**Configuration** :
```bash
# /etc/krb5.conf
[libdefaults]
    default_realm = CLUSTER.LOCAL
    ticket_lifetime = 24h
    renew_lifetime = 7d
    forwardable = true

[realms]
    CLUSTER.LOCAL = {
        kdc = frontal-01.cluster.local:88
        admin_server = frontal-01.cluster.local:749
    }

[domain_realm]
    .cluster.local = CLUSTER.LOCAL
    cluster.local = CLUSTER.LOCAL
```

**Utilisation** :
```bash
# Obtenir un ticket
kinit jdoe@CLUSTER.LOCAL
# Entrer le mot de passe

# VÃ©rifier le ticket
klist

# Renouveler le ticket
kinit -R

# DÃ©truire le ticket
kdestroy

# SSH sans mot de passe (si ticket valide)
ssh jdoe@node-01
# (pas besoin de mot de passe si ticket Kerberos valide)
```

**Maintenance** :
```bash
# VÃ©rification
systemctl status krb5kdc
kadmin.local -q "listprincs"

# CrÃ©er un principal
kadmin.local -q "addprinc jdoe@CLUSTER.LOCAL"

# Changer mot de passe
kadmin.local -q "change_password jdoe@CLUSTER.LOCAL"

# Logs
tail -f /var/log/krb5kdc.log
tail -f /var/log/kadmin.log
```

---

### 2.3 FreeIPA (Alternative)

#### Qu'est-ce que c'est ?

**FreeIPA** (Identity, Policy, and Audit) est une solution intÃ©grÃ©e qui combine :
- LDAP (389 Directory Server)
- Kerberos
- DNS
- PKI (Certificats)
- Gestion des politiques

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Solution unifiÃ©e** : Tout-en-un au lieu de configurer LDAP + Kerberos sÃ©parÃ©ment
- âœ… **Interface web** : Administration via interface graphique
- âœ… **Enterprise-ready** : Solution robuste pour production
- âœ… **Gestion des politiques** : ContrÃ´le d'accÃ¨s centralisÃ©
- âœ… **DNS intÃ©grÃ©** : Gestion DNS automatique

**InconvÃ©nients** :
- âŒ **Plus complexe** : Plus de composants, plus de dÃ©pendances
- âŒ **Moins flexible** : Tout-en-un, moins de contrÃ´le granulaire

**Quand utiliser FreeIPA vs LDAP+Kerberos** :
- **FreeIPA** : Si vous voulez une solution complÃ¨te avec interface web
- **LDAP+Kerberos** : Si vous voulez plus de contrÃ´le et simplicitÃ©

**Installation** :
```bash
./scripts/install-freeipa.sh
```

**Utilisation** :
```bash
# Interface web
https://frontal-01.cluster.local

# Commandes
ipa user-add jdoe
ipa user-mod jdoe --password
ipa group-add hpc-users
ipa group-add-member hpc-users --users=jdoe
```

---

## 3. Scheduler et Gestion des Jobs

### 3.1 Slurm (Workload Manager)

#### Qu'est-ce que c'est ?

**Slurm** (Simple Linux Utility for Resource Management) est un gestionnaire de jobs et de ressources pour clusters HPC.

**Composants** :
- **SlurmCTLD** : ContrÃ´leur (sur frontal-01)
- **SlurmDBD** : Base de donnÃ©es des jobs (optionnel)
- **SlurmD** : Daemon sur chaque nÅ“ud de calcul
- **squeue** : Affiche la file d'attente
- **sinfo** : Affiche les nÅ“uds
- **sbatch** : Soumet un job
- **srun** : Lance un job interactif
- **scancel** : Annule un job

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Standard HPC** : UtilisÃ© par 60%+ des clusters mondiaux (Top500)
- âœ… **Gestion ressources** : CPU, mÃ©moire, GPU, partitions, QOS
- âœ… **File d'attente intelligente** : PrioritÃ©, backfill, preemption
- âœ… **Multi-utilisateurs** : Partage Ã©quitable des ressources
- âœ… **Open-source** : Gratuit, communautÃ© trÃ¨s active
- âœ… **Performant** : GÃ¨re des milliers de nÅ“uds, millions de jobs
- âœ… **FonctionnalitÃ©s avancÃ©es** : Checkpointing, job arrays, dependencies

**Alternatives considÃ©rÃ©es** :
- âŒ **PBS/Torque** : Moins moderne, moins de fonctionnalitÃ©s, communautÃ© rÃ©duite
- âŒ **LSF (IBM)** : Commercial, licence coÃ»teuse, moins flexible
- âŒ **SGE (Sun Grid Engine)** : Moins maintenu, communautÃ© rÃ©duite
- âŒ **Kubernetes** : Pas optimisÃ© pour HPC, plus pour microservices

**Pourquoi Slurm a Ã©tÃ© choisi** :
- Standard de facto pour HPC moderne
- TrÃ¨s bien documentÃ©
- Performant et fiable
- Supporte toutes les fonctionnalitÃ©s nÃ©cessaires (GPU, MPI, etc.)
- CommunautÃ© Ã©norme et active
- UtilisÃ© par les plus grands clusters mondiaux

#### Comment Ã§a fonctionne ?

```
Utilisateur soumet un job
    â”‚
    â”œâ”€â–º sbatch job.sh
    â”‚
    â–¼
SlurmCTLD (Controller sur frontal-01)
    â”‚
    â”œâ”€â–º Parse le script (directives #SBATCH)
    â”‚   â”œâ”€â–º --nodes=2
    â”‚   â”œâ”€â–º --ntasks-per-node=16
    â”‚   â”œâ”€â–º --time=01:00:00
    â”‚   â””â”€â–º --partition=normal
    â”‚
    â”œâ”€â–º VÃ©rifie ressources disponibles
    â”‚   â”œâ”€â–º CPU disponible ?
    â”‚   â”œâ”€â–º RAM disponible ?
    â”‚   â”œâ”€â–º Partition disponible ?
    â”‚   â””â”€â–º QOS disponible ?
    â”‚
    â”œâ”€â–º Calcule prioritÃ©
    â”‚   â”œâ”€â–º Fair-share (utilisation historique)
    â”‚   â”œâ”€â–º Age (temps d'attente)
    â”‚   â””â”€â–º Partition priority
    â”‚
    â”œâ”€â–º Place dans file d'attente
    â”‚   â””â”€â–º Ordre par prioritÃ©
    â”‚
    â”œâ”€â–º Quand ressources disponibles :
    â”‚   â”œâ”€â–º SÃ©lectionne nÅ“uds (best-fit)
    â”‚   â”œâ”€â–º Alloue ressources (cgroups)
    â”‚   â””â”€â–º Lance le job
    â”‚       â”‚
    â”‚       â–¼
    â”‚   SlurmD (Daemon sur compute-01, compute-02, etc.)
    â”‚       â”‚
    â”‚       â”œâ”€â–º CrÃ©e environnement
    â”‚       â”œâ”€â–º Alloue CPU, RAM (cgroups)
    â”‚       â”œâ”€â–º Lance le job
    â”‚       â”œâ”€â–º Collecte mÃ©triques
    â”‚       â””â”€â–º Notifie SlurmCTLD (fin, erreur)
    â”‚
    â””â”€â–º SlurmCTLD met Ã  jour Ã©tat
        â””â”€â–º SlurmDBD (enregistre dans base de donnÃ©es)
```

**Fichiers de configuration** :
- `configs/slurm/slurm.conf` : Configuration principale
- `configs/slurm/cgroup.conf` : Gestion des ressources (cgroups)

**Exemple de job** :
```bash
#!/bin/bash
#SBATCH --job-name=myjob
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=16
#SBATCH --time=01:00:00
#SBATCH --partition=normal
#SBATCH --output=myjob.%j.out
#SBATCH --error=myjob.%j.err

# Charger modules
module load openmpi/4.1.5
module load gromacs/2023.2

# Lancer le job
mpirun -np 32 gmx mdrun -s topol.tpr -v
```

**Utilisation** :
```bash
# Soumettre un job
sbatch job.sh

# Voir les jobs
squeue
squeue -u $USER
squeue -j JOBID

# Voir les nÅ“uds
sinfo
sinfo -N -l

# Voir les partitions
sinfo -p normal

# Job interactif
srun --pty bash

# Annuler un job
scancel JOBID
scancel -u $USER

# Voir historique
sacct
sacct -j JOBID
```

**Maintenance** :
```bash
# VÃ©rification
systemctl status slurmctld
systemctl status slurmd

# ContrÃ´le
scontrol show nodes
scontrol show partition normal
scontrol show job JOBID

# Recharger configuration
scontrol reconfigure

# Logs
tail -f /var/log/slurmctld.log
tail -f /var/log/slurmd.log
```

---

## 4. Stockage

### 4.1 BeeGFS

#### Qu'est-ce que c'est ?

**BeeGFS** (formerly FhGFS) est un systÃ¨me de fichiers parallÃ¨le open-source optimisÃ© pour HPC et calcul haute performance.

**Composants** :
- **MGMtd** : Gestionnaire de mÃ©tadonnÃ©es (Management)
- **Metadata Servers** : Serveurs de mÃ©tadonnÃ©es (optionnel, pour scalabilitÃ©)
- **Storage Targets** : Serveurs de stockage (donnÃ©es)
- **Client** : Sur chaque nÅ“ud (compute-01 Ã  compute-06)

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Performance** : TrÃ¨s rapide pour HPC (I/O parallÃ¨le)
- âœ… **ScalabilitÃ©** : Supporte des milliers de nÅ“uds, pÃ©taoctets
- âœ… **Open-source** : Gratuit, pas de licence
- âœ… **FacilitÃ©** : Plus simple que Lustre
- âœ… **FlexibilitÃ©** : Peut dÃ©marrer petit et grandir
- âœ… **Haute disponibilitÃ©** : Redondance possible

**Alternatives considÃ©rÃ©es** :
- âŒ **GPFS (IBM Spectrum Scale)** : Commercial, licence coÃ»teuse (plusieurs dizaines de milliers d'euros)
- âŒ **Lustre** : Plus complexe, nÃ©cessite plus de ressources, plus difficile Ã  maintenir
- âŒ **GlusterFS** : Moins performant pour HPC, plus pour stockage distribuÃ© gÃ©nÃ©ral
- âŒ **NFS** : Trop lent pour HPC, pas parallÃ¨le
- âŒ **CephFS** : Plus pour cloud, moins optimisÃ© HPC

**Pourquoi BeeGFS a Ã©tÃ© choisi** :
- Meilleur compromis performance/facilitÃ©
- Open-source pur (pas de licence)
- Bien documentÃ©
- CommunautÃ© active
- Performances excellentes pour HPC
- Plus simple que Lustre

#### Comment Ã§a fonctionne ?

```
Application (sur compute-01)
    â”‚
    â”œâ”€â–º Ã‰criture fichier /mnt/beegfs/data.txt
    â”‚
    â–¼
BeeGFS Client
    â”‚
    â”œâ”€â–º Contacte MGMtd (mÃ©tadonnÃ©es)
    â”‚   â””â”€â–º "OÃ¹ stocker data.txt ?"
    â”‚
    â”œâ”€â–º MGMtd retourne localisation
    â”‚   â””â”€â–º "Storage Target 1 (frontal-01), Target 2 (frontal-02)"
    â”‚
    â”œâ”€â–º Client Ã©crit directement sur Storage Targets
    â”‚   â”œâ”€â–º Target 1 (frontal-01) : Stripe 1
    â”‚   â”œâ”€â–º Target 2 (frontal-02) : Stripe 2
    â”‚   â””â”€â–º I/O parallÃ¨le (performance Ã©levÃ©e)
    â”‚
    â””â”€â–º Performance Ã©levÃ©e (bande passante agrÃ©gÃ©e)
```

**Architecture** :
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MGMtd     â”‚  â† Management (mÃ©tadonnÃ©es)
â”‚ (frontal-01)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Storage    â”‚  â† Storage Targets (donnÃ©es)
â”‚  Targets    â”‚
â”‚ (frontal-01)â”‚
â”‚ (frontal-02)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Clients   â”‚  â† Tous les nÅ“uds
â”‚ (compute-01)â”‚
â”‚ (compute-02)â”‚
â”‚     ...     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Installation** :
```bash
./scripts/storage/install-beegfs.sh
```

**Configuration** :
```bash
# Configuration MGMtd (frontal-01)
/opt/beegfs/sbin/beegfs-setup-mgmtd -p /mnt/beegfs-mgmtd

# Configuration Storage Target (frontal-01, frontal-02)
/opt/beegfs/sbin/beegfs-setup-storage -p /mnt/beegfs-storage -s 1

# Configuration Client (compute-01 Ã  compute-06)
/opt/beegfs/sbin/beegfs-setup-client -m frontal-01

# DÃ©marrer
systemctl start beegfs-mgmtd
systemctl start beegfs-storage
systemctl start beegfs-client

# Monter
mount -t beegfs beegfs /mnt/beegfs
```

**Utilisation** :
```bash
# VÃ©rifier
df -h /mnt/beegfs
beegfs-ctl --listnodes --nodetype=storage
beegfs-ctl --listnodes --nodetype=client

# Performance test
dd if=/dev/zero of=/mnt/beegfs/test bs=1M count=1000
```

**Maintenance** :
```bash
# Statut
systemctl status beegfs-mgmtd
systemctl status beegfs-storage
systemctl status beegfs-client

# Logs
tail -f /var/log/beegfs-mgmtd.log
tail -f /var/log/beegfs-storage.log
```

---

### 4.2 Lustre (Alternative)

#### Qu'est-ce que c'est ?

**Lustre** est un systÃ¨me de fichiers parallÃ¨le open-source, trÃ¨s performant mais plus complexe que BeeGFS.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **TrÃ¨s performant** : UtilisÃ© par les plus grands clusters (Top500)
- âœ… **Scalable** : Supporte exaoctets
- âœ… **Open-source** : Gratuit

**InconvÃ©nients** :
- âŒ **Plus complexe** : Configuration plus difficile
- âŒ **Plus de ressources** : NÃ©cessite plus de serveurs

**Quand utiliser Lustre vs BeeGFS** :
- **Lustre** : Si vous avez besoin de performances maximales et beaucoup de ressources
- **BeeGFS** : Si vous voulez simplicitÃ© et bonnes performances

**Installation** :
```bash
./scripts/storage/install-lustre.sh
```

---

## 5. Monitoring et ObservabilitÃ©

### 5.1 Prometheus

#### Qu'est-ce que c'est ?

**Prometheus** est un systÃ¨me de monitoring et d'alerting open-source, spÃ©cialisÃ© dans les mÃ©triques time-series.

**Concepts clÃ©s** :
- **Pull-based** : Prometheus "scrape" (tire) les mÃ©triques
- **Time-series** : Base de donnÃ©es optimisÃ©e pour sÃ©ries temporelles
- **PromQL** : Langage de requÃªte puissant
- **Alerting** : SystÃ¨me d'alertes intÃ©grÃ©
- **Exporters** : Agents qui exposent des mÃ©triques

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Pull-based** : Plus efficace que push (pas de perte si exporter down)
- âœ… **Time-series** : Base de donnÃ©es optimisÃ©e pour mÃ©triques
- âœ… **PromQL** : Langage de requÃªte trÃ¨s puissant
- âœ… **Alerting** : SystÃ¨me d'alertes intÃ©grÃ©
- âœ… **Open-source** : Gratuit, communautÃ© Ã©norme
- âœ… **Standard** : De facto pour monitoring moderne
- âœ… **Ã‰cosystÃ¨me** : Des centaines d'exporters disponibles

**Alternatives considÃ©rÃ©es** :
- âŒ **Nagios** : Plus ancien, push-based, moins moderne
- âŒ **Zabbix** : Plus complexe, interface moins intuitive
- âŒ **Datadog/New Relic** : Commercial, coÃ»teux (plusieurs centaines d'euros/mois)
- âŒ **InfluxDB seul** : Pas de systÃ¨me d'alertes intÃ©grÃ©

**Pourquoi Prometheus a Ã©tÃ© choisi** :
- Standard de facto pour monitoring moderne
- TrÃ¨s performant (millions de mÃ©triques)
- Excellent pour mÃ©triques time-series
- Ã‰cosystÃ¨me Ã©norme (exporters pour tout)
- PromQL trÃ¨s puissant
- Alerting intÃ©grÃ©

#### Comment Ã§a fonctionne ?

```
Prometheus Server
    â”‚
    â”œâ”€â–º Scrape (toutes les 15s)
    â”‚   â”œâ”€â–º Node Exporter (port 9100)
    â”‚   â”‚   â””â”€â–º MÃ©triques systÃ¨me
    â”‚   â”‚       â”œâ”€â–º CPU (node_cpu_seconds_total)
    â”‚   â”‚       â”œâ”€â–º RAM (node_memory_MemTotal_bytes)
    â”‚   â”‚       â”œâ”€â–º Disk (node_filesystem_size_bytes)
    â”‚   â”‚       â””â”€â–º Network (node_network_receive_bytes_total)
    â”‚   â”‚
    â”‚   â”œâ”€â–º Telegraf (port 9273)
    â”‚   â”‚   â””â”€â–º MÃ©triques applicatives
    â”‚   â”‚
    â”‚   â””â”€â–º Slurm Exporter
    â”‚       â””â”€â–º MÃ©triques Slurm
    â”‚           â”œâ”€â–º Jobs (slurm_jobs_total)
    â”‚           â””â”€â–º Partitions (slurm_partition_nodes)
    â”‚
    â”œâ”€â–º Stockage (TSDB)
    â”‚   â””â”€â–º RÃ©tention 30 jours
    â”‚
    â”œâ”€â–º Ã‰valuation rÃ¨gles (toutes les 15s)
    â”‚   â””â”€â–º Alertes si seuils dÃ©passÃ©s
    â”‚
    â””â”€â–º API HTTP (port 9090)
        â””â”€â–º RequÃªtes PromQL
```

**Configuration** :
- `configs/prometheus/prometheus.yml` : Configuration scraping
- `configs/prometheus/alerts.yml` : RÃ¨gles d'alertes

**Exemple de configuration** :
```yaml
# configs/prometheus/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'frontal-01-node'
    static_configs:
      - targets: ['172.20.0.101:9100']
        labels:
          role: 'frontal'
          node: 'frontal-01'
```

**Utilisation** :
```bash
# AccÃ¨s web
http://localhost:9090

# RequÃªte PromQL
up{job="frontal-01-node"}
node_cpu_seconds_total{mode="idle"}
rate(node_network_receive_bytes_total[5m])
```

**Maintenance** :
```bash
# VÃ©rification
curl http://localhost:9090/-/healthy

# Reload configuration
curl -X POST http://localhost:9090/-/reload

# Logs
docker logs hpc-prometheus
```

---

### 5.2 Grafana

#### Qu'est-ce que c'est ?

**Grafana** est une plateforme de visualisation et d'analyse de mÃ©triques.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Dashboards** : Visualisation graphique des mÃ©triques
- âœ… **Multi-sources** : Prometheus, InfluxDB, Elasticsearch, etc.
- âœ… **Alerting** : Alertes visuelles
- âœ… **Templates** : Dashboards prÃ©-configurÃ©s (54+ dans ce projet)
- âœ… **Open-source** : Gratuit
- âœ… **Standard** : De facto pour visualisation mÃ©triques

**Alternatives considÃ©rÃ©es** :
- âŒ **Kibana** : Principalement pour logs (ELK), moins pour mÃ©triques
- âŒ **Custom HTML** : Trop de travail, maintenance difficile
- âŒ **Tableaux de bord propriÃ©taires** : CoÃ»teux, moins flexible

**Pourquoi Grafana a Ã©tÃ© choisi** :
- Standard pour visualisation mÃ©triques
- Interface intuitive et moderne
- Large communautÃ© de dashboards
- IntÃ©gration native avec Prometheus
- Open-source

#### Comment Ã§a fonctionne ?

```
Grafana Server
    â”‚
    â”œâ”€â–º Datasource : Prometheus
    â”‚   â””â”€â–º http://172.20.0.10:9090
    â”‚
    â”œâ”€â–º Dashboards (54+)
    â”‚   â”œâ”€â–º HPC Cluster Overview
    â”‚   â”œâ”€â–º CPU/Memory by Node
    â”‚   â”œâ”€â–º Network I/O
    â”‚   â”œâ”€â–º Slurm Jobs
    â”‚   â”œâ”€â–º Slurm Partitions
    â”‚   â”œâ”€â–º Applications (Redis, RabbitMQ, etc.)
    â”‚   â”œâ”€â–º SÃ©curitÃ© (Firewall, IDS, Compliance)
    â”‚   â””â”€â–º 40+ autres...
    â”‚
    â””â”€â–º Alerting
        â””â”€â–º Notifications (email, webhook, Slack)
```

**Dashboards** :
- `grafana-dashboards/` : 54+ dashboards JSON
- Auto-provisioning via `configs/grafana/provisioning/dashboards/default.yml`

**Utilisation** :
```bash
# AccÃ¨s web
http://localhost:3000
# Login: admin / admin

# Dashboards disponibles
# - HPC Cluster Overview
# - CPU/Memory by Node
# - Network I/O
# - Slurm Jobs
# - Slurm Partitions
# - Applications (Redis, RabbitMQ, Kafka, etc.)
# - SÃ©curitÃ© (Firewall, IDS, Compliance)
# - Et 40+ autres...
```

**CrÃ©ation de dashboard** :
1. Aller dans Grafana : http://localhost:3000
2. CrÃ©er â†’ Dashboard
3. Ajouter panel
4. RequÃªte PromQL : `node_cpu_seconds_total{mode="idle"}`
5. Sauvegarder

**Maintenance** :
```bash
# VÃ©rification
curl http://localhost:3000/api/health

# Backup dashboards
cp -r grafana-dashboards/ backup/

# Logs
docker logs hpc-grafana
```

---

### 5.3 Telegraf

#### Qu'est-ce que c'est ?

**Telegraf** est un agent de collecte de mÃ©triques Ã©crit en Go, trÃ¨s lÃ©ger et performant.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **LÃ©ger** : Faible consommation ressources (< 50MB RAM)
- âœ… **Plugins** : 200+ plugins disponibles
- âœ… **Flexible** : Inputs et outputs multiples
- âœ… **Prometheus** : Exporte en format Prometheus
- âœ… **Performant** : Collecte trÃ¨s rapide

**Alternatives considÃ©rÃ©es** :
- âŒ **Collectd** : Moins moderne, moins de plugins
- âŒ **StatsD** : Uniquement UDP, moins flexible
- âŒ **Custom scripts** : Trop de maintenance

**Pourquoi Telegraf a Ã©tÃ© choisi** :
- Standard moderne
- TrÃ¨s performant
- Large Ã©cosystÃ¨me de plugins
- Export Prometheus natif
- LÃ©ger

#### Comment Ã§a fonctionne ?

```
Telegraf Agent (sur chaque nÅ“ud)
    â”‚
    â”œâ”€â–º Inputs (collecte toutes les 15s)
    â”‚   â”œâ”€â–º CPU
    â”‚   â”‚   â””â”€â–º cpu_usage_idle, cpu_usage_user, etc.
    â”‚   â”œâ”€â–º Memory
    â”‚   â”‚   â””â”€â–º mem_total, mem_available, etc.
    â”‚   â”œâ”€â–º Disk
    â”‚   â”‚   â””â”€â–º disk_used, disk_free, etc.
    â”‚   â”œâ”€â–º Network
    â”‚   â”‚   â””â”€â–º bytes_recv, bytes_sent, etc.
    â”‚   â””â”€â–º Processes
    â”‚       â””â”€â–º processes_total, etc.
    â”‚
    â”œâ”€â–º Processors (optionnel)
    â”‚   â””â”€â–º Tags, transformations
    â”‚
    â””â”€â–º Outputs (exposition)
        â””â”€â–º Prometheus Client (port 9273)
            â””â”€â–º Format Prometheus
            â””â”€â–º ScrapÃ© par Prometheus
```

**Configuration** :
- `configs/telegraf/telegraf-frontal.conf` : Pour nÅ“uds frontaux
- `configs/telegraf/telegraf-slave.conf` : Pour nÅ“uds de calcul

**Exemple de configuration** :
```toml
# configs/telegraf/telegraf-frontal.conf
[global_tags]
  cluster = "hpc-demo"
  role = "frontal"

[agent]
  interval = "15s"

[[inputs.cpu]]
  percpu = true
  totalcpu = true

[[inputs.mem]]

[[inputs.disk]]

[[outputs.prometheus_client]]
  listen = ":9273"
```

**Utilisation** :
```bash
# VÃ©rifier mÃ©triques
curl http://172.20.0.101:9273/metrics

# Logs
journalctl -u telegraf -f

# Test configuration
telegraf --test --config /etc/telegraf/telegraf.conf
```

**Maintenance** :
```bash
# VÃ©rification
systemctl status telegraf

# Reload configuration
systemctl reload telegraf

# Logs
tail -f /var/log/telegraf/telegraf.log
```

---

### 5.4 Node Exporter

#### Qu'est-ce que c'est ?

**Node Exporter** est un exporter Prometheus pour mÃ©triques systÃ¨me (CPU, RAM, Disk, Network).

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Standard** : Exporter officiel Prometheus
- âœ… **Complet** : Toutes les mÃ©triques systÃ¨me
- âœ… **LÃ©ger** : Faible consommation
- âœ… **Performant** : Collecte trÃ¨s rapide

#### Comment Ã§a fonctionne ?

```
Node Exporter (port 9100)
    â”‚
    â”œâ”€â–º Collecte mÃ©triques systÃ¨me
    â”‚   â”œâ”€â–º CPU (node_cpu_seconds_total)
    â”‚   â”œâ”€â–º Memory (node_memory_MemTotal_bytes)
    â”‚   â”œâ”€â–º Disk (node_filesystem_size_bytes)
    â”‚   â”œâ”€â–º Network (node_network_receive_bytes_total)
    â”‚   â””â”€â–º System (node_load1, node_uptime_seconds)
    â”‚
    â””â”€â–º Expose sur /metrics (format Prometheus)
        â””â”€â–º ScrapÃ© par Prometheus
```

**Utilisation** :
```bash
# VÃ©rifier mÃ©triques
curl http://172.20.0.101:9100/metrics

# AccÃ¨s web
http://172.20.0.101:9100
```

---

## 6. Applications Scientifiques

### 6.1 GROMACS

#### Qu'est-ce que c'est ?

**GROMACS** (GROningen MAchine for Chemical Simulations) est un logiciel de simulation molÃ©culaire.

**Utilisations** :
- Dynamique molÃ©culaire
- Simulation de protÃ©ines
- Simulation de membranes
- Ã‰tudes de mÃ©dicaments

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Standard** : UtilisÃ© par la majoritÃ© des chercheurs
- âœ… **Performant** : TrÃ¨s optimisÃ© (MPI, GPU)
- âœ… **Open-source** : Gratuit
- âœ… **Documentation** : TrÃ¨s bien documentÃ©

**Installation** :
```bash
./scripts/applications/install-gromacs.sh
```

**Utilisation** :
```bash
# Job Slurm
#!/bin/bash
#SBATCH --job-name=gromacs
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=16
#SBATCH --time=01:00:00

module load gromacs/2023.2

gmx mdrun -s topol.tpr -v
```

---

### 6.2 OpenFOAM

#### Qu'est-ce que c'est ?

**OpenFOAM** (Open Field Operation and Manipulation) est un logiciel CFD (Computational Fluid Dynamics).

**Utilisations** :
- Simulation de fluides
- AÃ©rodynamique
- Combustion
- Turbulence

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Standard CFD** : UtilisÃ© partout
- âœ… **Open-source** : Gratuit
- âœ… **Performant** : TrÃ¨s optimisÃ© (MPI)
- âœ… **Flexible** : Personnalisable

**Installation** :
```bash
./scripts/applications/install-openfoam.sh
```

**Utilisation** :
```bash
# Job Slurm
#!/bin/bash
#SBATCH --job-name=openfoam
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=8

module load openfoam/2312

mpirun -np 32 simpleFoam -parallel
```

---

### 6.3 Quantum ESPRESSO

#### Qu'est-ce que c'est ?

**Quantum ESPRESSO** est un logiciel de calculs quantiques (DFT - Density Functional Theory).

**Utilisations** :
- Ã‰lectronique de structure
- PropriÃ©tÃ©s de matÃ©riaux
- Catalyse
- Physique du solide

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Standard** : UtilisÃ© par la majoritÃ© des chercheurs
- âœ… **Open-source** : Gratuit
- âœ… **Performant** : TrÃ¨s optimisÃ© (MPI)
- âœ… **Complet** : Tous les outils nÃ©cessaires

**Installation** :
```bash
./scripts/applications/install-quantum-espresso.sh
```

**Utilisation** :
```bash
# Job Slurm
#!/bin/bash
#SBATCH --job-name=qe
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=16

module load quantum-espresso/7.2

mpirun -np 32 pw.x < input.in > output.out
```

---

### 6.4 ParaView

#### Qu'est-ce que c'est ?

**ParaView** est un logiciel de visualisation scientifique.

**Utilisations** :
- Visualisation de donnÃ©es HPC
- Visualisation de rÃ©sultats de simulation
- Analyse de donnÃ©es volumÃ©triques

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Standard** : UtilisÃ© partout
- âœ… **Open-source** : Gratuit
- âœ… **Performant** : ParallÃ¨le (MPI)
- âœ… **Puissant** : Beaucoup de fonctionnalitÃ©s

**Installation** :
```bash
./scripts/applications/install-paraview.sh
```

**Utilisation** :
```bash
# Via X2Go (remote graphics)
paraview

# Ou batch
pvpython script.py
```

---

## 7. Bases de DonnÃ©es

### 7.1 PostgreSQL

#### Qu'est-ce que c'est ?

**PostgreSQL** est une base de donnÃ©es relationnelle open-source, ACID compliant.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Standard SQL** : Compatible avec tout
- âœ… **Performant** : TrÃ¨s rapide
- âœ… **Open-source** : Gratuit
- âœ… **Extensible** : Beaucoup d'extensions

**Utilisation dans le cluster** :
- SlurmDBD (base de donnÃ©es des jobs Slurm)
- Applications nÃ©cessitant une base SQL

**Installation** :
```bash
./scripts/database/install-postgresql.sh
```

**Utilisation** :
```bash
# Connexion
psql -U slurm -d slurm_acct_db

# RequÃªtes
SELECT * FROM job_table;
SELECT * FROM job_table WHERE user_name = 'jdoe';
```

---

### 7.2 MongoDB

#### Qu'est-ce que c'est ?

**MongoDB** est une base de donnÃ©es NoSQL document-oriented.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Flexible** : SchÃ©ma dynamique
- âœ… **Scalable** : Horizontal scaling
- âœ… **Open-source** : Gratuit
- âœ… **Performant** : TrÃ¨s rapide

**Utilisation dans le cluster** :
- DonnÃ©es non-structurÃ©es
- Logs structurÃ©s
- MÃ©tadonnÃ©es applications

**Installation** :
```bash
./scripts/database/install-mongodb.sh
```

**Utilisation** :
```bash
# Connexion
mongo

# RequÃªtes
db.collection.find()
db.collection.insert({name: "test"})
```

---

### 7.3 InfluxDB

#### Qu'est-ce que c'est ?

**InfluxDB** est une base de donnÃ©es time-series optimisÃ©e pour mÃ©triques.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **OptimisÃ© time-series** : TrÃ¨s performant pour mÃ©triques
- âœ… **Open-source** : Gratuit
- âœ… **Compatible Telegraf** : IntÃ©gration native
- âœ… **Retention policies** : Gestion automatique

**Utilisation dans le cluster** :
- Stockage mÃ©triques (alternative Ã  Prometheus TSDB)
- Longue rÃ©tention

**Installation** :
```bash
./scripts/database/install-influxdb.sh
```

**Utilisation** :
```bash
# AccÃ¨s web
http://localhost:8086

# RequÃªtes InfluxQL
SELECT * FROM cpu WHERE time > now() - 1h
```

---

## 8. Messaging et Streaming

### 8.1 RabbitMQ

#### Qu'est-ce que c'est ?

**RabbitMQ** est un message broker open-source, implÃ©mentation AMQP.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Standard AMQP** : Protocole standardisÃ©
- âœ… **Performant** : TrÃ¨s rapide
- âœ… **Open-source** : Gratuit
- âœ… **Haute disponibilitÃ©** : Clustering

**Utilisation dans le cluster** :
- Communication asynchrone entre services
- File d'attente de messages
- DÃ©couplage de services

**Installation** :
```bash
./scripts/messaging/install-rabbitmq-complete.sh
```

**Utilisation** :
```bash
# AccÃ¨s web
http://localhost:15672
# Login: guest / guest

# Publier message
rabbitmqadmin publish exchange=amq.default routing_key=test payload="Hello"

# Consommer
rabbitmqadmin get queue=test
```

---

### 8.2 Kafka

#### Qu'est-ce que c'est ?

**Kafka** est une plateforme de streaming d'Ã©vÃ©nements.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **TrÃ¨s performant** : Millions de messages/seconde
- âœ… **Scalable** : Horizontal scaling
- âœ… **Open-source** : Gratuit
- âœ… **Standard Big Data** : UtilisÃ© partout

**Utilisation dans le cluster** :
- Streaming de donnÃ©es
- Event sourcing
- Big Data pipelines

**Installation** :
```bash
./scripts/streaming/install-kafka-complete.sh
```

**Utilisation** :
```bash
# CrÃ©er topic
kafka-topics.sh --create --topic test --bootstrap-server localhost:9092

# Publier
kafka-console-producer.sh --topic test --bootstrap-server localhost:9092

# Consommer
kafka-console-consumer.sh --topic test --bootstrap-server localhost:9092
```

---

## 9. Big Data et Machine Learning

### 9.1 Apache Spark

#### Qu'est-ce que c'est ?

**Apache Spark** est un framework de processing distribuÃ© pour Big Data.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **TrÃ¨s performant** : In-memory processing
- âœ… **Scalable** : Horizontal scaling
- âœ… **Open-source** : Gratuit
- âœ… **Standard Big Data** : UtilisÃ© partout

**Installation** :
```bash
./scripts/bigdata/install-spark.sh
```

**Utilisation** :
```bash
# Spark Shell
spark-shell

# Job Spark
spark-submit --class MyApp --master yarn myapp.jar
```

---

### 9.2 TensorFlow

#### Qu'est-ce que c'est ?

**TensorFlow** est un framework deep learning dÃ©veloppÃ© par Google.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Standard** : UtilisÃ© partout
- âœ… **Performant** : GPU support
- âœ… **Open-source** : Gratuit
- âœ… **Large communautÃ©** : Beaucoup de ressources

**Installation** :
```bash
./scripts/ml/install-tensorflow.sh
```

**Utilisation** :
```python
import tensorflow as tf

model = tf.keras.Sequential([...])
model.fit(x_train, y_train)
```

---

### 9.3 PyTorch

#### Qu'est-ce que c'est ?

**PyTorch** est un framework deep learning dÃ©veloppÃ© par Facebook.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Flexible** : Plus flexible que TensorFlow
- âœ… **Performant** : GPU support
- âœ… **Open-source** : Gratuit
- âœ… **Popular en recherche** : UtilisÃ© par beaucoup de chercheurs

**Installation** :
```bash
./scripts/ml/install-pytorch.sh
```

**Utilisation** :
```python
import torch

model = torch.nn.Sequential(...)
loss = criterion(output, target)
```

---

## 10. CI/CD et Automatisation

### 10.1 GitLab CI

#### Qu'est-ce que c'est ?

**GitLab CI** est un systÃ¨me d'intÃ©gration continue intÃ©grÃ© Ã  GitLab.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **IntÃ©grÃ©** : Dans GitLab
- âœ… **Flexible** : Pipelines configurables
- âœ… **Open-source** : Gratuit
- âœ… **Performant** : Runners distribuÃ©s

**Installation** :
```bash
./scripts/ci-cd/install-gitlab-ci.sh
```

---

### 10.2 Terraform

#### Qu'est-ce que c'est ?

**Terraform** est un outil Infrastructure as Code (IaC).

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **IaC** : Infrastructure versionnÃ©e
- âœ… **Multi-cloud** : Fonctionne partout
- âœ… **Open-source** : Gratuit
- âœ… **Standard** : UtilisÃ© partout

**Installation** :
```bash
./scripts/iac/install-terraform.sh
```

---

## 11. Orchestration et Conteneurs

### 11.1 Kubernetes

#### Qu'est-ce que c'est ?

**Kubernetes** est un orchestrateur de conteneurs.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Standard** : De facto pour orchestration
- âœ… **Scalable** : Auto-scaling
- âœ… **Open-source** : Gratuit
- âœ… **Ã‰cosystÃ¨me** : TrÃ¨s large

**Installation** :
```bash
./scripts/orchestration/install-kubernetes-complete.sh
```

---

## 12. SÃ©curitÃ© AvancÃ©e

### 12.1 Vault

#### Qu'est-ce que c'est ?

**Vault** (HashiCorp) est un gestionnaire de secrets.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Gestion secrets** : CentralisÃ©e
- âœ… **Rotation** : Automatique
- âœ… **Audit** : TraÃ§abilitÃ©
- âœ… **Open-source** : Gratuit

**Installation** :
```bash
./scripts/security/install-vault.sh
```

---

### 12.2 Falco

#### Qu'est-ce que c'est ?

**Falco** est un runtime security monitoring pour conteneurs.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Runtime security** : DÃ©tection d'anomalies
- âœ… **Conteneurs** : SpÃ©cialisÃ© conteneurs
- âœ… **Open-source** : Gratuit
- âœ… **Performant** : Faible overhead

**Installation** :
```bash
./scripts/security/install-falco.sh
```

---

## 13. Remote Graphics

### 13.1 X2Go

#### Qu'est-ce que c'est ?

**X2Go** est une solution de remote graphics open-source.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **100% Gratuit** : Open-source
- âœ… **Performance** : OptimisÃ© via SSH
- âœ… **SÃ©curitÃ©** : Chiffrement SSH
- âœ… **Multi-utilisateurs** : Plusieurs sessions

**Installation** :
```bash
./scripts/remote-graphics/install-x2go.sh
```

**Utilisation** :
```bash
# Client (Windows/Linux)
# TÃ©lÃ©charger X2Go Client
# Connexion : frontal-01, utilisateur, mot de passe
# Session : XFCE ou autre
```

---

### 13.2 NoMachine

#### Qu'est-ce que c'est ?

**NoMachine** est une alternative gratuite pour remote desktop.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Gratuit** : Version gratuite disponible
- âœ… **Performant** : TrÃ¨s rapide
- âœ… **Facile** : Interface simple

**Installation** :
```bash
./scripts/remote-graphics/install-nomachine.sh
```

**Utilisation** :
```bash
# Client
# Connexion : frontal-01:4000
```

---

## 14. Gestion de Packages

### 14.1 Spack

#### Qu'est-ce que c'est ?

**Spack** est un gestionnaire de packages scientifique pour HPC.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Packages scientifiques** : OpenMPI, HDF5, NetCDF, etc.
- âœ… **Optimisation** : Compilation optimisÃ©e
- âœ… **Versions multiples** : Plusieurs versions simultanÃ©es
- âœ… **Environnements** : Isolation

**Installation** :
```bash
./scripts/spack/configure-spack.sh
```

**Utilisation** :
```bash
# Installer package
spack install openmpi@4.1.5

# Charger
spack load openmpi

# Environnement
spack env create myenv
spack env activate myenv
```

---

### 14.2 Nexus Repository

#### Qu'est-ce que c'est ?

**Nexus Repository** est un gestionnaire de dÃ©pÃ´ts d'artefacts.

#### Pourquoi l'utiliser ?

**Avantages** :
- âœ… **Air-gapped** : Fonctionne sans Internet
- âœ… **Performance** : Cache local
- âœ… **SÃ©curitÃ©** : ContrÃ´le des packages
- âœ… **Audit** : TraÃ§abilitÃ©

**Installation** :
```bash
./scripts/artifacts/install-nexus.sh
```

**Utilisation** :
```bash
# AccÃ¨s web
http://localhost:8081

# Configuration pip
# ~/.pip/pip.conf
[global]
index-url = http://frontal-01:8081/repository/pypi-group/simple
```

---

## ğŸ“š Documentation ComplÃ©mentaire

Pour plus de dÃ©tails sur chaque technologie, voir :
- `docs/TECHNOLOGIES_CLUSTER.md` - Technologies dÃ©taillÃ©es
- `docs/GUIDE_APPLICATIONS_SCIENTIFIQUES_COMPLET.md` - Applications scientifiques
- `docs/GUIDE_MONITORING_COMPLET.md` - Monitoring complet
- `docs/GUIDE_SECURITE_AVANCEE.md` - SÃ©curitÃ© avancÃ©e

---

**Version**: 1.0  
**DerniÃ¨re mise Ã  jour**: 2024
