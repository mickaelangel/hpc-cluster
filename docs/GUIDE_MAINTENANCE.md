# Guide de Maintenance du Cluster HPC
## Proc√©dures Op√©rationnelles et D√©pannage

**Classification**: Documentation Op√©rationnelle  
**Public**: Administrateurs Syst√®me / Ing√©nieurs  
**Version**: 1.0

---

## üìã Table des Mati√®res

1. [Maintenance Pr√©ventive](#maintenance-pr√©ventive)
2. [Maintenance des Services](#maintenance-des-services)
3. [Monitoring et Alertes](#monitoring-et-alertes)
4. [Sauvegardes](#sauvegardes)
5. [Mises √† Jour](#mises-√†-jour)
6. [D√©pannage](#d√©pannage)
7. [Proc√©dures d'Urgence](#proc√©dures-durgence)

---

## üîß Maintenance Pr√©ventive

### V√©rifications Quotidiennes

```bash
#!/bin/bash
# Script de v√©rification quotidienne

echo "=== V√©rification Cluster HPC ==="

# 1. √âtat des services
echo "Services syst√®me:"
systemctl status slurmctld --no-pager
systemctl status dirsrv@cluster --no-pager
systemctl status krb5kdc --no-pager

# 2. √âtat des n≈ìuds
echo "√âtat des n≈ìuds:"
sinfo -N -l

# 3. Jobs en cours
echo "Jobs actifs:"
squeue

# 4. Utilisation disque
echo "Utilisation stockage:"
df -h /mnt/beegfs
df -h /mnt/lustre
beegfs-ctl --listquota /mnt/beegfs

# 5. Monitoring
echo "Services monitoring:"
systemctl status prometheus --no-pager
systemctl status grafana-server --no-pager
systemctl status telegraf --no-pager

# 6. R√©seau
echo "Connectivit√© r√©seau:"
ping -c 1 frontal-02
ping -c 1 node-01
```

### V√©rifications Hebdomadaires

- **Logs syst√®me** : V√©rifier les erreurs dans `/var/log`
- **Quotas utilisateurs** : V√©rifier les d√©passements
- **Sauvegardes** : V√©rifier que les sauvegardes sont √† jour
- **Mises √† jour de s√©curit√©** : V√©rifier les patches disponibles

### V√©rifications Mensuelles

- **Audit de s√©curit√©** : V√©rifier les acc√®s et permissions
- **Performance** : Analyser les m√©triques de performance
- **Capacit√©** : Planifier l'extension si n√©cessaire
- **Documentation** : Mettre √† jour la documentation

---

## üîÑ Maintenance des Services

### Slurm

#### Red√©marrage

```bash
# Arr√™ter
systemctl stop slurmctld
systemctl stop slurmd

# D√©marrer
systemctl start slurmctld
systemctl start slurmd

# V√©rifier
scontrol ping
sinfo
```

#### Recharger la Configuration

```bash
# Sans arr√™ter les jobs
scontrol reconfigure
```

#### Nettoyage des Jobs Zombies

```bash
# Lister les jobs zombies
squeue | grep Z

# Nettoyer
scancel --state=ZOMBIE
```

### LDAP (389 Directory Server)

#### Red√©marrage

```bash
systemctl restart dirsrv@cluster
```

#### V√©rification

```bash
ldapsearch -x -b "dc=cluster,dc=local" -s base
```

#### Indexation

```bash
# Reconstruire les index
db2index -n userRoot
```

### Kerberos

#### Red√©marrage

```bash
systemctl restart krb5kdc
systemctl restart kadmin
```

#### V√©rification

```bash
kadmin.local -q "listprincs"
```

#### Nettoyage des Tickets Expir√©s

```bash
# Les tickets expir√©s sont automatiquement nettoy√©s
# V√©rifier manuellement si n√©cessaire
klist -A
```

### Stockage Parall√®le (BeeGFS / Lustre)

#### V√©rification de l'√âtat BeeGFS

```bash
# √âtat services
systemctl status beegfs-mgmtd beegfs-meta beegfs-storage

# V√©rification montage
mountpoint -q /mnt/beegfs && echo "OK" || echo "FAIL"

# Quotas
beegfs-ctl --listquota /mnt/beegfs
```

#### V√©rification de l'√âtat Lustre

```bash
# √âtat services
systemctl status lustre

# V√©rification montage
mountpoint -q /mnt/lustre && echo "OK" || echo "FAIL"

# √âtat filesystem
lfs df -h
```

#### Red√©marrage BeeGFS

```bash
# Red√©marrer services
systemctl restart beegfs-mgmtd
systemctl restart beegfs-meta
systemctl restart beegfs-storage
```

#### Red√©marrage Lustre

```bash
# Red√©marrer services
systemctl restart lustre
```

### Monitoring (Prometheus, Grafana, Telegraf)

#### Prometheus

```bash
# Red√©marrage
systemctl restart prometheus

# V√©rification
curl http://localhost:9090/-/healthy

# Recharger la configuration
curl -X POST http://localhost:9090/-/reload
```

#### Grafana

```bash
# Red√©marrage
systemctl restart grafana-server

# V√©rification
curl http://localhost:3000/api/health
```

#### Telegraf

```bash
# Red√©marrage
systemctl restart telegraf

# Test de configuration
telegraf --config /etc/telegraf/telegraf.conf --test
```

---

## üìä Monitoring et Alertes

### M√©triques √† Surveiller

1. **CPU** : Utilisation > 95%
2. **M√©moire** : Utilisation > 90%
3. **Disque** : Utilisation > 85%
4. **R√©seau** : Erreurs > 0.1%
5. **Services** : √âtat != running
6. **Jobs** : √âchecs > 5%

### Configuration des Alertes

**Prometheus Alerts** (`monitoring/prometheus/alerts.yml`) :
```yaml
groups:
  - name: hpc_cluster
    rules:
      - alert: HighCPUUsage
        expr: node_cpu_usage > 0.95
        for: 5m
        annotations:
          summary: "CPU usage is above 95%"
      
      - alert: HighMemoryUsage
        expr: node_memory_usage > 0.90
        for: 5m
        annotations:
          summary: "Memory usage is above 90%"
```

### Dashboard Grafana

Acc√©der √† `http://frontal-01:3000` pour visualiser :
- √âtat du cluster
- Utilisation des ressources
- Jobs actifs
- M√©triques BeeGFS/Lustre

---

## üíæ Sauvegardes

### Sauvegarde LDAP

```bash
#!/bin/bash
# Script de sauvegarde LDAP quotidienne

BACKUP_DIR="/backup/ldap"
DATE=$(date +%Y%m%d)

# Export LDIF
ldapsearch -x -b "dc=cluster,dc=local" > \
    ${BACKUP_DIR}/ldap_backup_${DATE}.ldif

# Compression
gzip ${BACKUP_DIR}/ldap_backup_${DATE}.ldif

# Conservation 30 jours
find ${BACKUP_DIR} -name "ldap_backup_*.ldif.gz" -mtime +30 -delete
```

### Sauvegarde Kerberos

```bash
#!/bin/bash
# Sauvegarde base de donn√©es Kerberos

BACKUP_DIR="/backup/kerberos"
DATE=$(date +%Y%m%d)

# Dump de la base
kdb5_util dump ${BACKUP_DIR}/krb5_dump_${DATE}

# Compression
gzip ${BACKUP_DIR}/krb5_dump_${DATE}
```

### Sauvegarde GPFS

```bash
# GPFS a sa propre r√©plication
# V√©rifier la r√©plication
mmlsfs gpfsfs1 -Y | grep replication

# Sauvegarde des m√©tadonn√©es
mmbackup config /backup/gpfs/config_$(date +%Y%m%d).tar.gz
```

### Sauvegarde Configuration Slurm

```bash
#!/bin/bash
# Sauvegarde configuration Slurm

BACKUP_DIR="/backup/slurm"
DATE=$(date +%Y%m%d)

tar czf ${BACKUP_DIR}/slurm_config_${DATE}.tar.gz \
    /etc/slurm/ \
    /var/spool/slurmctld/
```

---

## üîÑ Mises √† Jour

### Mise √† Jour SUSE

```bash
# V√©rifier les mises √† jour
zypper list-updates

# Mise √† jour de s√©curit√©
zypper patch --security

# Mise √† jour compl√®te
zypper update
```

### Mise √† Jour Slurm

```bash
# Arr√™ter Slurm
systemctl stop slurmctld
systemctl stop slurmd

# Mise √† jour
zypper update slurm

# Red√©marrer
systemctl start slurmctld
systemctl start slurmd
```

### Mise √† Jour Prometheus/Grafana

```bash
# Prometheus
systemctl stop prometheus
docker pull prom/prometheus:latest
# Mettre √† jour docker-compose.yml
docker-compose up -d prometheus

# Grafana
systemctl stop grafana-server
zypper update grafana
systemctl start grafana-server
```

---

## üîß D√©pannage

### Probl√®me: N≈ìud Inaccessible

```bash
# V√©rifier la connectivit√© r√©seau
ping node-01

# V√©rifier SSH
ssh node-01 "echo 'OK'"

# V√©rifier l'√©tat Slurm
scontrol show node node-01

# Red√©marrer le daemon Slurm
ssh node-01 "systemctl restart slurmd"
```

### Probl√®me: Jobs en √âchec

```bash
# Voir les logs du job
scontrol show job <job_id>

# Voir les logs syst√®me
journalctl -u slurmd -n 100

# V√©rifier les ressources
sinfo -N -l
```

### Probl√®me: GPFS Non Mont√©

```bash
# V√©rifier l'√©tat
mmgetstate -a
mmlsfs gpfsfs1

# Red√©marrer GPFS
mmstartup -a

# Monter manuellement
mmmount gpfsfs1 -a
```

### Probl√®me: Authentification √âchou√©e

```bash
# V√©rifier LDAP
ldapsearch -x -b "dc=cluster,dc=local" -s base

# V√©rifier Kerberos
kinit test@CLUSTER.LOCAL

# V√©rifier les logs
tail -f /var/log/dirsrv/slapd-cluster/errors
tail -f /var/log/krb5kdc.log
```

---

## üö® Proc√©dures d'Urgence

### Panne d'un N≈ìud Frontal

1. **Basculer sur le secondaire** :
   ```bash
   # Sur frontal-02
   systemctl start slurmctld
   ```

2. **Mettre √† jour la configuration** :
   ```bash
   # Modifier slurm.conf pour pointer vers frontal-02
   ```

3. **Red√©marrer les services** :
   ```bash
   scontrol reconfigure
   ```

### Panne Stockage

1. **V√©rifier l'√©tat** :
   ```bash
   mmgetstate -a
   mmhealth cluster show
   ```

2. **Red√©marrer** :
   ```bash
   mmstartup -a
   ```

3. **V√©rifier les disques** :
   ```bash
   # BeeGFS
   beegfs-ctl --listnodes
   # Lustre
   lfs df -h
   ```

### Perte de Donn√©es LDAP

1. **Arr√™ter le service** :
   ```bash
   systemctl stop dirsrv@cluster
   ```

2. **Restauration** :
   ```bash
   # Restaurer depuis la sauvegarde
   ldapadd -x -D "cn=Directory Manager" -w "password" \
       -f /backup/ldap/ldap_backup_YYYYMMDD.ldif
   ```

3. **Red√©marrer** :
   ```bash
   systemctl start dirsrv@cluster
   ```

---

## üìö Ressources

- **Slurm**: https://slurm.schedmd.com/troubleshooting.html
- **BeeGFS**: https://www.beegfs.io/documentation/
- **Lustre**: https://wiki.lustre.org/
- **LDAP**: https://directory.fedoraproject.org/docs/
- **Kerberos**: https://web.mit.edu/kerberos/krb5-latest/doc/

---

**Version**: 1.0  
**Derni√®re mise √† jour**: 2024
