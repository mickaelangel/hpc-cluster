# üèõÔ∏è Dictionnaire encyclop√©dique HPC

> **Standard de l'ouvrage ‚Äî Niveau Doctorat / Architecte ‚Äî Format encyclop√©dique rigoureux**

---

## Pr√©sentation du dictionnaire

Ce dictionnaire adopte un **format encyclop√©dique** √† entr√©es d√©taill√©es. Chaque entr√©e majeure suit la structure ci-dessous, permettant une lecture √† la fois **rigoureuse** et **op√©rationnelle**.

**Structure type d'une entr√©e :**

| Section | Contenu |
|--------|--------|
| **D√©finition rigoureuse** | D√©finition pr√©cise, sans ambigu√Øt√©. |
| **Pourquoi c'est important** | Impact sur les performances, les co√ªts ou l'architecture. |
| **Comment √ßa marche (internals)** | M√©canismes internes (algorithmes, couches logicielles, mat√©riel). |
| **Bonnes pratiques / Mauvaises pratiques** | Ce qu'il faut faire et √©viter. |
| **Commandes / outils associ√©s** | Outils de diagnostic et de configuration. |
| **Param√®tres & tuning** | Fichiers de config, variables, r√©glages typiques. |
| **Troubleshooting rapide** | Sympt√¥mes ‚Üí Causes ‚Üí Actions. |
| **Renvois crois√©s** | Voir aussi (autres entr√©es du dictionnaire ou du manuel). |
| **R√©f√©rences** | Articles, livres, documentation officielle. |

Les entr√©es sont class√©es **alphab√©tiquement** par nom principal. Ce document constitue l'**extrait fondamental** ; le dictionnaire complet (800‚Äì1500 entr√©es) peut √™tre √©tendu progressivement selon le m√™me format.

---

## B

### Backfill Scheduling (Ordonnancement par remplissage)

**D√©finition rigoureuse**  
Algorithme d'ordonnancement **non pr√©emptif** qui autorise des t√¢ches (jobs) de **faible priorit√©** et de **courte dur√©e** √† s'ex√©cuter **avant** des t√¢ches de haute priorit√©, √† la condition stricte que leur ex√©cution **n'entra√Æne aucun retard** sur l'heure de d√©marrage calcul√©e (Start Time) de la t√¢che prioritaire en t√™te de file.

**Pourquoi c'est important**  
En FIFO classique, un job demandant 100 n≈ìuds bloque toute la file alors qu'il ne reste que 90 n≈ìuds libres ‚Üí ces 90 n≈ìuds restent inactifs (¬´ drainage ¬ª). Le Backfill permet de porter l'**utilisation globale** d'un supercalculateur de ~60 % √† souvent **plus de 95 %**, en garantissant un retour sur investissement massif sans p√©naliser les gros calculs scientifiques.

**Comment √ßa marche (niveau internals)**  
- Slurm maintient : la **file d'attente** tri√©e par priorit√© (Fairshare) et l'**√©tat des n≈ìuds**.  
- Le thread **sched/backfill** simule le placement du **job en t√™te** (Job A), parcourt le temps futur jusqu'√† trouver assez de n≈ìuds qui se lib√®rent, et **verrouille** cette r√©servation temporelle.  
- Il examine les jobs suivants (B, C‚Ä¶). Si le Job B demande 10 n≈ìuds pendant 2 h et que 10 n≈ìuds sont **libres maintenant** et ne seront r√©quisitionn√©s par A que dans 3 h, le Backfill **lance B imm√©diatement**.  
- **Contrainte** : chaque job doit d√©clarer un **Walltime** (temps max d'ex√©cution). L'algorithme utilise ce temps d√©clar√© pour ses pr√©dictions.

**Bonnes pratiques / Mauvaises pratiques**  
- **Bonne** : Imposer des Walltimes par d√©faut courts (ex. 1 h) sur les partitions standard ; p√©naliser ou restreindre les utilisateurs qui demandent syst√©matiquement 7 jours pour des jobs de 10 minutes.  
- **Mauvaise** : Configurer **bf_window** sur 30 jours ‚Üí le contr√¥leur passe 100 % de son CPU √† calculer des calendriers th√©oriques inutiles.

**Commandes / outils associ√©s**  
- `sdiag` : statistiques internes du thread backfill (temps moyen du cycle, profondeur de file trait√©e).  
- `squeue --start` : heure de d√©marrage **pr√©dictive** pour les jobs en attente.

**Param√®tres & tuning (slurm.conf)**  
- `SchedulerParameters=bf_window=1440` : limite la pr√©diction √† 24 h (clusters tr√®s charg√©s).  
- `bf_resolution=600` : blocs de 10 min au lieu de 1 min ‚Üí r√©duction forte de la complexit√©.  
- `bf_max_job_test=1000` : limite le nombre de jobs que le backfill tente de caser par it√©ration.

**Troubleshooting rapide**  
- **Sympt√¥mes** : `squeue` met 30 s √† r√©pondre ; slurmctld √† 100 % sur un c≈ìur.  
- **Causes** : Boucle Backfill trop lourde **O(N√óM)** (trop de jobs + fen√™tre trop profonde).  
- **Actions** : Augmenter `bf_resolution`, diminuer `bf_window` ou `bf_max_job_test` ; `scontrol reconfigure`.

**Renvois crois√©s**  
Voir aussi : Fairshare, Walltime, Slurmctld, Scheduler.

**R√©f√©rences**  
Feitelson, D. G., et al. (2001). *The Case for Workload-Based Evaluation of HPC Systems.*

---

### BeeGFS (Bee Parallel File System)

**D√©finition rigoureuse**  
Syst√®me de **fichiers parall√®les** (POSIX) √† logiciel libre, d√©velopp√© par ThinkParQ (ex Fraunhofer), qui r√©partit **donn√©es et m√©tadonn√©es** sur des serveurs de stockage (**storage servers**) et des serveurs de m√©tadonn√©es (**metadata servers**) distincts, avec un **client en espace utilisateur** (FUSE ou noyau) et un protocole propri√©taire sur TCP ou RDMA.

**Pourquoi c'est important**  
Alternative **open-source** √† Lustre pour les clusters HPC et les environnements o√π la simplicit√© de d√©ploiement et l'**agilit√©** (ajout de n≈ìuds √† chaud, pas de d√©pendance √† un noyau Lustre sp√©cifique) priment. Tr√®s utilis√© en recherche et en mid-range HPC ; performances comparables √† Lustre pour de nombreux workloads (I/O s√©quentiel, petits fichiers si les MDS sont bien dimensionn√©s).

**Comment √ßa marche (niveau internals)**  
- **Metadata** : un ou plusieurs MDS g√®rent les noms, permissions, layout (r√©partition des blocs sur les storage targets).  
- **Storage** : les **storage targets** (OST √©quivalent) stockent les blocs de donn√©es ; le client conna√Æt le **layout** et envoie les I/O directement aux storage servers concern√©s (parall√©lisme).  
- Pas de DLM centralis√© comme Lustre ; coh√©rence via protocole et verrous c√¥t√© serveur. Support **RDMA** (verbs) pour les chemins de donn√©es √† faible latence.

**Bonnes pratiques / Mauvaises pratiques**  
- **Bonne** : Dimensionner les MDS (CPU, RAM) pour la charge en m√©tadonn√©es ; utiliser des **stripes** adapt√©es √† la taille des fichiers ; r√©seau d√©di√© (ou VLAN) pour le trafic BeeGFS.  
- **Mauvaise** : Un seul MDS pour des millions de petits fichiers sans cache m√©tadonn√©es c√¥t√© client ‚Üí goulot MDS.

**Commandes / outils associ√©s**  
- `beegfs-ctl` : statut des services, listes des storage/metadata servers, param√®tres.  
- `beegfs-df` : utilisation par storage target (√©quivalent `lfs df`).  
- Fichiers de config : `/etc/beegfs/beegfs-client.conf`, `beegfs-storage.conf`, `beegfs-meta.conf`.

**Param√®tres & tuning**  
- **tuneFileReadSize**, **tuneFileWriteSize** : tailles de transfert.  
- **tuneRemoteFSync** : comportement fsync (s√©curit√© vs perfs).  
- **connInterfaces**, **connNetFilter** : binding r√©seau / interfaces.

**Troubleshooting rapide**  
- **Sympt√¥mes** : I/O lents, erreurs ¬´ No route to host ¬ª ou d√©connexions.  
- **Causes** : MDS ou storage server down ; r√©seau satur√© ou mauvaise config interface.  
- **Actions** : `beegfs-ctl --getentryinfo` ; v√©rifier les services (`systemctl status beegfs-*`) et les logs ; tester la connectivit√© entre clients et serveurs.

**Renvois crois√©s**  
Voir aussi : Lustre, Striping, POSIX, MPI-IO, IOR, Stockage parall√®le.

**R√©f√©rences**  
ThinkParQ. *BeeGFS Documentation.* ‚Äî Lustre Operations Manual (comparaison conceptuelle).

---

### Burst Buffers (Tampons d'√©clatement)

**D√©finition rigoureuse**  
Couche de stockage **interm√©diaire**, g√©n√©ralement √† **latence tr√®s faible** (SSD/NVMe, voire NVDIMM ou m√©moire), positionn√©e entre les **n≈ìuds de calcul** et le syst√®me de fichiers parall√®le de production (Lustre, etc.), utilis√©e pour **absorber les pics d'I/O** (checkpoints, restarts, sorties massives) sans saturer le FS global ni d√©grader les autres jobs.

**Pourquoi c'est important**  
Les applications HPC ont des phases d'I/O **tr√®s bursty** : des milliers de processus √©crivent en m√™me temps un checkpoint, puis reprennent le calcul. Si tout transite directement vers Lustre, le **MDS** et les **OST** subissent une temp√™te de requ√™tes ‚Üí latence qui explose et d√©bit effectif qui chute pour tout le monde. Les burst buffers **d√©couplent** : le job √©crit d'abord sur un espace rapide (n≈ìud local ou d√©di√©), puis un **drain** asynchrone pousse les donn√©es vers Lustre.

**Comment √ßa marche (niveau internals)**  
- **Mod√®le typique** : espace **par job** ou **par n≈ìud** sur NVMe (tmpfs, LVM, ou FS local). L'application √©crit en **scratch local** ; un d√©mon ou un script post-job copie vers Lustre.  
- **Int√©gration Slurm** : **GRES** (Generic Resource) peut r√©server des ¬´ burst buffer ¬ª ; des plugins ou des **prolog/epilog** allouent et lib√®rent l'espace.  
- **Niveau avanc√©** : syst√®mes d√©di√©s (DataWarp, DDN IME, etc.) avec API et int√©gration scheduler.

**Bonnes pratiques / Mauvaises pratiques**  
- **Bonne** : R√©server une **taille coh√©rente** avec la taille du checkpoint (Slurm `--bb` ou partition d√©di√©e) ; pr√©voir un **drain** fiable (retry, monitoring) pour √©viter de perdre des donn√©es.  
- **Mauvaise** : Burst buffer trop petit ‚Üí d√©passement et √©criture directe sur Lustre (pic non absorb√©) ; pas de politique de purge ‚Üí disques pleins et jobs suivants en √©chec.

**Commandes / outils associ√©s**  
- Slurm : `sbatch --bb "capacity=100G"` (si burst buffer GRES configur√©).  
- `scontrol show burst` (si plugin activ√©).  
- Scripts prolog/epilog pour allouer un r√©pertoire local (ex. `/tmp/job_$SLURM_JOB_ID`) et copier vers Lustre en fin de job.

**Param√®tres & tuning**  
- Taille par job (capacity), politique de drain (imm√©diat vs diff√©r√©), dur√©e de r√©tention.  
- Choix du backend : tmpfs (RAM), NVMe local, ou appliance d√©di√©e.

**Troubleshooting rapide**  
- **Sympt√¥mes** : Job √©choue avec ¬´ No space left ¬ª sur le burst buffer ; ou checkpoint jamais visible sur Lustre.  
- **Causes** : Capacit√© sous-dimensionn√©e ; script de drain en √©chec (r√©seau, quota Lustre).  
- **Actions** : V√©rifier la taille r√©serv√©e et l'usage effectif ; consulter les logs du prolog/epilog et du drain.

**Renvois crois√©s**  
Voir aussi : Lustre, Striping, Checkpoint, I/O burst, Slurm GRES, Scratch.

**R√©f√©rences**  
NVIDIA Data Center. *Burst Buffer Concepts.* ‚Äî Documentation Slurm : *Burst Buffer Guide.*

---

## C

### Cgroups v2 (Control Groups v2) ‚Äî Contexte HPC

**D√©finition rigoureuse**  
M√©canisme du **noyau Linux** (depuis 2.6.24, unifi√© en v2) permettant de **grouper des processus** dans une hi√©rarchie et d'appliquer des **limites et compteurs** (CPU, m√©moire, I/O, devices) √† chaque groupe. En HPC, **Slurm** utilise les **Cgroups v2** pour **isoler** chaque job (ou step) dans un sous-arbre d√©di√©, avec des plafonds stricts sur la RAM et les CPU visibles.

**Pourquoi c'est important**  
Sans isolation, un job qui d√©passe sa r√©serve m√©moire peut **affamer** les autres processus du n≈ìud et provoquer un **OOM global** ; le noyau peut alors tuer **slurmd** ou un processus critique. Avec **ConstrainRAMSpace=yes** et **AllowedRAMSpace=98**, Slurm cr√©e un **cgroup** par job avec une limite m√©moire (ex. 250 Go sur un n≈ìud de 256 Go). Si le job d√©passe, l'**OOM-Killer** n'agit **que dans ce cgroup** ‚Üí seul le job est tu√©, l'OS et les autres jobs (ou slurmd) restent intacts.

**Comment √ßa marche (niveau internals)**  
- **Cgroups v2** : hi√©rarchie unique sous `/sys/fs/cgroup/` ; sous-arbre **slurm** (ou `slurmstepd`) avec un r√©pertoire par **job_id** et **step_id**.  
- Slurm (slurmd) cr√©e le cgroup au lancement du step, y **attache** les processus du job, et √©crit les **limites** (memory.max, cpuset.cpus, etc.).  
- √Ä la fin du job, slurmd supprime le cgroup ; le noyau garantit qu'aucun processus du job ne peut d√©passer les limites du groupe.

**Bonnes pratiques / Mauvaises pratiques**  
- **Bonne** : Activer **ConstrainCores=yes**, **ConstrainRAMSpace=yes**, **ConstrainDevices=yes** ; fixer **AllowedRAMSpace** √† 95‚Äì98 % pour laisser de la marge au syst√®me.  
- **Mauvaise** : D√©sactiver les cgroups pour ¬´ simplifier ¬ª ‚Üí risque OOM global et n≈ìuds en drain ; ou AllowedRAMSpace=100 % (aucune marge pour le noyau et les d√©mons).

**Commandes / outils associ√©s**  
- `cat /sys/fs/cgroup/.../memory.current` : usage m√©moire du cgroup (depuis l'h√¥te).  
- `cgroup.conf` (Slurm) : **CgroupAutomount=yes**, **ConstrainCores**, **ConstrainRAMSpace**, **AllowedRAMSpace**, **ConstrainDevices**.  
- `systemd-cgls` : arbre des cgroups (si Slurm utilise cgroup v2 avec systemd).

**Param√®tres & tuning**  
- **slurm.conf** : `TaskPlugin=task/cgroup` (ou task/cgroup/v2).  
- **cgroup.conf** : **ConstrainCores**, **ConstrainRAMSpace**, **AllowedRAMSpace**, **ConstrainDevices** ; **CgroupAutomount=yes** pour que Slurm g√®re le montage.

**Troubleshooting rapide**  
- **Sympt√¥mes** : Job tu√© sans message ; n≈ìud en drain ¬´ Kill task failed ¬ª.  
- **Causes** : D√©passement m√©moire ‚Üí OOM dans le cgroup ; ou cgroup non mont√© / permission refus√©e.  
- **Actions** : V√©rifier `dmesg` et `sacct -j <id>` (ExitCode, MaxRSS) ; confirmer que **cgroup.conf** et **TaskPlugin** sont coh√©rents ; tester avec un job qui alloue volontairement trop de RAM.

**Renvois crois√©s**  
Voir aussi : OOM-Killer, Slurmd, AllowedRAMSpace, ConstrainRAMSpace, Slurm.

**R√©f√©rences**  
Documentation noyau Linux : *Control Groups v2.* ‚Äî Slurm : *Cgroup Guide*, *cgroup.conf.*

---

## D

### DLM (Distributed Lock Manager) & Coh√©rence POSIX

**D√©finition rigoureuse**  
Le **DLM** (gestionnaire de verrous distribu√©), dans Lustre le **LDLM**, est le sous-syst√®me qui garantit la **coh√©rence des donn√©es et des m√©tadonn√©es** (norme POSIX) entre des milliers de clients concurrents. Il assure que si le client A modifie un fichier, le client B ne lira pas une donn√©e obsol√®te ou corrompue pr√©sente dans son cache local.

**Pourquoi c'est important**  
C'est l'un des composants les plus **complexes et sensibles** d'un syst√®me de fichiers parall√®le. Sans DLM, les lectures/√©critures concurrentes provoqueraient des **corruptions silencieuses**. En revanche, l'√©change constant de messages pour accorder, r√©voquer ou v√©rifier les verrous (**lock traffic**) est une cause majeure de **d√©gradation des performances** sur un cluster mal utilis√©.

**Comment √ßa marche (niveau internals)**  
- Lustre utilise des **verrous d'√©tendue** (Extent Locks).  
- Si le **client A** veut √©crire les octets 0‚Äì1 M d'un fichier, il demande un **verrou exclusif (Write)** √† l'OSS. L'OSS l'accorde ; le client A √©crit en cache local puis envoie les donn√©es √† l'OSS.  
- Si le **client B** veut lire les m√™mes octets, l'OSS envoie une **AST** (Asynchronous System Trap) au client A pour lui ordonner de **vider (flush)** son cache vers le disque et de **rel√¢cher** le verrou (ou de le r√©trograder en lecture partag√©e).  
- Une fois fait, l'OSS accorde le verrou de **lecture** au client B.

**Bonnes pratiques / Mauvaises pratiques**  
- **Bonne** : Utiliser les **API collectives MPI-IO** (HDF5, NetCDF) qui orchestrent les processus pour qu'un seul g√®re une grande plage contigu√´ ‚Üí r√©duction des conflits de verrous.  
- **Mauvaise** : **False sharing** ‚Äî 1000 processus √©crivent chacun 8 octets dans le m√™me fichier, entrem√™lant les √©critures. Le DLM passe 99 % du temps √† r√©voquer et r√©assigner des verrous ‚Üí d√©bit qui tombe √† quelques Ko/s.

**Commandes / outils associ√©s**  
- `lctl get_param ldlm.namespaces.*.lock_count` : nombre de verrous actifs.

**Param√®tres & tuning**  
- **obd_timeout** (d√©faut 100 s) : si un client qui d√©tient un verrou exclusif crashe (ex. coupure r√©seau) et ne r√©pond pas aux AST, le serveur attend ce d√©lai avant de l'**√©vincer** (Eviction) et de lib√©rer de force le verrou.

**Troubleshooting rapide**  
- **Sympt√¥mes** : Des dizaines de jobs figent (processus en √©tat **D** ‚Äî Uninterruptible Sleep). `dmesg` : `LustreError: ... ping timeout`.  
- **Causes** : Un client ou un routeur LNet d√©faillant d√©tient des verrous et ne r√©pond plus ; le MDS/OSS attend **obd_timeout** avant d'agir.  
- **Actions** : Attendre l'√©viction automatique. Si √ßa persiste : isoler ou red√©marrer le client fautif (identifi√© dans les logs MGS).

**Renvois crois√©s**  
Voir aussi : POSIX, False Sharing, Striping, MPI-IO.

**R√©f√©rences**  
Braam, P. J. (2019). *Lustre File System: Architecture and Internals.*

---

## G

### GPUDirect RDMA (Remote Direct Memory Access for GPUs)

**D√©finition rigoureuse**  
Technologie mat√©rielle et logicielle (NVIDIA) permettant √† des **p√©riph√©riques sur le bus PCI Express** (ex. carte r√©seau InfiniBand Mellanox) d'effectuer des **acc√®s m√©moire directs (DMA)** vers et depuis la **m√©moire locale d'un GPU (VRAM)**, en **contournant** la m√©moire syst√®me (RAM) et le processeur (CPU) de l'h√¥te.

**Pourquoi c'est important**  
En entra√Ænement distribu√© (LLM, Deep Learning), les GPU √©changent massivement des **gradients** (collectives type AllReduce). Sans GPUDirect : VRAM ‚Üí PCIe ‚Üí RAM CPU ‚Üí PCIe ‚Üí Carte r√©seau ‚Üí ‚Ä¶ Ce double saut **sature le bus PCIe** et le contr√¥leur m√©moire, limite la bande passante et augmente la latence ‚Üí les GPU restent ¬´ affam√©s ¬ª de donn√©es.

**Comment √ßa marche (niveau internals)**  
- Sp√©cification **PCIe** (DMA standard) et mappage **BAR** (Base Address Register).  
- Le driver NVIDIA **expose la VRAM** du GPU dans l'espace d'adressage physique via le **BAR1**.  
- Le module noyau r√©seau (ex. **nv_peer_mem** ou OFED) mappe ces adresses PCIe virtuelles.  
- La **HCA** (carte r√©seau) lit/√©crit **directement** dans les adresses du GPU sur le bus PCIe (souvent via les PCIe Switches de la carte m√®re), **sans r√©veiller le CPU**.

**Bonnes pratiques / Mauvaises pratiques**  
- **Bonne** : **Topology-aware placement** : la carte r√©seau doit √™tre sur la **racine PCIe (Root Complex)** qui contr√¥le le GPU avec lequel elle communique ; sinon le transfert traverse l'interconnexion inter-CPU (QPI/UPI) et les perfs s'effondrent.  
- **Mauvaise** : Activer l'**IOMMU** de fa√ßon trop restrictive dans le BIOS sans configurer les groupes IOMMU pour le passthrough ‚Üí blocage du Peer-to-Peer.

**Commandes / outils associ√©s**  
- `nvidia-smi topo -m` : matrice de topologie (PIX, PXB, SYS) entre GPU et NIC.  
- `ib_write_bw --use_cuda=<gpu_id>` : test bande passante InfiniBand directe depuis/vers la VRAM.

**Param√®tres & tuning**  
- Variables **NCCL** : `NCCL_P2P_DISABLE=0`, `NCCL_NET_GDR_LEVEL=5` (niveau minimum pour autoriser GPUDirect RDMA).

**Troubleshooting rapide**  
- **Sympt√¥mes** : perfs d'entra√Ænement IA multi-n≈ìuds qui plafonnent ; **nvtop** ‚Üí PCIe satur√© (Tx/Rx) ; **htop** ‚Üí fort %sys CPU.  
- **Causes** : GPUDirect RDMA **inactif** ; fallback par la m√©moire syst√®me. OFED sans support CUDA ou module noyau peer-to-peer manquant.  
- **Actions** : V√©rifier le module noyau (`lsmod | grep nv_peer_mem` ou √©quivalent OFED). Relancer les services de la carte r√©seau.

**Renvois crois√©s**  
Voir aussi : NCCL, PCIe, InfiniBand, RDMA, NUMA.

**R√©f√©rences**  
NVIDIA Corporation. (2023). *Developing a Linux Kernel Module using GPUDirect RDMA.*

---

### GPU Tensor Cores (C≈ìurs tenseur)

**D√©finition rigoureuse**  
Unit√©s de calcul **matriciel** int√©gr√©es aux GPU NVIDIA (√† partir de Volta, puis Turing, Ampere, Hopper) et con√ßues pour acc√©l√©rer les op√©rations **GEMM** (General Matrix Multiply) et les **transformations** de basse pr√©cision (FP16, BF16, INT8, TF32), au c≈ìur des **r√©seaux de neurones** (convolutions, attention) et du calcul scientifique mixte pr√©cision.

**Pourquoi c'est important**  
En **IA / Deep Learning**, la majorit√© des FLOPs sont des multiplications matricielles. Les **Tensor Cores** ex√©cutent des **blocs** (ex. 16√ó16√ó16) en une seule instruction avec un d√©bit (TFLOP/s) **plusieurs fois sup√©rieur** aux c≈ìurs CUDA classiques pour ces op√©rations. Sans Tensor Cores, l'entra√Ænement de grands mod√®les (LLM, vision) serait consid√©rablement plus long ; en HPC scientifique, les biblioth√®ques (cuBLAS, cuDNN) les utilisent automatiquement pour les kernels compatibles.

**Comment √ßa marche (niveau internals)**  
- **Architecture** : sous-unit√©s d√©di√©es au **D = A √ó B + C** (matrix multiply-accumulate) en FP16/BF16/INT8/TF32 ; accumulation souvent en FP32.  
- **Logiciel** : les API CUDA (WMMA), cuBLAS (GEMM), cuDNN (convolutions) g√©n√®rent des instructions **tensor** ; le compilateur (NVCC, libs) doit cibler la bonne architecture (sm_70, sm_80, etc.).  
- **NVLink** (entre GPU) et **NVSwitch** (multi-GPU) permettent d'alimenter les Tensor Cores en donn√©es √† haut d√©bit.

**Bonnes pratiques / Mauvaises pratiques**  
- **Bonne** : Utiliser **FP16** ou **BF16** (et TF32 si Ampere+) quand la pr√©cision le permet ; choisir des **tailles de batch et de matrice** align√©es sur les blocs Tensor (multiples de 8/16) ; profiler avec **Nsight Compute** pour confirmer l'utilisation des Tensor Cores.  
- **Mauvaise** : Forcer FP64 partout ‚Üí les Tensor Cores ne s'activent pas ou peu ; kernels custom mal align√©s ‚Üí r√©gression par rapport aux c≈ìurs CUDA classiques.

**Commandes / outils associ√©s**  
- `nvidia-smi` : mod√®le GPU, utilisation.  
- **Nsight Compute** : rapport par kernel (utilisation Tensor Cores, throughput).  
- **PyTorch / TensorFlow** : `torch.autocast`, `tf.keras.mixed_precision` pour activer FP16/BF16 et exploiter les Tensor Cores.

**Param√®tres & tuning**  
- **CUDA_ARCH** (compilation) : sm_70 (Volta), sm_80 (Ampere), sm_90 (Hopper).  
- **TF32** (Ampere+) : `NVIDIA_TF32_OVERRIDE=1` pour forcer TF32 en matmul (par d√©faut activ√© dans beaucoup de frameworks).  
- **Environment** : `NVIDIA_TF32_OVERRIDE=0` pour d√©sactiver si besoin de reproductibilit√© FP32 stricte.

**Troubleshooting rapide**  
- **Sympt√¥mes** : Performances GPU ¬´ normales ¬ª alors qu'on attend une acc√©l√©ration (ex. entra√Ænement pas plus rapide qu'en FP32).  
- **Causes** : Kernel non √©ligible aux Tensor Cores (taille, type) ; pr√©cision FP64 ; driver/CUDA trop ancien.  
- **Actions** : V√©rifier l'architecture GPU et la version CUDA ; profiler avec Nsight Compute ; activer mixed precision (FP16/BF16) dans le framework.

**Renvois crois√©s**  
Voir aussi : NVLink, GPUDirect RDMA, NCCL, cuBLAS, cuDNN, Roofline, Mixed Precision.

**R√©f√©rences**  
NVIDIA. *Tensor Core Programming (CUDA).* ‚Äî *NVIDIA A100 Tensor Core GPU Architecture.*

---

## H

### Hugepages & TLB (Translation Lookaside Buffer)

**D√©finition rigoureuse**  
Les **Hugepages** sont des blocs de m√©moire vive g√©r√©s par le noyau Linux dont la taille est **sup√©rieure √† la page standard** (g√©n√©ralement 4 Ko). En HPC on utilise des pages de **2 Mo** ou **1 Go** pour optimiser la **traduction d'adresses virtuelles en physiques** par le processeur (MMU).

**Pourquoi c'est important**  
Un code allouant 128 Go de RAM repr√©sente, en pages 4 Ko, **33 millions de pages**. Le CPU utilise un cache mat√©riel (le **TLB**) pour m√©moriser o√π se trouvent ces pages ; le TLB ne contient que quelques milliers d'entr√©es ‚Üí **saturation** (TLB misses). Chaque miss impose un **Page Walk** en RAM et d√©grade fortement les perfs (jusqu'√† ~20 % de p√©nalit√©). Avec des Hugepages de 1 Go, 128 Go = **128 entr√©es** ‚Üí le TLB ne sature pas.

**Comment √ßa marche (niveau internals)**  
- La **MMU** parcourt des structures arborescentes (Page Directory, Page Table). Une Hugepage permet de **s'arr√™ter plus haut** dans l'arbre.  
- **Explicit Hugepages** : r√©serv√©es au d√©marrage, contigu√´s ; l'application doit les demander (ex. `mmap()` avec `MAP_HUGETLB`).  
- **Transparent Hugepages (THP)** : le noyau (khugepaged) regroupe en arri√®re-plan des pages 4 Ko en pages 2 Mo.

**Bonnes pratiques / Mauvaises pratiques**  
- **Bonne** : Activer les **Hugepages explicites** pour les buffers RDMA/InfiniBand ‚Üí acc√©l√©ration de la **Memory Registration** par la carte r√©seau.  
- **Mauvaise** : Laisser **THP en mode always** sur des serveurs critiques (Slurmdbd, MDS Lustre) : la d√©fragmentation provoque des **pics de latence** (system jitter).

**Commandes / outils associ√©s**  
- `cat /proc/meminfo | grep Huge` : v√©rifier l'allocation des pages.  
- `perf stat -e dTLB-load-misses ./code` : mesurer le goulot TLB.

**Param√®tres & tuning**  
- **GRUB** : `hugepagesz=1G hugepages=64`.  
- **D√©sactiver THP** : `echo never > /sys/kernel/mm/transparent_hugepage/enabled`.

**Troubleshooting rapide**  
- **Sympt√¥mes** : CPU √† 100 % sur un processus **kcompactd0** ; job utilisateur tr√®s ralenti.  
- **Causes** : THP activ√©, m√©moire physique fragment√©e ; le noyau cherche des blocs contigus de 2 Mo.  
- **Actions** : D√©sactiver THP ou lib√©rer du contigu (ex. `echo 3 > /proc/sys/vm/drop_caches`).

**Renvois crois√©s**  
Voir aussi : NUMA, Perf, RDMA, OS Jitter.

**R√©f√©rences**  
Gorman, M. (2004). *Understanding the Linux Virtual Memory Manager.*

---

## M

### MPI Collectives ‚Äî AllReduce (Collectives MPI)

**D√©finition rigoureuse**  
Les **collectives MPI** sont des op√©rations de **communication de groupe** o√π **tous** les processus d'un communicateur participent selon un sch√©ma d√©fini par la norme MPI. **MPI_Allreduce** est une collective qui combine (r√©duction : somme, max, min, etc.) les donn√©es **locales** de chaque rang puis **redistribue le r√©sultat √† tous** les rangs, de sorte qu'√† la fin chaque processus poss√®de la **m√™me valeur** (ou le m√™me vecteur) globale.

**Pourquoi c'est important**  
En calcul parall√®le (optimisation, deep learning distribu√©), **AllReduce** est la collective la **plus co√ªteuse** en bande passante et en latence : chaque n≈ìud doit contribuer et recevoir le r√©sultat. Les algorithmes (arbre binaire, ring, r√©duction puis broadcast) et l'**overlap** calcul/communication d√©terminent la scalabilit√©. En **IA**, les frameworks (NCCL, Horovod) impl√©mentent des **AllReduce** optimis√©s (ring, tree) sur GPU pour la synchronisation des gradients ; la performance du r√©seau (InfiniBand, RoCE) et du logiciel (NCCL, MPI) est critique.

**Comment √ßa marche (niveau internals)**  
- **MPI_Allreduce** : la biblioth√®que MPI choisit un algorithme (souvent **ring AllReduce** ou **recursive halving/doubling**) en fonction de la taille du message et du nombre de rangs.  
- **Ring AllReduce** : les rangs forment un anneau ; en N-1 √©tapes, chaque rang envoie un bloc au suivant et re√ßoit du pr√©c√©dent ; √† la fin tous ont la somme compl√®te. Bande passante proche de l'optimal.  
- **Impl√©mentations** : OpenMPI, MPICH, MVAPICH utilisent des chemins optimis√©s (RDMA, collectives offload) ; **NCCL** c√¥t√© GPU fournit des collectives GPU-GPU (AllReduce, AllGather, ReduceScatter) tr√®s optimis√©es.

**Bonnes pratiques / Mauvaises pratiques**  
- **Bonne** : R√©duire la **fr√©quence** des AllReduce (accumuler des gradients avant de r√©duire) ; utiliser des **tailles de message** align√©es ; profiler avec **IPM** ou **TAU** pour identifier les collectives dominantes.  
- **Mauvaise** : AllReduce √† chaque it√©ration avec des messages minuscules ‚Üí latence dominante ; ou choix d'un algorithme MPI sous-optimal pour la taille de message (v√©rifier les tuned collectives).

**Commandes / outils associ√©s**  
- **MPI** : `mpirun -np N ./app` ; variables `MPICH_*`, `OMPI_*` pour forcer l'algorithme de collective.  
- **NCCL** : `NCCL_DEBUG=INFO`, `NCCL_ALGO=Ring` (ou Tree) pour le debug.  
- **OSU Micro-Benchmarks** : `osu_allreduce` pour mesurer la latence et le d√©bit AllReduce en fonction de la taille.

**Param√®tres & tuning**  
- **OpenMPI** : `--mca coll_tuned_use_dynamic_rules 1`, `coll_tuned_allreduce_algorithm` (pour forcer ring, binominal, etc.).  
- **UCX** : choix du transport (rc, ud) pour les collectives.  
- **NCCL** : `NCCL_IB_DISABLE` (forcer TCP), `NCCL_NET` (s√©lection du backend).

**Troubleshooting rapide**  
- **Sympt√¥mes** : Application MPI ou entra√Ænement distribu√© tr√®s lent ; un rang ¬´ en retard ¬ª bloque tout le monde.  
- **Causes** : AllReduce dominant ; d√©s√©quilibre de charge ; r√©seau lent ou pertes ; algorithme de collective inadapt√©.  
- **Actions** : Profiler (IPM, Nsight Systems) ; lancer `osu_allreduce` entre les m√™mes n≈ìuds ; v√©rifier les erreurs r√©seau (ibstat, RoCE PFC).

**Renvois crois√©s**  
Voir aussi : MPI, RDMA, NCCL, GPUDirect RDMA, InfiniBand, RoCE v2, OSU Benchmarks.

**R√©f√©rences**  
MPI Forum. *MPI-4.0 Standard.* ‚Äî Thakur et al. *Optimization of Collective Communication Operations in MPICH.*

---

## N

### NUMA (Non-Uniform Memory Access) & Pinning

**D√©finition rigoureuse**  
Architecture multiprocesseur o√π le **temps d'acc√®s** √† une zone de la m√©moire principale **d√©pend de l'emplacement physique** de cette m√©moire par rapport au processeur qui fait la requ√™te. Chaque socket (ou chiplet) a son **contr√¥leur m√©moire local** ; acc√©der √† la m√©moire d'un autre socket impose de traverser un **bus inter-processeur** (ex. Intel UPI, AMD Infinity Fabric).

**Pourquoi c'est important**  
Un **mauvais tuning NUMA** est une cause majeure de sous-performance en HPC. Si un processus MPI alloue des tableaux dans la RAM du **CPU 2** mais s'ex√©cute sur le **CPU 1**, la requ√™te traverse le bus inter-socket : bande passante **divis√©e par deux**, latence **+30 % √† +50 %**. Les caches **L3** (sp√©cifiques au n≈ìud NUMA) deviennent peu efficaces.

**Comment √ßa marche (niveau internals)**  
- Linux utilise par d√©faut la politique **First Touch**. La m√©moire n'est pas allou√©e physiquement au `malloc()` (adresses virtuelles seulement), mais au **premier acc√®s en √©criture** (page fault). Linux place alors la **page physique** dans la RAM du **n≈ìud NUMA** sur lequel le thread s'ex√©cute √† ce moment.

**Bonnes pratiques / Mauvaises pratiques**  
- **Bonne** : Toujours **√©pingler** (pinning / binding) les processus aux c≈ìurs. En OpenMP, **parall√©liser l'initialisation** des grands tableaux pour que chaque thread fasse le First Touch sur sa fraction de m√©moire locale.  
- **Mauvaise** : Utiliser `numactl --interleave=all` pour un code de dynamique des fluides ‚Üí r√©partition des pages sur tous les n≈ìuds ‚Üí statistiquement ~50 % (bi-socket) d'acc√®s **distants** et lents.

**Commandes / outils associ√©s**  
- `numactl -H` : n≈ìuds mat√©riels, m√©moire par n≈ìud, **matrice des distances** (p√©nalit√©s).  
- `lstopo` (hwloc) : repr√©sentation graphique des c≈ìurs, caches L3 partag√©s et n≈ìuds NUMA.

**Param√®tres & tuning**  
- **Slurm** : `TaskPlugin=task/affinity,task/cgroup`.  
- **MPI** : `--bind-to core --map-by socket` (OpenMPI).  
- **OS** : D√©sactiver **numad** sur les n≈ìuds de calcul (√©viter que l'OS d√©place des processus d√©j√† optimis√©s par Slurm/MPI).

**Troubleshooting rapide**  
- **Sympt√¥mes** : benchmark **STREAM** affiche 150 Go/s au lieu de 300 Go/s.  
- **Causes** : Migration des threads d'un CPU √† l'autre (OS scheduler jitter) ou m√©moire allou√©e de fa√ßon distante.  
- **Actions** : Relancer avec `OMP_PLACES=cores OMP_PROC_BIND=close` ou `numactl --cpunodebind=0 --membind=0 ./stream`.

**Renvois crois√©s**  
Voir aussi : Affinity, MPI, Hugepages, Roofline Model, Cgroups.

**R√©f√©rences**  
Drepper, U. (2007). *What Every Programmer Should Know About Memory.*

---

### NVLink (NVIDIA High-Speed Interconnect)

**D√©finition rigoureuse**  
Interconnexion **point √† point** √† **tr√®s haut d√©bit** et **faible latence** d√©velopp√©e par NVIDIA pour connecter **plusieurs GPU** au sein d'un m√™me n≈ìud (ou entre n≈ìuds avec NVLink Switch). Elle permet des transferts **GPU-GPU** directs (m√©moire √† m√©moire) sans passer par le **PCIe** de l'h√¥te, avec une bande passante agr√©g√©e bien sup√©rieure (ex. 600 Go/s par paire de GPU en NVLink 3.0) et une latence plus basse que PCIe.

**Pourquoi c'est important**  
En **multi-GPU** (entra√Ænement, inference, calcul scientifique), les GPU √©changent des **tenseurs** et des **gradients** en permanence. Le **PCIe** (typiquement 32 Go/s en PCIe 4.0 x16) devient le **goulot** d√®s que deux GPU ou plus partagent des donn√©es. **NVLink** multiplie la bande passante (jusqu'√† des centaines de Go/s) et r√©duit la latence, ce qui permet de **scaler** les applications sur 4, 8 ou 16 GPU par n≈ìud sans √™tre limit√© par le bus.

**Comment √ßa marche (niveau internals)**  
- **Topologie** : liens **sym√©triques** entre GPU (mesh ou switch). Chaque lien est un bus s√©rie multi-lanes (NVLink 3 : 50 Gb/s par sens par lien ; plusieurs liens par GPU).  
- **NVSwitch** (n≈ìuds type DGX) : switch interne qui connecte tous les GPU en full bisection, √©vitant les chemins en plusieurs sauts.  
- **Software** : NCCL, CUDA Unified Memory, et les runtimes MPI/GPU utilisent NVLink automatiquement quand il est disponible ; `nvidia-smi topo -m` affiche la topologie.

**Bonnes pratiques / Mauvaises pratiques**  
- **Bonne** : Pr√©f√©rer les n≈ìuds avec **NVLink** (ou NVLink Switch) pour les jobs multi-GPU ; lier les processus aux GPU **proches** (m√™me NVLink domain) via **CUDA_VISIBLE_DEVICES** ou Slurm **GPU binding** ; v√©rifier la topologie avec `nvidia-smi topo -m`.  
- **Mauvaise** : Placer 8 GPU sur un n≈ìud sans NVLink et saturer le PCIe ; ou binder les rangs MPI de fa√ßon √† ce que les paires qui communiquent le plus soient sur des GPU non reli√©s par NVLink.

**Commandes / outils associ√©s**  
- `nvidia-smi topo -m` : matrice de connectivit√© GPU (NVLink, PCIe).  
- `nvidia-smi nvlink --status` : √©tat des liens NVLink.  
- **NCCL** : `NCCL_DEBUG=INFO` pour voir les chemins utilis√©s (NVLink vs PCIe).

**Param√®tres & tuning**  
- **Slurm** : **Gres** et **topology/plugin** pour r√©server des GPU et (si support√©) respecter la topologie.  
- **NCCL** : utilise NVLink par d√©faut quand disponible ; pas de param√®tre sp√©cifique √† activer en g√©n√©ral.  
- **CUDA** : pas de r√©glage utilisateur ; le driver et le runtime s√©lectionnent le chemin.

**Troubleshooting rapide**  
- **Sympt√¥mes** : Multi-GPU plus lent qu'attendu ; bande passante inter-GPU faible.  
- **Causes** : NVLink absent (carte ou n≈ìud sans NVLink) ; topologie non respect√©e (binding) ; un lien NVLink down (rare).  
- **Actions** : `nvidia-smi topo -m` et `nvlink --status` ; v√©rifier le binding des processus aux GPU ; comparer avec un n≈ìud connu NVLink.

**Renvois crois√©s**  
Voir aussi : GPUDirect RDMA, GPU Tensor Cores, NCCL, PCIe, Multi-GPU, Slurm GRES.

**R√©f√©rences**  
NVIDIA. *NVLink and NVSwitch.* ‚Äî *NVIDIA DGX Architecture.*

---

## O

### OOM-Killer (Out-Of-Memory Killer)

**D√©finition rigoureuse**  
M√©canisme de **survie** du noyau Linux : lorsque la m√©moire physique (RAM) et l'√©change (Swap) sont √©puis√©s, le noyau risque une panique. L'**OOM-Killer** choisit **heuristiquement** un ou plusieurs processus √† **tuer (SIGKILL)** pour lib√©rer de la m√©moire et sauver le syst√®me.

**Pourquoi c'est important**  
Sur un n≈ìud HPC le **Swap est proscrit** (pr√©dictibilit√© et perfs MPI). Les OOM sont donc possibles si un job est mal dimensionn√© (ex. maillage trop fin). L'heuristique Linux peut choisir de tuer **slurmd** ou **sshd** plut√¥t que le code utilisateur ‚Üí n≈ìud d√©clar√© **DOWN**, job perdu, intervention admin.

**Comment √ßa marche (niveau internals)**  
- Lors d'une allocation qui √©choue, la routine **out_of_memory()** est invoqu√©e.  
- Le noyau calcule un **oom_score** par processus (RSS, uptime, **oom_score_adj**).  
- Le processus au **score le plus √©lev√©** re√ßoit un **SIGKILL** (non interceptable).

**Bonnes pratiques / Mauvaises pratiques**  
- **Bonne** : Utiliser les **Cgroups v2** de Slurm (**ConstrainRAMSpace=yes**). Le job est limit√© (ex. 250 Go sur 256 Go) ; en cas de d√©passement, l'OOM-Killer agit **dans le Cgroup** uniquement ‚Üí seul le job est tu√©, l'OS et slurmd sont prot√©g√©s.  
- **Mauvaise** : D√©sactiver l'overcommit (`vm.overcommit_memory=2`) sur un cluster qui compile beaucoup (Spack/Make) ‚Üí √©checs **malloc** pr√©matur√©s m√™me avec de la RAM libre.

**Commandes / outils associ√©s**  
- `dmesg -T | grep -i oom` : traces des ex√©cutions de l'OOM-Killer.  
- `sacct -j <jobid>` : statut **OUT_OF_MEMORY** si g√©r√© par Slurm/Cgroups.

**Param√®tres & tuning**  
- **Systemd** : `OOMScoreAdjust=-1000` sur l'unit√© de **slurmd** (et √©ventuellement sshd) pour les rendre quasi intouchables par l'OOM-Killer.

**Troubleshooting rapide**  
- **Sympt√¥mes** : ¬´ Mon job a crash√© sans message dans le .out ¬ª ; n≈ìud en **drain** avec ¬´ Kill task failed ¬ª.  
- **Causes** : Le job a satur√© la m√©moire ; OOM-Killer d√©clench√© ; Cgroup a fonctionn√© ou OOM global a frapp√©, avec processus zombie emp√™chant le nettoyage.  
- **Actions** : V√©rifier `dmesg` (ex. ¬´ Killed process 4567 (python) ¬ª). V√©rifier **Cgroups** et **AllowedRAMSpace** (ex. 98 %) pour pr√©server le syst√®me.

**Renvois crois√©s**  
Voir aussi : Cgroups, Slurmd, Hugepages, Swap.

**R√©f√©rences**  
Documentation noyau Linux : *oom-killer.*

---

## F

### Fairshare (Partage √©quitable)

**D√©finition rigoureuse**  
Algorithme d'**ajustement dynamique de la priorit√©** des jobs dans la file d'attente, bas√© sur l'**historique de consommation** des ressources (CPU, GPU-heures) par utilisateur ou par compte (projet/laboratoire), sur une fen√™tre temporelle glissante avec **d√©croissance** (demi-vie), de sorte que les entit√©s ayant **sous-consomm√©** voient leur priorit√© augmenter et celles ayant **sur-consomm√©** la voir diminuer par rapport √† une cible (share) pr√©d√©finie.

**Pourquoi c'est important**  
En environnement **multi-tenant**, un ordonnancement purement FIFO ou par priorit√© fixe permet √† un seul laboratoire de monopoliser le cluster. Le Fairshare garantit que chaque groupe re√ßoit, sur la dur√©e, la **part de ressources** pour laquelle il a contract√© (ou pay√©), tout en permettant des pics temporaires et en ¬´ r√©compensant ¬ª les sous-consommateurs.

**Comment √ßa marche (niveau internals)**  
- Slurm (via **slurmdbd** et la base **Accounting**) enregistre l'usage par utilisateur/compte.  
- La priorit√© d'un job est une **combinaison pond√©r√©e** : Fairshare + √Çge du job + QOS + Taille du job.  
- Le composant Fairshare compare la **consommation r√©cente** (avec demi-vie) √† la **part cible** (share) ; un ratio &lt; 1 (sous-consommation) √©l√®ve la priorit√©, un ratio &gt; 1 la baisse.

**Bonnes pratiques / Mauvaises pratiques**  
- **Bonne** : D√©finir des **comptes** (accounts) et des **associations** utilisateur‚Üîcompte coh√©rents avec la gouvernance (projets, labos). Ajuster les **shares** (Fairshare=) pour refl√©ter les engagements.  
- **Mauvaise** : Laisser tous les utilisateurs dans le m√™me compte sans hi√©rarchie ‚Üí le Fairshare ne peut pas diff√©rencier les groupes.

**Commandes / outils associ√©s**  
- `sshare` : affiche les parts (Fairshare) et l'usage par compte/utilisateur.  
- `sacctmgr show assoc` : hi√©rarchie comptes/utilisateurs et param√®tres (Fairshare, MaxJobs, etc.).

**Param√®tres & tuning**  
- `PriorityWeightFairshare` dans slurm.conf : poids du facteur Fairshare dans la priorit√© globale.  
- `PriorityDecayHalfLife`, `PriorityUsageResetPeriod` : demi-vie et reset de l'historique.

**Troubleshooting rapide**  
- **Sympt√¥mes** : un laboratoire se plaint de ne jamais voir ses jobs d√©marrer alors que le cluster est ¬´ √† moiti√© vide ¬ª.  
- **Causes** : Sur-consommation pass√©e ‚Üí priorit√© Fairshare tr√®s basse ; ou ressources demand√©es (GPU, licence, partition) satur√©es alors que les CPU semblent libres.  
- **Actions** : V√©rifier `sshare` et les limites du compte (MaxJobs, QOS). Expliquer le Fairshare ; √©ventuellement ajuster les shares ou les QOS.

**Renvois crois√©s**  
Voir aussi : Backfill, Slurm, Accounting, Chargeback, Partition, QOS.

**R√©f√©rences**  
Documentation Slurm : *Fairshare*, *Priority Multifactor*.

---

### Slurm Fairshare ‚Äî Impl√©mentation et param√®tres

**D√©finition rigoureuse**  
Dans Slurm, le **Fairshare** est impl√©ment√© par le **plugin de priorit√©** (priority plugin) qui calcule un **score de priorit√©** pour chaque job en attente en combinant plusieurs facteurs, dont un **composant Fairshare** d√©riv√© de l'**usage enregistr√©** (slurmdbd, base Accounting) par **utilisateur** et **compte** (association), avec **demi-vie** (decay) et comparaison √† une **part cible** (Fairshare=) d√©finie par l'administrateur.

**Pourquoi c'est important**  
Sans configuration explicite (comptes, associations, **PriorityType=priority/multifactor**), Slurm utilise des priorit√©s **FIFO** ou **basic** qui ne refl√®tent pas l'√©quit√© entre projets. L'**impl√©mentation Slurm** du Fairshare permet de **pond√©rer** la priorit√© selon l'historique de consommation (CPU, GPU-heures) et d'**ajuster** le comportement via **PriorityWeightFairshare**, **PriorityDecayHalfLife**, et les **shares** par compte, ce qui est indispensable en environnement multi-tenant (laboratoires, projets payants).

**Comment √ßa marche (niveau internals)**  
- **slurmdbd** enregistre l'usage par **job** (CPU time, etc.) dans la base ; le **slurmctld** interroge ou re√ßoit des mises √† jour pour calculer l'**usage effectif** par association.  
- **Priority plugin** : √† chaque cycle de scheduling, pour chaque job en file, calcul du **fairshare component** = f(usage r√©cent avec decay, share cible). Un ratio **usage/share &lt; 1** (sous-consommation) augmente la priorit√©.  
- **Priorit√© finale** = combinaison pond√©r√©e : **Fairshare + Age + Job size + QOS + ‚Ä¶** (voir **PriorityParameters** dans slurm.conf).

**Bonnes pratiques / Mauvaises pratiques**  
- **Bonne** : D√©finir une **hi√©rarchie de comptes** (sacctmgr) et des **associations** utilisateur‚Üîcompte ; attribuer des **Fairshare=** r√©alistes ; utiliser **PriorityDecayHalfLife** (ex. 7‚Äì14 jours) pour que l'historique r√©cent p√®se plus ; documenter la politique pour les utilisateurs.  
- **Mauvaise** : Laisser **PriorityType=priority/basic** alors que la gouvernance exige du Fairshare ; ou **Fairshare=0** pour un compte (le rend quasi invisible au Fairshare).

**Commandes / outils associ√©s**  
- **sshare** : affiche les parts (Fairshare) et l'usage par compte/utilisateur.  
- **sprio** : priorit√© d√©taill√©e des jobs en file (composant Fairshare, age, etc.).  
- **sacctmgr show assoc** : hi√©rarchie et param√®tres (Fairshare, MaxJobs, QOS).  
- **scontrol show config** : PriorityType, PriorityWeightFairshare, PriorityDecayHalfLife, PriorityFavorSmall, etc.

**Param√®tres & tuning (slurm.conf)**  
- **PriorityType=priority/multifactor** : active le calcul multifactor dont Fairshare.  
- **PriorityWeightFairshare=10000** (ex.) : poids du facteur Fairshare.  
- **PriorityDecayHalfLife=7-0** : demi-vie 7 jours (format jours-heures).  
- **PriorityUsageResetPeriod=monthly** (ou none) : reset p√©riodique de l'usage pour le calcul.  
- **sacctmgr** : **Fairshare=** par association (nombre entier, relatif aux autres comptes).

**Troubleshooting rapide**  
- **Sympt√¥mes** : ¬´ Mes jobs ne partent jamais ¬ª alors que le cluster semble peu charg√© ; ou priorit√© incoh√©rente avec les attentes.  
- **Causes** : Fairshare tr√®s bas (sur-consommation pass√©e) ; **slurmdbd** down ou base Accounting non √† jour ; **PriorityWeightFairshare=0** (Fairshare d√©sactiv√© dans le calcul).  
- **Actions** : V√©rifier **sshare** et **sprio** ; confirmer que slurmdbd tourne et que les jobs termin√©s sont bien comptabilis√©s ; ajuster les shares ou **PriorityDecayHalfLife** si la politique le permet.

**Renvois crois√©s**  
Voir aussi : Fairshare, Backfill, Slurm, Accounting, slurmdbd, Partition, QOS, sacctmgr.

**R√©f√©rences**  
Slurm : *Priority Multifactor Plugin*, *Fairshare*, *sacctmgr.*

---

## R

### RDMA (Remote Direct Memory Access)

**D√©finition rigoureuse**  
Technologie permettant √† une **carte r√©seau** (HCA) d'**√©crire ou lire directement** dans la **m√©moire RAM** d'un autre ordinateur, **sans intervention du processeur** (CPU) ni du noyau du syst√®me d'exploitation (OS Bypass), en utilisant le **bus** (PCIe) et le **protocole** adapt√©s (InfiniBand ou RoCE).

**Pourquoi c'est important**  
En calcul parall√®le (MPI), la **latence** et la **bande passante** du r√©seau d√©terminent la scalabilit√©. Avec TCP/IP classique, chaque message traverse le noyau (copies, interruptions, checksums) ‚Üí latence typique **10‚Äì50 ¬µs**. Avec le RDMA, la HCA acc√®de √† la RAM distante en **~1‚Äì2 ¬µs** et sans charger le CPU, ce qui permet aux applications MPI et aux collectives (NCCL, MPI_Allreduce) d'atteindre le d√©bit physique du r√©seau.

**Comment √ßa marche (niveau internals)**  
- La **HCA** expose des **files de travail** (Work Queues) et des **cl√©s d'acc√®s** (Memory Keys) pour des zones m√©moire enregistr√©es (Registered Memory).  
- L'application (ou la librairie MPI/UCX) enregistre un buffer avec le noyau/driver, obtient un **descripteur** (LKey, RKey).  
- L'envoi consiste √† poster une **Work Request** (Send, RDMA Write, RDMA Read) qui r√©f√©rence l'adresse locale et l'adresse/distante (RKey). La HCA effectue le transfert en **DMA** sans r√©veiller le CPU.

**Bonnes pratiques / Mauvaises pratiques**  
- **Bonne** : Utiliser un **r√©seau d√©di√©** (InfiniBand ou Ethernet avec RoCE + PFC/ECN) pour le trafic RDMA ; v√©rifier que le **Subnet Manager** (InfiniBand) ou la config **RoCE** (PFC, ECN) est correcte.  
- **Mauvaise** : M√©langer trafic RDMA et trafic TCP massif sur les m√™mes liens sans QoS ‚Üí congestion et perte de paquets (RoCE est sensible aux pertes).

**Commandes / outils associ√©s**  
- `ibstat`, `ibv_devinfo` : √©tat des ports InfiniBand.  
- `ib_write_bw`, `ib_read_bw` : tests bande passante RDMA point √† point.  
- `perftest` (ib_send_bw, etc.) : micro-benchmarks latence et d√©bit.

**Param√®tres & tuning**  
- **UCX** : `UCX_NET_DEVICES`, `UCX_TLS=rc,ud` (InfiniBand).  
- **OpenMPI** : `--mca btl openib` ou utilisation d‚ÄôUCX pour le transport.

**Troubleshooting rapide**  
- **Sympt√¥mes** : jobs MPI lents ; latence r√©seau √©lev√©e ; messages d'erreur ¬´ connection reset ¬ª ou ¬´ timeout ¬ª.  
- **Causes** : C√¢ble ou port d√©fectueux, Subnet Manager instable, ou RoCE mal configur√© (pas de PFC).  
- **Actions** : `ibstat`, `ibdiagnet` ; v√©rifier les erreurs sur les ports (Symbol Errors, LinkDowned) ; stabiliser le SM ou la config RoCE.

**Renvois crois√©s**  
Voir aussi : InfiniBand, RoCE, GPUDirect RDMA, MPI, UCX, OS Bypass.

**R√©f√©rences**  
InfiniBand Trade Association. *InfiniBand Architecture Specification.*  
RDMA Aware Networks Programming Guide.

---

### RoCE v2 (RDMA over Converged Ethernet ‚Äî Version 2)

**D√©finition rigoureuse**  
Protocole permettant d'effectuer des op√©rations **RDMA** (Remote Direct Memory Access) sur des **r√©seaux Ethernet** en utilisant une **pile de transport** (UDP/IP pour RoCE v2, contrairement √† RoCE v1 qui utilisait Ethernet seul). La **HCA** (Host Channel Adapter) ou la **NIC** compatible RoCE encapsule les verbes RDMA dans des paquets Ethernet routables, permettant du **RDMA sans InfiniBand** dans des datacenters Ethernet.

**Pourquoi c'est important**  
Beaucoup de sites n'ont pas d'**InfiniBand** (co√ªt, comp√©tences) mais disposent d'Ethernet 25/100 GbE. **RoCE v2** permet d'obtenir une **latence proche de l'IB** (quelques ¬µs) et une bande passante √©lev√©e sur Ethernet, √† condition que le r√©seau soit **sans perte** (PFC ‚Äî Priority Flow Control) et √©ventuellement avec **ECN** (Explicit Congestion Notification) pour √©viter les drops qui d√©gradent fortement le d√©bit RDMA.

**Comment √ßa marche (niveau internals)**  
- **RoCE v2** : trafic RDMA en **UDP** (ports de destination d√©di√©s), avec **routage IP** possible (L3). La NIC expose les **verbs** RDMA (Send, Write, Read) et g√®re le transport en mat√©riel.  
- **PFC** : pause frames sur les priorit√©s utilis√©es par RoCE pour √©viter les pertes de paquets en cas de congestion.  
- **DCQCN** (Data Center Quantized Congestion Notification) : variante avec ECN pour limiter le d√©bit des flux en congestion au lieu de tout bloquer.

**Bonnes pratiques / Mauvaises pratiques**  
- **Bonne** : Activer **PFC** sur les classes de trafic RoCE ; isoler le trafic RDMA (VLAN, QoS) ; utiliser des switchs **low-latency** et une topologie **non bloquante** (Spine-Leaf).  
- **Mauvaise** : RoCE sur un r√©seau partag√© sans PFC ‚Üí pertes ‚Üí retransmissions et effondrement du d√©bit ; m√©langer RoCE et TCP massif sans priorit√©.

**Commandes / outils associ√©s**  
- `rdma link` : √©tat des devices RDMA (RoCE).  
- `ibv_devinfo`, `perftest` (ib_write_bw, etc.) : v√©rifier que la NIC est en mode RoCE et tester latence/d√©bit.  
- Configuration switch : PFC, ECN, DSCP pour les priorit√©s RoCE.

**Param√®tres & tuning**  
- **Kernel** : `rdma_cm`, modules RoCE (mlx5 avec RoCE).  
- **UCX** : `UCX_TLS=rc,ud` (si RoCE support√©), `UCX_NET_DEVICES` pour s√©lectionner l'interface.  
- R√©seau : **MTU** (jumbo si coh√©rent partout), **PFC** sur la m√™me priorit√© que les paquets RoCE.

**Troubleshooting rapide**  
- **Sympt√¥mes** : Latence √©lev√©e, d√©bit faible, erreurs ¬´ connection reset ¬ª ou timeouts MPI.  
- **Causes** : PFC d√©sactiv√© ou mal configur√© ; pertes de paquets ; MTU incoh√©rent ; driver ou firmware NIC obsol√®te.  
- **Actions** : V√©rifier PFC/ECN sur les switchs ; `perftest` entre deux n≈ìuds ; `ethtool -S` pour les drops ; mettre √† jour firmware/driver.

**Renvois crois√©s**  
Voir aussi : RDMA, InfiniBand, GPUDirect RDMA, MPI, UCX, PFC, Spine-Leaf.

**R√©f√©rences**  
IBTA. *Supplement to InfiniBand Architecture ‚Äî RoCE.* ‚Äî IEEE 802.1Qbb (PFC).

---

## S

### Striping (Lustre) ‚Äî Entrelacement d'objets

**D√©finition rigoureuse**  
M√©canisme fondamental de Lustre (et d'autres FS parall√®les) consistant √† **diviser logiquement** un fichier unique en segments de taille fixe (**chunks**) et √† **distribuer** ces segments en **round-robin** sur plusieurs cibles de stockage physiques distinctes (**OST** ‚Äî Object Storage Targets).

**Pourquoi c'est important**  
Un seul disque ou SSD a une bande passante physique limit√©e (ex. 200 Mo/s HDD, 3 Go/s NVMe). En HPC, un job MPI peut devoir √©crire un fichier de checkpoint √† **100 Go/s**. Le striping permet d'**agr√©ger** la bande passante de dizaines ou centaines d'OST pour **un seul et m√™me fichier**.  
*Image mentale : un fichier coup√© en tranches ‚Äî tranche 1 ‚Üí serveur A, tranche 2 ‚Üí B, tranche 3 ‚Üí C, etc.*

**Comment √ßa marche (niveau internals)**  
- Lustre maintient cette information dans l'**EA** (Extended Attribute) du fichier sur le serveur de m√©tadonn√©es (**MDT**), via le composant **LOV** (Logical Object Volume).  
- Quand un client ouvre le fichier, le MDS lui fournit la **¬´ carte ¬ª** des objets. Si le **stripe_size** est 1 Mo, pour lire l'octet 1 048 577 (d√©but du 2·µâ m√©gaoctet), le client envoie une requ√™te **RDMA directement √† l'OST n¬∞2** sans reconsulter le MDS ‚Üí **OS-bypass** massif.

**Bonnes pratiques / Mauvaises pratiques**  
- **Bonne** : Adapter le **Stripe Count** √† la taille finale estim√©e du fichier (r√®gle empirique : ~1 OST par tranche de 100 Go). Utiliser **lfs setstripe** sur un **dossier** avant la cr√©ation des fichiers, car le striping **ne peut pas √™tre modifi√©** une fois le fichier cr√©√© (sans recopie compl√®te).  
- **Mauvaise** : Appliquer un **Stripe Count maximal** (-c -1, ¬´ tous les OST du cluster ¬ª) sur un dossier contenant des **millions de fichiers de quelques Ko** ‚Üí surcharge du MDS (allocation de millions d'objets vides) et **fragmentation** de l'espace libre des OST.

**Commandes / outils associ√©s**  
- `lfs getstripe /chemin/fichier` : affiche l'index des OST h√©bergeant physiquement le fichier.  
- `lfs setstripe -c <count> -S <size> /chemin/dossier` : configure l'**h√©ritage** d'entrelacement pour les fichiers cr√©√©s dans ce dossier.  
- `lfs df -h` : voir l'utilisation par OST (d√©tection **OST Imbalance**).  
- **lfs_migrate** : d√©placer des fichiers d'un OST vers d'autres (r√©√©quilibrage).

**Param√®tres & tuning**  
- **stripe_size** : taille du segment (d√©faut souvent 1 Mo). L'augmenter √† **4 Mo** ou **16 Mo** pour les √©critures **massivement s√©quentielles** en tr√®s gros blocs.

**Troubleshooting rapide**  
- **Sympt√¥mes** : Erreur **ENOSPC** (No space left on device) alors que `df -h` montre qu'il reste beaucoup d'espace libre sur le FS global.  
- **Causes** : **D√©s√©quilibre des OST** (OST Imbalance). Si beaucoup de fichiers ont √©t√© cr√©√©s avec un stripe de 1 (un seul OST), certains OST peuvent √™tre **pleins √† 100 %** alors que d'autres sont vides ; un OST plein emp√™che toute √©criture le ciblant.  
- **Actions** : Utiliser **lfs df -h** pour rep√©rer l'OST plein √† 100 %. **R√©√©quilibrer** avec **lfs_migrate** pour d√©placer des fichiers depuis l'OST plein vers des OST moins charg√©s.

**Renvois crois√©s**  
Voir aussi : MDT/OST, LOV, EA, IOR, MPI-IO, DLM, Lustre, LNet.

**R√©f√©rences**  
Braam, P. J. (2019). *Lustre File System: Architecture and Internals.* ‚Äî Lustre Operations Manual. *Striping.*

---

## Index des entr√©es (extrait fondamental)

| Lettre | Entr√©e | Th√®me |
|--------|--------|--------|
| B | Backfill Scheduling | Ordonnancement Slurm |
| B | BeeGFS | Stockage parall√®le |
| B | Burst Buffers | Stockage, I/O |
| C | Cgroups v2 (HPC) | Linux / SRE, Isolation |
| D | DLM (Lustre) & Coh√©rence POSIX | Stockage parall√®le |
| F | Fairshare | Ordonnancement Slurm |
| F | Slurm Fairshare | Ordonnancement Slurm |
| G | GPUDirect RDMA | R√©seau, GPU, IA |
| G | GPU Tensor Cores | GPU, IA |
| H | Hugepages & TLB | Linux / SRE, M√©moire |
| M | MPI Collectives (AllReduce) | MPI, R√©seau |
| N | NUMA & Pinning | Architecture, M√©moire |
| N | NVLink | GPU, Interconnexion |
| O | OOM-Killer | Linux / SRE, M√©moire |
| R | RDMA | R√©seau, Latence |
| R | RoCE v2 | R√©seau, Ethernet RDMA |
| S | Striping (Lustre) | Stockage parall√®le |

**Extension du dictionnaire**  
Chaque nouvelle entr√©e doit respecter la structure en 9 sections ci-dessus. Entr√©es d√©j√† pr√©sentes (B √† S) : Backfill, BeeGFS, Burst Buffers, Cgroups v2, DLM, Fairshare, Slurm Fairshare, GPUDirect RDMA, GPU Tensor Cores, Hugepages, MPI Collectives (AllReduce), NUMA, NVLink, OOM-Killer, RDMA, RoCE v2, Striping. Entr√©es pr√©vues (√† r√©diger au m√™me format) : Affinity, Apptainer, Co-design, DNE, Eager Protocol, Fat-Tree, HPL, InfiniBand, LNet, Lustre, MPI, MUNGE, NCCL, Root Squash, Slurm, Spine-Leaf, Walltime, Warewulf, etc. Voir aussi le [Glossaire et Acronymes](Glossaire-et-Acronymes) pour les d√©finitions courtes et le [Sommaire du Manuel HPC](Manuel-HPC-Sommaire-Complet) pour les chapitres d√©taill√©s.

---

## Liens utiles

- **[Glossaire et Acronymes](Glossaire-et-Acronymes)** : d√©finitions courtes et liste d‚Äôacronymes
- **[Sommaire du Manuel HPC](Manuel-HPC-Sommaire-Complet)** : 8 volumes, chapitres et labs
- **[Guide SLURM Complet](Guide-SLURM-Complet)** : commandes et configuration Slurm
- **[Home](Home)** : page d‚Äôaccueil du wiki

---

**Dictionnaire encyclop√©dique HPC ‚Äî Extrait fondamental**  
**Niveau** : Doctorat / Architecte ‚Äî **Derni√®re mise √† jour** : 2024
