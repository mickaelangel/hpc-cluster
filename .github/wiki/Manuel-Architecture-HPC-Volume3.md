# üìö Manuel d'architecture et d'ing√©nierie HPC

**Volume 3 : Stockage parall√®le et gestion des donn√©es (Deep Dive Lustre)**

> **Niveau** : DevOps Senior / Architecte HPC ‚Äî **Public** : Master, Doctorat, ing√©nieurs syst√®me

---

## Vue d'ensemble du volume

Le syst√®me de fichiers parall√®le est l'un des composants les **plus critiques, complexes et on√©reux** d'un supercalculateur. C'est ici que se gagnent ou se perdent les performances d'une application scientifique √† grande √©chelle. Ce volume couvre l'architecture du stockage HPC (POSIX, tiering), les **internals Lustre** (MGS, MDS/MDT, OSS/OST, LNet, DNE), le d√©ploiement et le tuning (striping, `lctl`), puis le panorama des alternatives (BeeGFS, GPFS, CephFS). Un [lab Lustre](#-lab-3--d√©ploiement-dun-mini-lustre-tcp-et-analyse-du-striping) et un [examen de fin de volume](#-examen-de-fin-de-volume-3) permettent de valider les acquis.

**Pr√©requis :**
- Syst√®mes de fichiers locaux (ext4, XFS) et r√©seaux (NFS) ‚Äî Ch. 8
- R√©seaux HPC (InfiniBand, RDMA) ‚Äî [Volume 2](Manuel-Architecture-HPC-Volume2) ‚Äî Ch. 9‚Äì10
- Administration Linux avanc√©e (block devices, LVM, multipathing) ‚Äî Ch. 10

---

## Chapitre 8 : Architecture du stockage HPC et s√©mantique POSIX

### Objectifs d'apprentissage

- Comprendre les **limites de la norme POSIX** dans un environnement massivement distribu√©
- Concevoir une **hi√©rarchie de stockage (tiering)** adapt√©e au cycle de vie des donn√©es scientifiques

---

### 8.1 Le goulot d'√©tranglement POSIX

Le standard **POSIX** (Portable Operating System Interface) a √©t√© con√ßu pour des syst√®mes **mono-n≈ìuds**. Il impose une **coh√©rence forte** (strict consistency) : si le n≈ìud A √©crit dans un fichier, le n≈ìud B doit voir cette modification **instantan√©ment** s'il lit ce m√™me fichier.

Dans un cluster de **2000 n≈ìuds**, maintenir cette coh√©rence lors d'√©critures concurrentes g√©n√®re un **trafic massif de verrous** (lock traffic). C'est le d√©fi fondamental des syst√®mes de fichiers parall√®les.

---

### 8.2 Hi√©rarchisation du stockage (Tiering)

Un cluster HPC performant ne stocke **pas** toutes les donn√©es au m√™me endroit. On divise le stockage en **tiers** distincts :

| Tier | R√¥le | Caract√©ristiques |
|------|------|------------------|
| **Scratch (Tier 0/1)** | Espace de travail des jobs en cours | Ultra-rapide (NVMe/SSD), parall√®le (Lustre/BeeGFS), **sans sauvegarde**. Purg√© automatiquement (ex. fichiers > 30 jours). |
| **Project / Work (Tier 2)** | Datasets actifs d'un projet | Capacitif performant (HDD + cache SSD), **sauvegard√©**. |
| **Archive (Tier 3)** | Stockage froid | Bandes LTO, S3, Erasure Coding. G√©r√© par un **HSM** (Hierarchical Storage Management) ou **DLM** (Data Lifecycle Management). |

---

### Pi√®ge : ¬´ Le syndrome du /home satur√© ¬ª

Laisser les utilisateurs lancer des calculs produisant des **I/O massifs** dans leur r√©pertoire personnel (souvent NFS). **Sympt√¥me** : le serveur NFS s'effondre, bloquant le login de tous les utilisateurs et figeant des commandes basiques comme `ls`.

---

### Check-list production (Chapitre 8)

- [ ] **Quotas stricts** sur `/home` (capacit√© **et** inodes)
- [ ] Script de **purge automatique** (Robinhood ou `find`) actif et document√© pour le `/scratch`

---

## Chapitre 9 : Lustre ‚Äî Les entrailles (Internals)

### Objectifs d'apprentissage

- Cartographier l'**architecture logique et physique** de Lustre
- Comprendre le r√¥le du r√©seau **LNet** (Lustre Network)
- Appr√©hender **DNE** (Distributed Namespace) pour la scalabilit√© des m√©tadonn√©es

---

### 9.1 L'architecture Lustre

[Lustre](Glossaire-et-Acronymes#l) (Linux Cluster) **s√©pare physiquement** les **m√©tadonn√©es** (noms, permissions, arborescence) des **donn√©es** (contenu des fichiers).

| Composant | R√¥le |
|-----------|------|
| **MGS** (Management Server) | Registre global, d√©tient la configuration du cluster. Unique (mais HA), peu de ressources. |
| **MDS** (Metadata Server) & **MDT** (Metadata Target) | Serveur + disque (id√©alement NVMe) pour l'arborescence. Un client qui fait `ls` ou `open()` parle au MDS. |
| **OSS** (Object Storage Server) & **OST** (Object Storage Target) | Serveurs et disques capacitifs (HDD/SSD) qui stockent les **objets** (morceaux) des fichiers. |
| **Client Lustre** | Module kernel sur les n≈ìuds de calcul ; `mount -t lustre`. |

**Sch√©ma : Flux d'I/O Lustre**

```
 +-----------------+
 | Client (Node 1) |  1. open("/scratch/data.h5")
 +--------+--------+ ---------------------------> +-------+-------+
          |                                       | MDS / MDT 1   |
          |  2. Re√ßoit la liste des OSTs contenant | (M√©tadonn√©es) |
          |     les objets du fichier              +---------------+
          |
          |  3. Lit/√âcrit les donn√©es directement aux OSS (OS Bypass / RDMA)
          v
 +--------+--------+       +--------+--------+
 | OSS 1 / OST 1,2 |       | OSS 2 / OST 3,4 |
 | (Donn√©es Pures) |       | (Donn√©es Pures) |
 +-----------------+       +-----------------+
```

---

### 9.2 LNet (Lustre Network)

**LNet** est la couche d'abstraction r√©seau de Lustre. Elle permet de faire transiter les I/O de mani√®re transparente sur :

- **TCP** (Ethernet)
- **o2ib** (InfiniBand / RDMA)
- **Routage** entre les deux via des n≈ìuds **LNet Routers**

---

### 9.3 DNE (Distributed Namespace)

Historiquement, un Lustre n'avait qu'**un seul MDT** ‚Üí goulot d'√©tranglement massif. **DNE** permet de r√©partir l'arborescence sur **plusieurs MDTs** (ex. `/scratch/projet_A` sur MDT1, `/scratch/projet_B` sur MDT2), ou de distribuer un seul gros dossier sur plusieurs MDTs (DNE phase 2).

---

### Pi√®ge : ¬´ Le MDT sur disques lents ¬ª

Mettre un **MDT sur des HDD**. Les m√©tadonn√©es sont des I/O **minuscules et hautement al√©atoires**. Un HDD saturera √† ~200 IOPS, **figeant tout le cluster**. Un MDT doit **toujours** √™tre sur **NVMe/SSD** ou en RAM-cache (Optane).

---

### Check-list production (Chapitre 9)

- [ ] V√©rifier que le **MGS** et le **MDS** sont en **haute disponibilit√©** (Corosync/Pacemaker + disques partag√©s/multipath)

---

## Chapitre 10 : D√©ploiement, tuning et op√©rations Lustre

### Objectifs d'apprentissage

- D√©ployer et formater des cibles Lustre (**DANGER**)
- Ma√Ætriser le concept vital de **striping** (entrelacement)
- R√©aliser du **troubleshooting** de base via `lctl`

---

### 10.1 Le striping (entrelacement)

Un gros fichier peut √™tre **d√©coup√© en bandes** (stripes) et r√©parti sur **plusieurs OSTs**. Lorsqu'un job MPI lit ce fichier avec 1000 processus, il lit **simultan√©ment** sur des dizaines de serveurs ‚Üí multiplication de la bande passante par N.

**Exemples (c√¥t√© client) :**

```bash
# V√©rifier le striping d'un fichier existant
lfs getstripe /scratch/mon_fichier.dat

# R√©pertoire : nouveaux fichiers stripp√©s sur 4 OSTs, bandes de 2 MB
lfs setstripe -c 4 -S 2M /scratch/gros_run_mpi/

# Fichier "Wide Stripe" (tous les OSTs) ‚Äî id√©al pour fichier > 1 To
lfs setstripe -c -1 /scratch/massive_checkpoint.out
```

---

### 10.2 Formatage et d√©ploiement (c√¥t√© serveur)

Lustre s'appuie sur un backend : **ldiskfs** (ext4 modifi√©) ou **ZFS**.

**Exemple de d√©ploiement d'un OST (ldiskfs) :**

```bash
# DANGER : D√©truit toutes les donn√©es sur /dev/sdb
mkfs.lustre --reformat --fsname=lustre --ost --mgsnode=10.0.0.5@o2ib /dev/sdb

# Montage de l'OST
mkdir -p /mnt/ost1
mount -t lustre /dev/sdb /mnt/ost1
```

---

### 10.3 Op√©rations et lctl

**lctl** (Lustre Control) : outil d'administration.

```bash
# √âtat du r√©seau LNet et interfaces actives
lctl network up
lctl list_nids

# Statistiques de sant√© (MDS ou OSS)
lctl get_param health_check
```

---

### Pi√®ge : ¬´ L'over-striping ¬ª

Forcer un **stripe count √† -1** (tous les OSTs) pour des **millions de petits fichiers** de quelques Ko ‚Üí trafic de m√©tadonn√©es et de locks **catastrophique**.

> **R√®gle d'or** : Petit fichier = **1 OST** (d√©faut). Gros fichier = **plusieurs OSTs**.

---

### Check-list production (Chapitre 10)

- [ ] D√©finir des **Project Quotas** (Lustre supporte les quotas POSIX)
- [ ] Toujours **d√©monter proprement** les clients avant de rebooter un routeur LNet

---

## Chapitre 11 : Panorama des alternatives (BeeGFS, GPFS, CephFS)

### Objectifs d'apprentissage

- Comparer Lustre avec les autres standards du march√© HPC
- Identifier l'√©mergence du **stockage objet** (S3, DAOS) dans le HPC

---

### 11.1 BeeGFS

Alternative **open-source** (ThinkParQ) la plus populaire √† Lustre.

| Avantage | Inconv√©nient |
|----------|--------------|
| Tr√®s simple √† d√©ployer (services userspace, pas de modules kernel exotiques). M√©tadonn√©es **distribu√©es par d√©faut**. | Un peu moins performant que Lustre en acc√®s concurrent direct sur un seul tr√®s gros fichier partag√© (MPI-IO). |

---

### 11.2 GPFS / IBM Spectrum Scale

Syst√®me **orient√© blocs** (Lustre est orient√© objets).

| Avantage | Inconv√©nient |
|----------|--------------|
| Richesse fonctionnelle (ILM natif, snapshots fiables). Ultra-robuste en environnement ¬´ Entreprise ¬ª. | Co√ªt des licences, complexit√© (mmchconfig, mmcrfs). |

---

### 11.3 CephFS

Issu du monde Cloud/OpenStack, en mont√©e en puissance en HPC.

| Avantage | Inconv√©nient |
|----------|--------------|
| Tol√©rance aux pannes g√©r√©e par **CRUSH** (pas besoin de RAID mat√©riel complexe). | Surco√ªt CPU/r√©seau de la r√©plication ‚Üí souvent trop lent pour un `/scratch` pur calcul intensif ; **excellent** en `/project`. |

---

## üß™ Lab 3 : D√©ploiement d'un mini-Lustre (TCP) et analyse du striping

### √ânonc√©

Vous avez **3 VMs** (CentOS/Rocky) avec disques additionnels virtuels et le d√©p√¥t Lustre activ√©.

1. **vm-mgs** : Formatez un disque hybride **MGS/MDT**.
2. **vm-oss** : Formatez 2 disques (`/dev/sdb`, `/dev/sdc`) comme **OST0000** et **OST0001**.
3. **vm-client** : Montez le syst√®me de fichiers sur `/mnt/lustre`.
4. Cr√©ez un dossier `test_stripe`, configurez-le pour **2 stripes**.
5. G√©n√©rez un fichier de **100 MB** dedans et observez sa r√©partition mat√©rielle.

### Crit√®res de r√©ussite

- `lfs df -h` sur le client affiche le **MDT** et les **deux OSTs** avec leurs capacit√©s.
- `lfs getstripe /mnt/lustre/test_stripe/fichier_test` affiche l'utilisation des **deux OSTs** (index 0 et 1).

### Corrig√© (snippets)

```bash
# Sur vm-mgs (IP: 10.0.0.10)
mkfs.lustre --fsname=mini --mgs --mdt /dev/sdb
mount -t lustre /dev/sdb /mnt/mdt

# Sur vm-oss
mkfs.lustre --fsname=mini --mgsnode=10.0.0.10@tcp0 --ost --index=0 /dev/sdb
mkfs.lustre --fsname=mini --mgsnode=10.0.0.10@tcp0 --ost --index=1 /dev/sdc
mount -t lustre /dev/sdb /mnt/ost0
mount -t lustre /dev/sdc /mnt/ost1

# Sur vm-client
mount -t lustre 10.0.0.10@tcp0:/mini /mnt/lustre
lfs setstripe -c 2 /mnt/lustre/test_stripe
dd if=/dev/urandom of=/mnt/lustre/test_stripe/data.bin bs=1M count=100
lfs getstripe /mnt/lustre/test_stripe/data.bin
```

---

## üìù Examen de fin de volume 3

### QCM (1 point chaque)

**1.** Quel composant de Lustre est interrog√© lorsqu'un utilisateur ex√©cute `ls -l` ?  
- A) Le MGS  
- B) **Le MDT**  
- C) L'OST  

**2.** Pourquoi un fichier de 5 Ko ne devrait-il **pas** avoir un stripe count de 4 ?  
- A) Parce que Lustre ne g√®re pas les fichiers de moins de 1 Mo  
- B) **Parce que cela oblige le client √† contacter 4 OSTs pour r√©cup√©rer 5 Ko, augmentant la latence sans b√©n√©fice de d√©bit**  
- C) Parce que cela corrompt le fichier  

---

### Question ouverte (Exploitation)

Un chercheur signale que son job (qui g√©n√®re **5 millions de fichiers textes de 10 octets** chacun) **fige totalement** le cluster. Vous observez les m√©triques de la baie Lustre. **Quel composant** (MGS, MDS ou OSS) affiche une charge de 100 %, **pourquoi** ce workload est-il inadapt√© au stockage HPC, et **que proposez-vous** au chercheur ?

**R√©ponse attendue** : Le **MDS** (Metadata Server) sera √† 100 %. Cr√©er un fichier = cr√©er un inode (m√©tadonn√©e). 5 millions de micro-fichiers = **temp√™te d‚ÄôIOPS al√©atoires** (metadata storm). **Solution** : utiliser **HDF5**, **NetCDF**, ou regrouper les sorties via **MPI-IO** dans un seul gros fichier structur√© ; √† d√©faut, SQLite ou format d‚Äôarchive non compress√©e.

---

### √âtude de cas : ¬´ Le Split-Brain Lustre et la panique des locks ¬ª

Vous avez un cluster Lustre. Le **lien r√©seau du MDS** ¬´ bagotte ¬ª (d√©connexions/reconnexions toutes les 5 secondes).

1. **D√©crivez** l‚Äôimpact imm√©diat sur les clients Lustre en train d‚Äô√©crire.
2. **Expliquez** le m√©canisme d‚Äô**√©viction** (Lustre Eviction).
3. **Quelle commande** l‚Äôadministrateur peut-il utiliser sur le MDS pour forcer la purge des verrous obsol√®tes (ou permettre aux clients de crasher proprement) ?

**R√©ponses attendues :**

1. Les clients **bloquent** (hang) en mode **D state** (Uninterruptible Sleep) car ils perdent le contact avec le **DLM** (Distributed Lock Manager) du MDS.
2. Apr√®s un **timeout** (souvent 100 s, r√©glable via `obd_timeout`), le MDS consid√®re les clients inaccessibles et les **√©vince** : il d√©truit leurs verrous pour prot√©ger l‚Äôint√©grit√© du FS.
3. **`lctl clear`** ou reboot cibl√© du MDS (cassera les jobs en cours). La **vraie** solution est la **stabilisation du r√©seau LNet**.

---

## Solutions des QCM

- **Q1** : **B** ‚Äî `ls -l` interroge les m√©tadonn√©es (MDT).
- **Q2** : **B** ‚Äî Petit fichier = 1 OST ; multi-OST pour 5 Ko est contre-productif.

---

## üìö R√©f√©rences (Volume 3)

- OpenSFS & EOFS. (2024). *Lustre Operations Manual.*
- Shipman, G., et al. (2010). *Lustre: The default IO stack for the HPC community.*
- ThinkParQ. (2023). *BeeGFS Architecture Guide.*

---

## üìã Relecture qualit√© du volume 3

- [x] Couverture : Hi√©rarchisation, internals Lustre, striping, LNet, comparatifs
- [x] Rigueur technique : DLM, stripe-count, DNE
- [x] Format : Markdown, sch√©ma ASCII Lustre, blocs de code
- [x] P√©dagogie : Lab Lustre en VM, questions d‚Äôexamen orient√©es production

---

## Liens utiles

- **[Sommaire complet du Manuel HPC](Manuel-HPC-Sommaire-Complet)** : plan des 8 volumes, chapitres, labs
- **[Manuel Architecture HPC ‚Äî Volume 1](Manuel-Architecture-HPC-Volume1)** : fondations, provisioning
- **[Manuel Architecture HPC ‚Äî Volume 2](Manuel-Architecture-HPC-Volume2)** : r√©seaux, InfiniBand, s√©curit√©
- **[Cours HPC Complet](Cours-HPC-Complet)** : concepts, stockage, parall√©lisme
- **[Glossaire et Acronymes](Glossaire-et-Acronymes)** : Lustre, MDS, OST, LNet, HSM, etc.
- **[Home](Home)** : page d'accueil du wiki

---

**Volume 3** ‚Äî Stockage parall√®le et gestion des donn√©es (Deep Dive Lustre)  
**Derni√®re mise √† jour** : 2024
