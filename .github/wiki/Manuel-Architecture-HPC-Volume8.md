# üìö Manuel d'architecture et d'ing√©nierie HPC

**Volume 8 : Le fil rouge ¬´ De z√©ro √† la prod ¬ª et tendances de l'Exascale**

> **Niveau** : DevOps Senior / Architecte HPC ‚Äî **Public** : Master, Doctorat, architectes

---

## Vue d'ensemble du volume

Ce volume est l'**aboutissement** du manuel : synth√®se des connaissances en un **projet de d√©ploiement complet** (le ¬´ Fil Rouge ¬ª) et exploration des **fronti√®res** du calcul intensif ‚Äî Exascale, convergence IA-HPC, hybridation Cloud. On y traite les **phases chronologiques** du d√©ploiement (design, bare-metal, stockage, ordonnancement, s√©curisation, observabilit√©, tests de charge), les **crit√®res de Go-Live**, puis les **d√©fis Exascale** (√©nergie, DLC), la **convergence IA/HPC** et le **Cloud Bursting**. Le [Lab 11](#-lab-11--√©tude-de-cas-architecture--design) et l'[examen de fin de volume](#-examen-de-fin-de-volume-8-et-de-louvrage) cl√¥turent l'ouvrage.

**Pr√©requis :** Ma√Ætrise des [Volumes 1 √† 7](Manuel-Architecture-HPC-Volume1.md).

---

## Chapitre 25 : Projet guid√© complet ¬´ De la page blanche √† la production ¬ª

### Objectifs d'apprentissage

- Ordonnancer **chronologiquement** les √©tapes de d√©ploiement d'un supercalculateur
- √âviter les r√©gressions par une **m√©thodologie d'int√©gration continue**
- Valider les **crit√®res de passage en production** (Go-Live)

---

### 25.1 Phase 1 : Design et dimensionnement (Jours 1 √† 15)

Tout commence par le **Co-Design** avec les chercheurs.

| Th√®me | Questions cl√©s |
|-------|----------------|
| **Analyse du workload** | Deep Learning (GPU H100, NVLink) ou dynamique mol√©culaire (CPU denses, InfiniBand) ? |
| **Dimensionnement stockage** | Ratio capacit√© / bande passante. Ex. : 5 To/jour ‚Üí dur√©e de r√©tention sur `/scratch` Lustre ? Politique de purge. |
| **Topologie r√©seau** | Non-blocking (1:1) ou oversubscription (ex. 2:1) si budget limit√© et communications MPI peu denses. |

---

### 25.2 Phase 2 : Fondations bare-metal et r√©seau (Jours 16 √† 30)

| √âtape | Contenu |
|-------|---------|
| **Rack & Stack** | C√¢blage physique. **Cable Plan** strict (√©viter le croisement des c√¢bles InfiniBand). |
| **R√©seau OOB** | VLAN isol√© pour IPMI/Redfish (management). |
| **Provisioning** | N≈ìud Master, [Warewulf](Manuel-Architecture-HPC-Volume1.md) v4, image OS (ex. Rocky Linux 9) en RAM (tmpfs). |
| **Fabric IB** | Lancement d'**opensm**, v√©rification des liens (ibnetdiscover, pas de liens d√©grad√©s). |

---

### 25.3 Phase 3 : D√©ploiement du stockage parall√®le (Jours 31 √† 40)

| √âtape | Contenu |
|-------|---------|
| **SAN/Block** | LUNs mat√©riels (RAID 6 ou Declustered Parity). |
| **Lustre** | Formatage MGS, MDT (NVMe), OSTs (HDD capacitifs). |
| **LNet** | Interfaces Lustre sur InfiniBand (o2ib). |
| **Tests** | Premier **IOR** (I/O al√©atoire et s√©quentiel) pour valider la bande passante avant prod. |

---

### 25.4 Phase 4 : Ordonnancement et environnements (Jours 41 √† 50)

| √âtape | Contenu |
|-------|---------|
| **Identity** | FreeIPA/LDAP, synchronisation via SSSD sur les n≈ìuds. |
| **Slurm** | slurmctld, slurmdbd (MariaDB), slurmd ; **cgroups** (cgroup.conf) pour isolation CPU/RAM. |
| **Toolchains** | [Spack](Manuel-Architecture-HPC-Volume5.md) : GCC, Intel, OpenMPI, MVAPICH2 ; arbre **Lmod**. |

---

### 25.5 Phase 5 : S√©curisation, observabilit√© et tests de charge (Jours 51 √† 60)

| √âtape | Contenu |
|-------|---------|
| **Monitoring** | [Prometheus](Manuel-Architecture-HPC-Volume7.md), Grafana, node_exporter, slurm_exporter, Alertmanager. |
| **HPL & HPCG** | HPL sur **100 %** des n≈ìuds pendant **24 h** (burn-in thermique). |
| **Chaos Engineering** | D√©brancher une fibre pendant un job MPI ; red√©marrer le n≈ìud slurmctld primaire pour valider le **basculement HA**. |

---

### Check-list Go-Live

- [ ] Benchmarks d'acceptation (HPL/IOR) **‚â• 85 %** de la promesse constructeur
- [ ] **Runbook** des 10 pannes les plus courantes r√©dig√© et test√©
- [ ] Script de purge du `/scratch` activ√© en **dry-run**
- [ ] Portail de support utilisateur (Jira/GLPI) op√©rationnel

---

## Chapitre 26 : Hybridation Cloud et avenir du HPC

### Objectifs d'apprentissage

- Comprendre les **d√©fis physiques** de l'√®re Exascale (√©nergie, refroidissement)
- Diff√©rencier HPC traditionnel, **Cloud Bursting** et **convergence IA**

---

### 26.1 Le mur de l'√©nergie et l'Exascale

L'**Exascale** = **10¬π‚Å∏ FLOPS** (un milliard de milliards d'op√©rations/s). Le d√©fi n'est plus seulement le silicium, mais l'**alimentation** et le **refroidissement**.

**Efficacit√© √©nerg√©tique :**

```
E_sys = R_max / P_totale
```

*(R_max = performance mesur√©e, P_totale = consommation √©lectrique totale de la salle ; en GFLOPS/Watt.)*

Pour rester sous **20‚Äì30 MW** (puissance d'une petite ville), les syst√®mes abandonnent le refroidissement **par air** au profit du **DLC** (Direct Liquid Cooling) : eau ti√®de (jusqu'√† 45 ¬∞C) sur les dissipateurs CPU/GPU pour capter ~90 % de la chaleur.

---

### 26.2 La convergence IA et HPC

| Domaine | Pr√©cision typique | Usage |
|---------|-------------------|--------|
| **HPC classique** | FP64 (double) | Stabilit√© num√©rique, physique |
| **IA / Deep Learning** | FP32, FP16, FP8, INT4 | Entra√Ænement, inf√©rence |

L'IA **remplace ou acc√©l√®re** les solveurs traditionnels (ex. **PINNs** ‚Äî Physics-Informed Neural Networks ‚Äî pour des EDP 1000√ó plus rapides). ‚Üí Architecture **hybride** : CPU + **Tensor Cores** massivement parall√®les.

---

### 26.3 Le Cloud Bursting (HPC √©lastique)

Quand la file [Slurm](Guide-SLURM-Complet.md) locale **d√©borde**, le Cloud Bursting alloue des **n≈ìuds virtuels** sur un cloud public (AWS, GCP, Azure).

**Fonctionnement Slurm :**
- N≈ìuds Cloud avec √©tat **CLOUD** dans slurm.conf
- **ResumeProgram** : appel API pour d√©marrer les VMs
- **SuspendProgram** : destruction des VMs quand inactives ‚Üí arr√™t de la facturation

---

### Pi√®ge : ¬´ La gravit√© de la donn√©e ¬ª (Data Gravity)

D√©porter le **calcul** dans le Cloud en laissant les **donn√©es** (ex. 100 To) sur le Lustre **on-premise** ‚Üí les n≈ìuds Cloud passent **90 %** du temps (factur√©) √† attendre les donn√©es via le lien VPN. Si des donn√©es sont **g√©n√©r√©es dans le cloud** et rapatri√©es, les **co√ªts d'egress** explosent.

> **R√®gle** : Le calcul doit aller **vers la donn√©e**. En bursting, les donn√©es doivent √™tre **synchronis√©es** dans le cloud (S3, FSx for Lustre, etc.) au pr√©alable.

---

## üß™ Lab 11 : √âtude de cas ¬´ Architecture & Design ¬ª (exercice sur table)

### √ânonc√©

Vous √™tes l'**architecte HPC** d'un institut de recherche en **climatologie**.

- **Budget** : 4 M‚Ç¨  
- **Contrainte √©lectrique** : 200 kW max pour la salle  
- **Code** : Mod√®le **WRF** (fortement MPI, bande passante m√©moire ; **pas de GPU**)  
- **Donn√©es** : 50 To/semaine  

Proposez une **architecture macroscopique** (type processeur, ratio compute/login, interconnexion, syst√®me de fichiers, dimensionnement). **Expliquez** pourquoi vous excluez les GPU.

### Crit√®res de r√©ussite (corrig√© conceptuel)

| Composant | Choix | Justification |
|-----------|--------|----------------|
| **Compute** | **CPU-only**. Processeurs √† fort ratio bande passante m√©moire / c≈ìur (ex. AMD EPYC Genoa-X, Intel Xeon Max avec HBM). | WRF n'exploite pas les GPU ‚Üí GPU = budget et quota √©lectrique gaspill√©s. |
| **R√©seau** | InfiniBand NDR ou RoCE v2 **non-blocking** (Fat-Tree 1:1). | Communications de halos WRF permanentes, sensibles √† la latence. |
| **Stockage** | Lustre ou BeeGFS ~**1 Po** utilisable. **Tiering** vers bandes ou S3 pour archivage. | 50 To/semaine ‚Üí purge agressive ou archivage pour p√©rennit√©. |
| **√ânergie** | 200 kW restrictif ‚Üí processeurs TDP mod√©r√© et/ou **Direct Liquid Cooling** pour r√©duire climatisation et ventilateurs. | |

---

## üìù Examen de fin de volume 8 (et de l'ouvrage)

### QCM (1 point chaque)

**1.** Quelle est la **limite physique** majeure poussant vers le **Direct Liquid Cooling (DLC)** pour l'Exascale ?  
- A) L'eau conduit mieux les signaux √©lectriques que le cuivre  
- B) **L'air n'a plus la capacit√© d'absorption thermique suffisante pour des racks √† 50‚Äì100 kW de dissipation (CPU/GPU ultra-denses)**  
- C) Les pompes √† eau prennent moins de place que les SSD  

**2.** Qu'est-ce que la **¬´ Data Gravity ¬ª** en contexte Cloud Bursting HPC ?  
- A) **La difficult√© √† d√©placer de tr√®s grands volumes de donn√©es, qui ¬´ attirent ¬ª naturellement le calcul vers elles**  
- B) L'usure plus rapide des disques sur site que dans le cloud  
- C) La compression algorithmique des donn√©es scientifiques  

---

### Question ouverte (Exploitation et gouvernance)

**Expliquez** pourquoi le **Chaos Engineering** (simulation intentionnelle de pannes : d√©brancher le n≈ìud primaire slurmctld ou un routeur LNet en production simul√©e) est une **√©tape cruciale** du jalon **Go-Live** (Phase 5).

**R√©ponse attendue** : Un plan de **HA non test√©** est une th√©orie dangereuse. Le Chaos Engineering **valide** que le failover fonctionne en conditions de stress, que les **alertes** (Prometheus) remontent aux bonnes personnes et que le **MTTR** des √©quipes sur les Runbooks est acceptable **avant** que de vrais utilisateurs soient impact√©s.

---

### √âtude de cas : ¬´ La facture sal√©e du Bursting ¬ª

Un centre a configur√© des n≈ìuds **CLOUD** dans slurm.conf (ResumeProgram AWS). Objectif : g√©rer les pics. Au bout d'un mois : **150 000 ‚Ç¨** de facture AWS au lieu de **10 000 ‚Ç¨** pr√©vus.

Les logs Slurm montrent que des **milliers de petits jobs** (dur√©e **30 s**) ont √©t√© envoy√©s sur les n≈ìuds CLOUD. Les instances Cloud mettent **2‚Äì3 min** √† d√©marrer.

1. **Expliquez** l'inefficacit√© technique et financi√®re.  
2. **Quel param√®tre** (QOS ou limites de partition) aurait d√ª restreindre le Cloud Bursting √† certains types de calculs ?

**R√©ponses attendues :**

1. Facturation √† la **minute** (ou minimum forfaitaire). Allumer une VM (3 min), ex√©cuter 30 s, √©teindre ‚Üí **h√©r√©sie √©conomique** : overhead d'initialisation √©norme, co√ªt par job prohibitif.  
2. **MinTime** sur la partition Cloud (ex. `MinTime=02:00:00`) pour √©viter les micro-jobs ; ou r√©server la partition √† des **QOS** sp√©cifiques (projets avec budget Cloud allou√©).

---

## Solutions des QCM

- **Q1** : **B** ‚Äî DLC pour dissiper 50‚Äì100 kW par rack.  
- **Q2** : **A** ‚Äî Data Gravity = la donn√©e ¬´ attire ¬ª le calcul.

---

## üìö R√©f√©rences (Volume 8)

- Dongarra, J., et al. (2022). *The Exascale computing project.*  
- Reed, D. A., et al. (2015). Exascale computing and big data. *Communications of the ACM.*  
- Google SRE (2016). *Site Reliability Engineering: How Google Runs Production Systems.* O'Reilly Media.

---

## üìã Relecture qualit√© du volume 8 (finale)

- [x] Couverture : Fil rouge (5 phases), Exascale, DLC, Data Gravity, IA vs HPC, Cloud Bursting  
- [x] Rigueur technique : TDP, efficacit√© √©nerg√©tique (GFLOPS/Watt), ResumeProgram/SuspendProgram Slurm  
- [x] Format : Markdown, formules (E_sys), questions adapt√©es Master/PhD  
- [x] P√©dagogie : Lab architecture syst√©mique (design sous contraintes budget/√©nergie)

---

## Liens utiles

- **[Sommaire complet du Manuel HPC](Manuel-HPC-Sommaire-Complet.md)** : plan des 8 volumes  
- **[Manuel Architecture HPC ‚Äî Vol. 1 √† 7](Manuel-Architecture-HPC-Volume1.md)** : fondations √† observabilit√©  
- **[Guide SLURM Complet](Guide-SLURM-Complet.md)** : partitions, CLOUD, QOS  
- **[Glossaire et Acronymes](Glossaire-et-Acronymes.md)** : DLC, Exascale, SRE, etc.  
- **[Home](Home.md)** : page d'accueil du wiki  

---

**Volume 8** ‚Äî Le fil rouge ¬´ De z√©ro √† la prod ¬ª et tendances de l'Exascale  
**Derni√®re mise √† jour** : 2024  

*Le corps du Manuel (8 volumes) est achev√© : cycle complet de l'ing√©nierie HPC, de la gestion m√©moire NUMA jusqu'√† la strat√©gie Cloud et Exascale.*
