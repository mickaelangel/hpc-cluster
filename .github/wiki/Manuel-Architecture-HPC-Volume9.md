# üìö Manuel d'architecture et d'ing√©nierie HPC

**Volume 9 : Data Science & Machine Learning sur cluster HPC (de l'ETL au training distribu√©)**

> **Niveau** : Ing√©nieur / Master / Doctorat ‚Äî **Public** : Data Scientists, MLOps, chercheurs, admins HPC  
> **Objectif** : passer d'un notebook "mono-GPU" √† un **pipeline reproductible** et un **entra√Ænement multi-n≈ìuds** performant sous **Slurm**.  
> **Chapitres** : 27 √† 30 (suite du Volume 8).

---

## Vue d'ensemble du volume

Le HPC "classique" (MPI/CFD) et le ML/IA convergent : **m√™mes supercalculateurs**, m√™mes contraintes (I/O, r√©seau, quotas, fairshare), mais des **patterns logiciels** diff√©rents (datasets shard√©s, checkpoints, collectifs NCCL, hyperparam search, tracking).  
Ce volume apporte une m√©thode "senior" pour :

- construire des **pipelines Data/ML** adapt√©s √† un stockage parall√®le (Lustre/BeeGFS/GPFS) ;
- lancer un **training distribu√©** (PyTorch DDP) sur **plusieurs n≈ìuds GPU** via Slurm ;
- industrialiser : **reproductibilit√©**, **tra√ßabilit√©**, **profiling**, **scaling study**, **MLOps on-prem**.

**Pr√©requis recommand√©s :**
- Slurm : allocations, partitions, job arrays (voir [Guide SLURM Complet](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Guide-SLURM-Complet.md)).
- Connaissances GPU (CUDA) et bases de PyTorch (ou √©quivalent).
- Notions de performance : latence, bande passante, saturation I/O (voir Vol. 6).
- Vol. 5 : NCCL, GPU, Apptainer (labs 6 & 7).

---

## Chapitre 27 : Workloads Data/ML sur HPC ‚Äî data locality, I/O et formats

### Objectifs d'apprentissage

- Comprendre pourquoi le **goulot** ML est souvent **la donn√©e** (pas le GPU)
- Choisir un **format** et un **pattern d'acc√®s** compatibles avec un FS parall√®le
- Savoir quand utiliser **cache NVMe local**, sharding, pr√©fetch, et quand **ne pas** le faire

---

### 27.1 Le triptyque "Compute / I/O / R√©seau" (et pourquoi le ML casse les habitudes)

En ML, on a souvent :

- **beaucoup** de lectures (dataset) ;
- **des √©critures p√©riodiques** (checkpoints) ;
- un **r√©seau** sollicit√© par les collectifs (all-reduce) ;
- une sensibilit√© √©norme √† la **variance** (un OST satur√© ‚Üí training instable).

> R√®gle empirique : si l'utilisation GPU oscille (ex. 20% ‚Üî 95%) sans raison, suspectez d'abord **I/O** ou **data loader** avant "le mod√®le".

---

### 27.2 Formats : Parquet / Zarr / HDF5 / TFRecords / WebDataset

| Format | Forces | Limites | Quand l'utiliser |
|--------|--------|---------|------------------|
| **Parquet** | Excellent pour tabulaire, pr√©dicats, compression | Moins naturel pour images/audio | ETL, features, analytics, Spark/Dask |
| **Zarr** | Chunking natif, cloud-friendly, bon pour arrays N-D | Demande un bon choix de chunks | Imagerie scientifique, grilles 3D, climat |
| **HDF5** | Tr√®s utilis√© en science, structure riche | Concurrence d'acc√®s, tuning n√©cessaire | Sci/Simu, gros tableaux + metadata |
| **TFRecords** | Streaming s√©quentiel efficace | √âcosyst√®me TF | Training TF, lecture s√©quentielle |
| **WebDataset (tar shards)** | Sharding simple, moins de "petits fichiers" | Op√©rations d'update plus complexes | Images/texte : dataset en tar.* shard√©s |

**Anti-pattern n¬∞1 : "des millions de petits fichiers"**  
M√™me si le FS parall√®le est puissant, l'overhead metadata tue le throughput. Pr√©f√©rez : *shards* (tar/parquet/zarr chunks).

---

### 27.3 Sharding & taille des shards (la partie la plus sous-estim√©e)

But : r√©duire les seeks, augmenter la s√©quentialit√©, amortir la latence metadata.

**Heuristiques (√† adapter) :**
- Images : shards **256 Mo √† 2 Go**
- NLP : shards **1 √† 10 Go** (selon tokenisation / streaming)
- HPC data : chunks Zarr cal√©s sur "un minibatch/worker"

> Un shard trop petit = metadata ; trop gros = moins de parall√©lisme et "stragglers".

---

### 27.4 Cache NVMe local : quand c'est magique‚Ä¶ et quand c'est dangereux

**Cas o√π c'est excellent :**
- Datasets "read-only" r√©utilis√©s souvent
- N≈ìuds GPU qui relancent des runs similaires (grid search)
- FS r√©seau tr√®s charg√©

**Cas o√π c'est un pi√®ge :**
- Datasets √©normes (copie plus longue que l'entra√Ænement)
- Nettoyage absent ‚Üí `/scratch_local` satur√©
- Incoh√©rences (dataset versionn√© mais cache stale)

**Pattern robuste :**
1) calculer un **hash/version** de dataset (manifest, DVC ou √©quivalent)
2) si absent localement ‚Üí **rsync** (ou `tar`/`aria2`) vers NVMe
3) pointer le loader sur le cache
4) purge contr√¥l√©e (LRU, quota)

---

### 27.5 Checkpoints : strat√©gie "√©crire moins, reprendre mieux"

- **Fr√©quence** : bas√©e sur *time-to-recover*, pas sur "toutes les N it√©rations"
- **Asynchrone** si possible (thread/process d√©di√©)
- **Sharded checkpoint** (FSDP/DeepSpeed) pour r√©duire le "stall"
- **Compression** prudente : parfois CPU devient le bottleneck

**Pi√®ge :** checkpoint sur un seul fichier "monstre" ‚Üí contention (lock), saturation OST unique.  
Pr√©f√©rer : **plusieurs fichiers** (shards) ou "save per rank".

---

### Check-list production (Chapitre 27)

- [ ] Dataset shard√© (pas "N millions de petits fichiers")
- [ ] Mesure objective : *GPU utilization* + *dataloader time* + *throughput I/O*
- [ ] Politique de checkpoint (r√©tention, purge, emplacement)
- [ ] Option cache NVMe document√©e + purge
- [ ] Donn√©es versionn√©es (manifest + hash, ex. DVC) pour reproductibilit√©

---

### Points cl√©s √† retenir (Ch. 27)

- Le goulot ML est souvent l'I/O ; formats et sharding sont critiques sur FS parall√®le.
- √âviter les millions de petits fichiers ; privil√©gier Parquet/Zarr/WebDataset selon le cas.
- Cache NVMe : utile si dataset r√©utilis√© et purge ma√Ætris√©e ; dangereux si cache stale ou quota non g√©r√©.
- Checkpoints : fr√©quence raisonnable, asynchrone si possible, shards pour limiter contention.

---

## Chapitre 28 : Entra√Ænement distribu√© sur Slurm ‚Äî DDP, rendezvous, NCCL

### Objectifs d'apprentissage

- Lancer un **PyTorch DDP multi-n≈ìuds** de mani√®re robuste avec Slurm
- Comprendre les variables d'environnement et les sympt√¥mes **NCCL**
- Mettre en place une d√©marche de **scaling study** (strong/weak)

---

### 28.1 Les trois modes "standard" de lancement DDP

| Mode | Principe | Avantages | Risques |
|------|----------|-----------|---------|
| **`srun python train.py`** | Slurm lance N t√¢ches, votre code init le process group | Simple, "HPC natif" | Il faut bien mapper rank/world_size |
| **`torchrun`** | PyTorch g√®re rendezvous + ranks | Standard ML | Interaction Slurm √† ma√Ætriser |
| **Submitit** | G√©n√®re job Slurm + configure env PyTorch | Tr√®s productif | Abstraction √† comprendre, sinon debug dur |

---

### 28.2 Template sbatch (multi-n≈ìuds GPU) ‚Äî "golden path"

> Hypoth√®se : 2 n≈ìuds, 4 GPU/n≈ìud, 1 process par GPU.  
> **Note** : `-c 8` (CPUs per task) doit √™tre coh√©rent avec `num_workers` du DataLoader pour √©viter contention.

```bash
#!/bin/bash
#SBATCH -J ddp-demo
#SBATCH -p gpu
#SBATCH -N 2
#SBATCH --ntasks-per-node=4
#SBATCH --gpus-per-node=4
#SBATCH -c 8
#SBATCH --time=02:00:00
#SBATCH -o logs/%x-%j.out
#SBATCH -e logs/%j.err

module purge
module load opensource/miniforge
conda activate ml

# Debug utile en phase d'int√©gration
export NCCL_DEBUG=INFO
export TORCH_DISTRIBUTED_DEBUG=DETAIL

# Adresse ma√Ætre = 1er n≈ìud de l'allocation
MASTER_ADDR=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
MASTER_PORT=29500

# Lancement : 1 process par t√¢che Slurm
srun --kill-on-bad-exit=1 \
  python -u train.py \
    --dist \
    --master_addr "$MASTER_ADDR" --master_port "$MASTER_PORT"
```

**Mapping minimal c√¥t√© Python :**
- `RANK = int(os.environ["SLURM_PROCID"])`
- `WORLD_SIZE = int(os.environ["SLURM_NTASKS"])`
- `LOCAL_RANK = int(os.environ["SLURM_LOCALID"])`

**Pi√®ge** : sur certains clusters, `MASTER_PORT` doit √™tre dans une plage autoris√©e ; v√©rifier la politique (ex. 29500‚Äì29600).

---

### 28.3 D√©bugger NCCL : m√©thode syst√©matique

**Sympt√¥mes typiques :**
- freeze au d√©marrage (rendezvous, connectivit√©)
- crash `unhandled system error` (iface r√©seau, driver, mismatch)
- performance anormalement basse (mauvaise iface, topo, oversubscription)

**M√©thode "du plus simple au plus probable" :**
1) **TCP OK** entre n≈ìuds (ping, ports ouverts, DNS)
2) v√©rifier `CUDA_VISIBLE_DEVICES` par t√¢che (`srun env | grep CUDA_VISIBLE_DEVICES`)
3) forcer l'interface : `export NCCL_SOCKET_IFNAME=eth0` (adapter au cluster)
4) si InfiniBand mal configur√© : `export NCCL_IB_DISABLE=1` (fallback TCP) ou `NCCL_NET` selon doc NCCL
5) augmenter logs : `NCCL_DEBUG=INFO` (+ √©ventuellement `NCCL_DEBUG_SUBSYS=ALL`)
6) valider collectifs avec `all_reduce` minimal avant d'entra√Æner le mod√®le

> Un "training qui d√©marre" n'est pas un "training correct" : un mauvais binding peut diviser la perf par 5.

---

### 28.4 Topologie-aware : ne pas gaspiller NVLink / PCIe / NUMA

- 1 process/GPU : standard DDP
- binding CPU : √©vite la contention dataloader (`--cpu-bind` via srun si n√©cessaire)
- si NVLink : privil√©gier des tailles de batch / buckets qui amortissent les collectifs

**Pi√®ge :** `num_workers` √©norme + FS parall√®le satur√© ‚Üí CPU bound + I/O bound.

---

### 28.5 Scaling study : comment prouver que √ßa scale (et pas juste "√ßa tourne")

**Strong scaling (dataset fixe)**
- objectif : r√©duire le temps
- limite : overhead comms + inefficacit√© batch

**Weak scaling (batch/global augmente)**
- objectif : maintenir le temps/epoch ~ constant
- limite : qualit√© (LR scaling), stabilit√©, m√©moire

**M√©triques √† produire :**
- samples/s (global)
- temps/epoch
- % temps comms vs compute (profiling)
- taux d'√©chec / repro (stabilit√©)

---

### Check-list production (Chapitre 28)

- [ ] "Hello all-reduce" valid√© sur N n≈ìuds
- [ ] `NCCL_DEBUG=INFO` en phase test, puis OFF en prod
- [ ] Interface r√©seau ma√Ætris√©e (`NCCL_SOCKET_IFNAME`)
- [ ] Strat√©gie de scaling document√©e (strong/weak + graphiques)

---

### Points cl√©s √† retenir (Ch. 28)

- DDP multi-n≈ìuds : MASTER_ADDR/MASTER_PORT + variables Slurm (PROCID, NTASKS, LOCALID) ; template sbatch + srun = chemin robuste.
- Debug NCCL : connectivit√© TCP, CUDA_VISIBLE_DEVICES, NCCL_SOCKET_IFNAME, NCCL_IB_DISABLE si besoin, puis logs INFO.
- Scaling study (strong/weak) et m√©triques (samples/s, temps comms) pour prouver l'efficacit√©.

---

## Chapitre 29 : Orchestration √† l'√©chelle ‚Äî job arrays, Submitit, Optuna, Ray/Dask/Spark

### Objectifs d'apprentissage

- Orchestrer 100‚Äì10 000 runs (grid / random / Bayesian) **sans casser le cluster**
- Comprendre quand utiliser **job arrays** vs Ray/Dask/Spark
- Mettre des garde-fous (quotas, backfill, priorit√©s) et √©viter les anti-patterns

---

### 29.1 Job arrays : le meilleur outil "simple et robuste"

Cas d'usage : hyperparam search, ablations, seeds multiples.

```bash
#SBATCH --array=0-199%20   # 200 runs, max 20 en parall√®le
```

- `%20` prot√®ge le cluster et votre quota
- `SLURM_ARRAY_TASK_ID` indexe une config (YAML/JSON)

**Pi√®ge :** 10 000 jobs individuels sans `%` ‚Üí scheduler flood.

---

### 29.2 Submitit : productivit√© Python + discipline HPC

Submitit automatise la soumission Slurm depuis Python, et g√®re le "torch distributed env" (ranks).

Usages typiques :
- lancer une **fonction** Python en batch
- faire un `map_array` pour soumettre un lot de runs
- r√©cup√©rer les logs/retours proprement

**Hydra** (multirun) est une alternative courante pour des runs pilot√©s par fichier de config (plusieurs configs = plusieurs runs).

---

### 29.3 Optuna (ou √©quivalent) sur Slurm : pattern recommand√©

- **Study** centralis√©e (SQLite/PostgreSQL)
- chaque trial = un job Slurm (array ou submitit)
- agr√©gation par un script "collector"

**R√®gle d'or :** le scheduler doit rester la source de v√©rit√© des ressources.

**Pi√®ge :** SQLite sur NFS/Lustre peut poser des probl√®mes de verrouillage ; pr√©f√©rer PostgreSQL ou un backend fichier sur stockage local (scratch du n≈ìud) si beaucoup de writers.

---

### 29.4 Ray/Dask/Spark sur Slurm : quand √ßa vaut le coup

| Framework | Quand | Attention |
|----------|-------|-----------|
| **Dask** | ETL Python, pandas √† l'√©chelle, arrays | Surveiller scheduler + spill disque |
| **Ray** | pipelines ML, tuning, actors | D√©marrage cluster **dans une allocation Slurm** (jamais sur le n≈ìud de login) |
| **Spark** | data lake, SQL, gros ETL | Int√©gration stockage + shuffle |

> Si vous avez 30 runs ind√©pendants : job arrays.  
> Si vous avez un DAG de t√¢ches d√©pendantes + scheduling applicatif : Ray/Dask.

---

### Anti-patterns fr√©quents (Chapitre 29)

| Anti-pattern | Pourquoi c'est mauvais | Alternative |
|-------------|------------------------|-------------|
| "Un Ray cluster permanent sur le login node" | Contourne l'ordonnanceur, risque s√©curit√© | Ray/Dask **dans** une allocation Slurm |
| "10 000 petits jobs d'1 minute" | Overhead scheduler | Bundling, arrays, t√¢ches plus grosses |
| "Un seul script qui spawn 1000 processes" | Non comptabilis√©, casse la politique | `srun`, arrays, submitit |

---

### Points cl√©s √† retenir (Ch. 29)

- Job arrays avec `%K` = outil simple et robuste pour hyperparam / seeds ; √©viter le flood du scheduler.
- Submitit/Hydra = productivit√© Python tout en restant dans le cadre Slurm.
- Ray/Dask/Spark : √† lancer **dans** un job Slurm, pas sur le login.

---

## Chapitre 30 : Reproductibilit√©, MLOps on-prem, et perf-to-solution

### Objectifs d'apprentissage

- Construire un pipeline **reproductible** (environnement + donn√©es + code)
- Mettre en place un minimum de **MLOps** sans cloud public
- Mesurer et am√©liorer la **perf-to-solution** (pas juste "FLOPS")

---

### 30.1 Reproductibilit√© : les 4 axes

1) **Code** : Git + tags + CI
2) **Environnement** : Conda-lock / Spack-lock / conteneur Apptainer
3) **Donn√©es** : version (hash), manifest, sharding stable
4) **Ex√©cution** : param√®tres, seed, logs, m√©triques

**Minimum viable :**
- `requirements.lock` (ou `environment.yml` + lock)
- `config.yaml` versionn√©
- `run_id` unique (timestamp + git sha)
- sauvegarde des m√©triques (CSV/JSON)

---

### 30.2 Tracking d'exp√©riences on-prem (MLflow : l'exemple classique)

- serveur MLflow interne (HTTP)
- stockage artifacts sur `/scratch` ou S3 interne
- tra√ßabilit√© : params, metrics, artifacts, model registry (selon maturit√©)

**Alternatives** : TensorBoard (logs locaux), Weights & Biases (W&B) en mode self-hosted ou limit√©, ou simplement CSV + dossier `runs/` disciplin√©.

> Pour un cours, m√™me un CSV + dossier `runs/` disciplin√© est d√©j√† une √©norme progression.

---

### 30.3 Profiling ML : relier sympt√¥mes et causes

- **GPU** : Nsight Systems / Nsight Compute
- **CPU** : perf, py-spy
- **I/O** : iostat, lustre stats, temps dataloader
- **R√©seau** : collectifs (temps all-reduce), saturation NIC

**Plan de profiling recommand√© :**
1) single GPU (baseline)
2) multi-GPU mono-n≈ìud (NVLink/PCIe)
3) multi-n≈ìuds (r√©seau)
4) augmenter batch / workers progressivement

---

### 30.4 Perf-to-solution : le KPI qui parle √† tout le monde

**D√©finition** : temps (ou co√ªt GPU-heures / √©nergie) pour atteindre une **cible de solution** (ex. pr√©cision, loss) ‚Äî plut√¥t que de ne regarder que les TFLOPS.

Au lieu de "TFLOPS", on veut :

- temps pour atteindre une pr√©cision cible
- co√ªt √©nerg√©tique / GPU-hours
- taux de reprise apr√®s incident (checkpoint)

---

### Check-list production (Chapitre 30)

- [ ] environnement fig√© (lock/containers)
- [ ] donn√©es versionn√©es (manifest + hash)
- [ ] tracking minimal (params/metrics/artifacts)
- [ ] baseline + scaling study versionn√©s

---

### Points cl√©s √† retenir (Ch. 30)

- Reproductibilit√© : code (Git) + env (lock/container) + donn√©es (hash/manifest) + ex√©cution (config, seed, run_id).
- Tracking : MLflow, TensorBoard ou CSV disciplin√© ; artifacts sur scratch/S3 interne.
- Perf-to-solution = temps/co√ªt pour atteindre la cible (pr√©cision, loss), pas seulement FLOPS.

---

## üß™ Lab 12 : PyTorch DDP multi-n≈ìuds via Slurm + Apptainer (avec debugging NCCL)

### Objectif

Lancer un entra√Ænement DDP **sur 2 n≈ìuds GPU** (1 process/GPU), mesurer le scaling, et diagnostiquer un probl√®me r√©seau simul√©.

### √âtapes

1) Construire un conteneur Apptainer (ou utiliser un module) avec PyTorch + CUDA  
2) Soumettre un job Slurm DDP (template Chap. 28.2)  
3) V√©rifier :
   - `CUDA_VISIBLE_DEVICES` par t√¢che
   - `nvidia-smi` par n≈ìud
   - logs `NCCL_DEBUG=INFO` (phase debug)
4) Mesurer : `samples/s`, temps/epoch (2 runs : 1 n≈ìud puis 2 n≈ìuds)
5) Simuler une erreur : mauvaise iface (`NCCL_SOCKET_IFNAME`) **ou** `MASTER_PORT` d√©j√† utilis√© ‚Üí interpr√©ter logs ‚Üí corriger

### Crit√®res de r√©ussite

- Le run 2 n≈ìuds est **plus rapide** que 1 n≈ìud (strong scaling) *ou* expliquez pourquoi non (bottleneck I/O, batch trop petit, comms).
- Vous produisez un mini-rapport : command lines, m√©triques, conclusion.

---

## üß™ Lab 13 : Recherche d'hyperparam√®tres (Submitit + Optuna) et agr√©gation de r√©sultats

### Objectif

Lancer 50‚Äì200 trials sans "flooder" Slurm, agr√©ger les r√©sultats, et sortir le meilleur run reproductible.

### √âtapes

1) D√©finir un espace de recherche (LR, batch, weight decay)  
2) Utiliser job arrays **ou** Submitit `map_array` avec un "cap" de concurrence  
3) Stocker :
   - `config.yaml` trial
   - m√©triques (CSV/JSON)
   - artifact (checkpoint minimal)
4) Script d'agr√©gation : top-k + export tableau + re-run du meilleur (m√™me seed/env)

### Crit√®res de r√©ussite

- Concurrence contr√¥l√©e (`%MAX`) et logs exploitables
- Re-run du meilleur trial = r√©sultat coh√©rent (m√©trique √† ¬±1 % ou √©cart expliqu√©)

---

## üìù Examen de fin de volume 9

### QCM (1 point chaque)

**1.** Pourquoi "des millions de petits fichiers" posent probl√®me sur un stockage parall√®le ?  
- A) Parce que les GPU ne savent pas lire des petits fichiers  
- B) **Parce que l'overhead metadata (cr√©ation/stat/open) domine et r√©duit fortement le throughput**  
- C) Parce que Parquet ne supporte pas les petits fichiers

**2.** Dans un job DDP multi-n≈ìuds, quelle variable est souvent utilis√©e pour choisir l'interface r√©seau NCCL ?  
- A) `CUDA_VISIBLE_DEVICES`  
- B) `OMP_NUM_THREADS`  
- C) **`NCCL_SOCKET_IFNAME`**

**3.** Quel outil Slurm est le plus "naturel" pour lancer 200 runs ind√©pendants sans saturer le scheduler ?  
- A) Lancer 200 `ssh` depuis le login node  
- B) **`--array=...%K` (job arrays avec limite de concurrence)**  
- C) Un unique job qui lance 200 processus non g√©r√©s

---

### Question ouverte (architecture + m√©thode)

Vous devez entra√Æner un mod√®le sur un dataset de 200 To sur un cluster :  
- FS parall√®le (Lustre), n≈ìuds GPU avec NVMe local, r√©seau InfiniBand.  

Proposez une strat√©gie compl√®te : **format + sharding + cache + checkpoints + lancement Slurm + m√©triques de scaling**.  
Expliquez aussi comment vous d√©boguez un **freeze NCCL** au d√©marrage.

---

## R√©f√©rences conseill√©es (lecture)

- Documentation Slurm (sbatch/srun), politiques GPU, cgroups  
- Documentation PyTorch Distributed / DDP (multi-node)  
- Documentation NCCL (variables d'environnement, debugging)  
- [Submitit](https://github.com/facebookresearch/submitit) (soumission Slurm depuis Python)  
- [Optuna](https://optuna.readthedocs.io/) (optimisation hyperparam√®tres, int√©gration Slurm)  
- [WebDataset](https://webdataset.github.io/webdataset/) (sharding tar pour ML)  
- Guides Ray/Dask sur Slurm (cas o√π scheduling applicatif est pertinent)  
- Nsight Systems / Nsight Compute (profiling GPU NVIDIA)

---

[‚Üê Sommaire Manuel](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Manuel-HPC-Sommaire-Complet.md) ¬∑ [‚Üê Accueil](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Home.md)
