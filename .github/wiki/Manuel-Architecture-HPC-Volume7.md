# üìö Manuel d'architecture et d'ing√©nierie HPC

**Volume 7 : Observabilit√©, MCO et incidentologie**

> **Niveau** : DevOps Senior / Architecte HPC ‚Äî **Public** : Master, Doctorat, ing√©nieurs SRE

---

## Vue d'ensemble du volume

Un supercalculateur n'a de valeur que s'il est **disponible, surveill√© et pr√©dictible**. Ce volume couvre le **Site Reliability Engineering (SRE)** appliqu√© au HPC : **stack d'observabilit√©** (m√©triques, logs, Pull vs Push, exporters Slurm/Lustre/GPU), **capacity planning** et **SLA** (allocation vs utilisation, showback/chargeback, SLOs), puis **runbooks**, **on-call** et **post-mortems** (RCA blameless, MTTR). Le [Lab 10](#-lab-10--d√©ploiement-de-lobservabilit√©-prometheus--slurm) et l'[examen de fin de volume](#-examen-de-fin-de-volume-7) permettent de valider les acquis.

**Pr√©requis :**
- Notions de bases de donn√©es, JSON/YAML, TCP/UDP (Ch. 22)
- Statistiques descriptives de base (Ch. 23)
- Exp√©rience d'environnements de production (Ch. 24)

---

## Chapitre 22 : Stack d'observabilit√© HPC (m√©triques et logs)

### Objectifs d'apprentissage

- D√©ployer une architecture de surveillance bas√©e sur des **s√©ries temporelles** (TSDB)
- Collecter les **m√©triques sp√©cifiques** de [Slurm](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Guide-SLURM-Complet.md) et de Lustre
- **Centraliser** et parser les logs distribu√©s

---

### 22.1 Le paradigme Pull vs Push

Sur des **milliers de n≈ìuds**, deux mod√®les s'opposent :

| Mod√®le | Exemple | Comportement | Risque |
|--------|---------|--------------|--------|
| **Push** | InfluxDB / Telegraf | Chaque n≈ìud **envoie** ses m√©triques au serveur central | Temp√™te de m√©triques (DDoS interne) si le serveur ralentit |
| **Pull** | **Prometheus** | Le serveur central **scrappe** (aspire) les m√©triques expos√©es par chaque n≈ìud (HTTP, souvent port 9100) | Le serveur **contr√¥le la cadence** ‚Üí standard moderne |

---

### 22.2 Les exporters vitaux en HPC

Un **exporter** expose les donn√©es (OS ou application) au format **Prometheus**.

| Exporter | R√¥le | O√π tourne |
|----------|------|-----------|
| **node_exporter** | CPU, RAM, I/O disque local, r√©seau | Tous les n≈ìuds |
| **slurm_exporter** | Jobs Pending/Running, √©tat des n≈ìuds (Drain, Down) ; interroge slurmctld | Management / frontal |
| **lustre_exporter** | IOPS, m√©tadonn√©es, remplissage des OSTs ; lit `/proc/fs/lustre` (MGS/MDS/OSS) | N≈ìuds stockage |
| **dcgm_exporter** | (NVIDIA) Temp√©rature, Watts, utilisation c≈ìurs Tensor ; indispensable pour l'IA | N≈ìuds GPU |

**Sch√©ma : Flux de l'observabilit√©**

```
+-------------------+      +-------------------+
| Compute Nodes     |      | Management Node   |
| - node_exporter   |<--+  |                   |       +---------------+
| - dcgm_exporter   |   |  | +---------------+ |       | Utilisateur / |
+-------------------+   |  | | Prometheus    | |       | Administrateur|
                        +--+-| (Scraper)     | |<----->| (Grafana)     |
+-------------------+   |  | +---------------+ |       +---------------+
| Storage Nodes     |   |  |         |         |
| - lustre_exporter |<--+  |         v         |
+-------------------+      | +---------------+ |
                           | | Alertmanager  | |------> (Email, Slack,
                           | +---------------+ |         PagerDuty)
                           +-------------------+
```

---

### 22.3 Centralisation des logs

Un job MPI plante sur **compute-084** : on ne se connecte pas en SSH pour lire `/var/log/messages`. Les logs doivent √™tre **exp√©di√©s** (rsyslog, **Promtail**, **Filebeat**) vers un point central (**Elasticsearch** ou **Loki**).

---

### Pi√®ge : ¬´ L'effet d'observateur ¬ª

Configurer Prometheus avec **scrape_interval: 1s** sur un cluster de **2000 n≈ìuds** ‚Üí le r√©seau d'admin et les CPU des n≈ìuds sont **satur√©s** par la surveillance. **R√®gle** : **15 √† 30 s** suffisent pour l'infrastructure.

---

### Check-list production (Chapitre 22)

- [ ] Configurer **Alertmanager** pour **regrouper** (group) les alertes : 500 n≈ìuds down ‚Üí **1** alerte ¬´ Switch Spine Down ¬ª, pas 500 emails
- [ ] **S√©curiser** les endpoints Prometheus (reverse-proxy ou r√®gles pare-feu)

---

## Chapitre 23 : Capacity planning et SLA

### Objectifs d'apprentissage

- Mod√©liser la **croissance** du cluster et anticiper la **saturation**
- Distinguer **allocation** et **utilisation r√©elle**
- D√©finir et mesurer des **SLOs** (Service Level Objectives) pertinents en HPC

---

### 23.1 Allocation vs utilisation (l'illusion du GPU)

Slurm indique qu'un GPU est **¬´ Allou√© ¬ª** (100 % r√©serv√©). **dcgm_exporter** indique si le GPU **calcule vraiment** (utilisation 80 %) ou est **en veille** (5 %) en attendant le CPU.

> Le **capacity planning** ne se base **jamais** sur l'allocation seule : sinon on ach√®te du mat√©riel inutile. Il faut **optimiser les codes** avant d'ajouter des ressources.

---

### 23.2 Showback et Chargeback

| Concept | R√¥le |
|--------|------|
| **Showback** | Tableau de bord informatif : ¬´ Ce mois-ci, vous avez consomm√© l'√©quivalent de 50 000 ‚Ç¨ de temps de calcul ¬ª. Responsabilisation, pas de facturation. |
| **Chargeback** | **Facturation r√©elle** √† partir de l'accounting Slurm (`sacct`). Exige une **pr√©cision absolue**. |

---

### 23.3 SLOs (Service Level Objectives) en HPC

Les m√©triques SRE en HPC diff√®rent du web (99,99 % uptime). Exemples :

| SLO | Cible typique |
|-----|----------------|
| **Queue Wait Time** | 95 % des jobs demandant &lt; 10 n≈ìuds d√©marrent en **&lt; 4 h** |
| **I/O Latency** | Cr√©ation d'un fichier (m√©tadonn√©e Lustre) **&lt; 5 ms** au **p99** |
| **Job Success Rate** | **&lt; 1 %** des jobs √©chouent √† cause d'une **d√©faillance infrastructure** (hors bug utilisateur) |

---

## Chapitre 24 : Runbooks, on-call et post-mortems (RCA)

### Objectifs d'apprentissage

- R√©diger des **SOP** (Standard Operating Procedures) **actionnables**
- G√©rer une **crise** en production
- R√©diger un **Root Cause Analysis (RCA) blameless** (sans bl√¢me)

---

### 24.1 Le Runbook (SOP)

√Ä 3 h du matin, face √† l'alarme **¬´ Lustre OST Full ¬ª**, l'ing√©nieur d'astreinte **ex√©cute**, ne r√©fl√©chit pas. Le Runbook est un document concis :

| Section | Contenu |
|---------|---------|
| **Sympt√¥me** | Ex. : alerte LustreOSTCapacityWarning sur PagerDuty |
| **V√©rification** | `lfs df -h` pour confirmer |
| **Action imm√©diate** | Identifier les gros consommateurs (`lfs quota -h /scratch`), avertir ou purger (`find /scratch -atime +30 -delete`) |
| **Escalade** | Si OST &gt; 95 %, bloquer les nouvelles √©critures et appeler l'expert niveau 3 |

---

### 24.2 Le Post-Mortem (RCA blameless)

Chaque **incident majeur** g√©n√®re un rapport. **R√®gle d'or** (Google SRE) : **Blameless RCA** ‚Äî on suppose que l'op√©rateur a pris la meilleure d√©cision avec les infos disponibles. On cherche **pourquoi le syst√®me a permis l'erreur**, pas qui a faut√©.

**M√©trique de fiabilit√© :**

```
MTTR = (Œ£ Temps de r√©paration de tous les incidents) / (Nombre total d'incidents)
```

*(Mean Time To Recovery. En HPC, le MTTR compte souvent plus que le MTBF : un cluster plantera ; la question est √† quelle vitesse on le relance.)*

---

### DANGER en prod : ¬´ Le fix √† la main non document√© ¬ª

Modifier un fichier de configuration **directement sur le n≈ìud** sans reporter la modification dans **Ansible/GitOps** ‚Üí √† la prochaine ex√©cution d'Ansible, l'incident **se reproduit**.

---

## üß™ Lab 10 : D√©ploiement de l'observabilit√© (Prometheus + Slurm)

### √ânonc√©

Sur votre n≈ìud **Master** :

1. T√©l√©chargez et lancez **Prometheus** (binaire pr√©-compil√©).
2. Installez **slurm_exporter** (GitHub).
3. Configurez **prometheus.yml** pour scrapper `localhost:9100` (node_exporter) et `localhost:8080` (slurm_exporter).
4. Lancez quelques jobs via **sbatch** et interrogez Prometheus (`http://master:9090`) pour afficher la m√©trique **slurm_jobs_running**.

### Crit√®res de r√©ussite

- La page **/targets** de Prometheus affiche l'exporter Slurm avec le statut **UP**.
- La requ√™te PromQL **sum(slurm_jobs_running)** affiche une valeur **&gt; 0** lorsque des jobs s'ex√©cutent.

### Corrig√© (snippets)

**prometheus.yml :**

```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: "slurm"
    static_configs:
      - targets: ["localhost:8080"]
  - job_name: "nodes"
    static_configs:
      - targets: ["localhost:9100", "node01:9100", "node02:9100"]
```

**Lancement :**

```bash
./prometheus --config.file=prometheus.yml &
./slurm_exporter &
```

---

## üìù Examen de fin de volume 7

### QCM (1 point chaque)

**1.** Quelle est la diff√©rence majeure entre le mod√®le **Push** (Telegraf) et le mod√®le **Pull** (Prometheus) ?  
- A) Le mod√®le Pull est moins s√©curis√©  
- B) Le mod√®le Push force le serveur central √† aspirer les donn√©es, ce qui surcharge son CPU  
- C) **Dans le mod√®le Pull, c'est le serveur central qui d√©cide de la fr√©quence de collecte, √©vitant d'√™tre submerg√©**  

**2.** Un job alloue **4 GPU A100** mais **dcgm_exporter** indique **2 %** d'utilisation et une **VRAM vide**. Quel est le probl√®me probable ?  
- A) Les GPU sont en surchauffe (thermal throttling)  
- B) **Le code n'utilise pas les GPU (ou CUDA/PyTorch mal charg√©) ; le job gaspille des ressources**  
- C) Le r√©seau InfiniBand est en panne  

---

### Question ouverte (Analyse de SLA)

Le directeur se plaint que le **Wait Time** (temps d'attente en file) a **explos√©** depuis 3 mois. Sur Grafana, l'**utilisation globale CPU** du cluster est pourtant √† **60 %** (40 % de n≈ìuds inactifs).

**Expliquez** pourquoi un cluster peut avoir des **jobs en file d'attente** alors qu'il est **presque √† moiti√© vide**. (Indice : au-del√† des CPU, que demande un job ?)

**R√©ponse attendue** : **Fragmentation des ressources** ou **goulot annexe**. Les jobs en attente demandent peut-√™tre des **GPU** (satur√©s √† 100 %), des **licences** √©puis√©es, ou une **grosse quantit√© de RAM** (les n≈ìuds libres n'ont plus de m√©moire malgr√© des c≈ìurs libres). Il est aussi possible que le **Backfill** ne puisse pas placer les jobs si les utilisateurs ne fournissent pas de **walltime** pr√©cis.

---

### √âtude de cas : ¬´ Post-Mortem du vendredi noir ¬ª

Vendredi 17 h. Un admin met √† jour **slurm.conf** via Ansible et lance **scontrol reconfigure**. **Tous les n≈ìuds** passent en **DOWN**, **5000 jobs** sont tu√©s.

1. **Quelle erreur** classique dans slurm.conf peut provoquer un **rejet massif** des n≈ìuds par le contr√¥leur ? (penser : caract√©ristiques n≈ìuds, m√©moire, auth.)  
2. **Quelle √©tape** manquait dans la pipeline (CI/CD ou SOP) **avant** la commande en production ?  
3. **R√©digez** l'**action corrective** pour √©viter la r√©cidive.

**R√©ponses attendues :**

1. Modification d'une **caract√©ristique critique** (ex. **RealMemory**, nombre de c≈ìurs) sans v√©rifier. Si slurmctld attend 256 Go de RAM et que slurmd annonce 255 Go, le contr√¥leur consid√®re le n≈ìud **incoh√©rent** et le marque **DOWN**.

2. **Validation** : ex√©cuter **slurmd -C** sur un n≈ìud pour v√©rifier la config mat√©rielle vue par Slurm, ou **tester** le nouveau slurm.conf sur un **staging**. Pas de **slurmctld -t** (test config) avant red√©marrage.

3. **Action Item** : Int√©grer un **linter** ou un **script de pr√©-flight** dans le pipeline Ansible qui lance **slurmctld -t** avant tout red√©marrage des services en production.

---

## Solutions des QCM

- **Q1** : **C** ‚Äî Pull : le serveur contr√¥le la cadence.  
- **Q2** : **B** ‚Äî Allocation ‚â† utilisation ; le code ne tire pas parti des GPU.

---

## üìã Relecture qualit√© du volume 7

- [x] Couverture : Exporters, Pull vs Push, capacity planning, SLO/SLA (wait time, IOPS), post-mortems blameless
- [x] Rigueur technique : Showback/Chargeback, Allocation/Utilisation (dcgm_exporter), formule MTTR
- [x] Format : Titres clairs, sch√©ma ASCII monitoring
- [x] P√©dagogie : Cas d'√©tude incident Slurm (SOP, CI/CD)

---

## Liens utiles

- **[Sommaire complet du Manuel HPC](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Manuel-HPC-Sommaire-Complet.md)** : plan des 8 volumes, chapitres, labs
- **[Manuel Architecture HPC ‚Äî Vol. 1 √† 6](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Manuel-Architecture-HPC-Volume1.md)** : fondations √† performances
- **[Monitoring](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Monitoring.md)** : Prometheus, Grafana, dashboards
- **[Guide SLURM Complet](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Guide-SLURM-Complet.md)** : slurmctld, accounting, sinfo
- **[Glossaire et Acronymes](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Glossaire-et-Acronymes.md)** : SLO, MTTR, TSDB, etc.
- **[Home](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Home.md)** : page d'accueil du wiki

---

**Volume 7** ‚Äî Observabilit√©, MCO et incidentologie  
**Derni√®re mise √† jour** : 2024
