# ğŸ“š Cours HPC Complet â€” Master Data Science / Doctorat

> **Formation exhaustive Calcul Haute Performance â€” Niveau DevOps Senior & Recherche**

---

## ğŸ¯ Objectifs pÃ©dagogiques

Ã€ l'issue de ce cours, vous maÃ®triserez :

- Les **concepts fondamentaux** du calcul haute performance (HPC)
- Lâ€™**architecture** des clusters et supercalculateurs
- Les **technologies** (schedulers, MPI, stockage parallÃ¨le, rÃ©seau)
- Le **fonctionnement** dâ€™un cluster de bout en bout
- Les **bonnes pratiques** pour jobs, performances et coÃ»ts

**Public** : Ã‰tudiants Master Data Science, Doctorat, ingÃ©nieurs et administrateurs HPC.

---

## 1. Introduction au calcul haute performance (HPC)

### 1.1 Quâ€™est-ce que le HPC ?

Le **calcul haute performance** (High Performance Computing) regroupe :

- Lâ€™utilisation de **plusieurs processeurs** (ou cÅ“urs) pour rÃ©soudre un problÃ¨me plus vite
- Lâ€™utilisation de **plusieurs nÅ“uds** (machines) reliÃ©s par un rÃ©seau rapide
- Lâ€™optimisation du **code** et des **donnÃ©es** pour tirer parti du parallÃ©lisme

**Domaines typiques** : simulation (climat, physique, chimie), intelligence artificielle / deep learning, gÃ©nomique, finance, ingÃ©nierie (CFD, Ã©lÃ©ments finis).

### 1.2 ParallÃ©lisme : concepts de base

| Type | Description | Exemple |
|------|-------------|--------|
| **ParallÃ©lisme de donnÃ©es** | MÃªme code, donnÃ©es diffÃ©rentes (SIMD, data parallelism) | EntraÃ®nement ML sur plusieurs GPU |
| **ParallÃ©lisme de tÃ¢ches** | TÃ¢ches indÃ©pendantes en parallÃ¨le | Embarrassingly parallel jobs |
| **ParallÃ©lisme Ã  mÃ©moire partagÃ©e** | Threads sur un nÅ“ud (OpenMP) | Boucles parallÃ¨les sur un serveur |
| **ParallÃ©lisme Ã  mÃ©moire distribuÃ©e** | Processus sur plusieurs nÅ“uds (MPI) | Simulation multi-nÅ“uds |

### 1.3 MÃ©triques clÃ©s

- **Speedup** : \( S(n) = T(1) / T(n) \) (temps sur 1 processeur / temps sur n processeurs)
- **EfficacitÃ©** : \( E(n) = S(n) / n \)
- **ScalabilitÃ©** : comportement de S(n) et E(n) quand n augmente (forte/faible scalabilitÃ©)
- **Flops** : opÃ©rations en virgule flottante par seconde (FLOPS, GFLOPS, TFLOPS, PFLOPS)

---

## 2. Architecture des clusters HPC

### 2.1 Composants dâ€™un cluster

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚           RÃ‰SEAU UTILISATEURS           â”‚
                    â”‚              (Internet / LAN)            â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  NÅ’UDS FRONTAUX (Login / Management)     â”‚
                    â”‚  - Authentification (LDAP, Kerberos)    â”‚
                    â”‚  - Scheduler (Slurm controller)          â”‚
                    â”‚  - NFS / home, soumission de jobs        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                               â”‚                               â”‚
        â–¼                               â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RÃ‰SEAU CALCUL â”‚             â”‚ RÃ‰SEAU STOCKAGEâ”‚             â”‚ RÃ‰SEAU MGMT   â”‚
â”‚ (Interconnect) â”‚             â”‚ (Storage)      â”‚             â”‚ (Admin)       â”‚
â”‚ InfiniBand/    â”‚             â”‚ 10G/25G/100G   â”‚             â”‚ 1G/10G        â”‚
â”‚ Ethernet RoCE  â”‚             â”‚                â”‚             â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                             â”‚                             â”‚
        â–¼                             â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NÅ’UDS DE CALCULâ”‚             â”‚ SYSTÃˆMES       â”‚             â”‚ MONITORING    â”‚
â”‚ (Compute nodes)â”‚             â”‚ FICHIERS       â”‚             â”‚ Prometheus,   â”‚
â”‚ - CPU / GPU    â”‚             â”‚ Lustre, BeeGFS,â”‚             â”‚ Grafana, etc. â”‚
â”‚ - MÃ©moire      â”‚             â”‚ NFS, Ceph      â”‚             â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 RÃ´les des nÅ“uds

| RÃ´le | RÃ´le | Exemple |
|------|------|--------|
| **Frontal (login)** | Connexion SSH, Ã©dition, soumission de jobs, compilation | frontend01, frontend02 |
| **ContrÃ´leur** | Scheduler (Slurm controller), gestion des jobs et nÅ“uds | Souvent sur un frontal |
| **Calcul (compute)** | ExÃ©cution des jobs uniquement, pas de login direct | compute01..compute06 |
| **Stockage** | Serveurs de fichiers parallÃ¨les (metadata + data) | MDS, OSS (Lustre), etc. |

### 2.3 RÃ©seaux dans un cluster

- **RÃ©seau de management** : administration, monitoring, PXE, NFS racine.
- **RÃ©seau de calcul (interconnect)** : communication MPI, faible latence, haut dÃ©bit (InfiniBand, Ethernet 10G/25G/100G, RoCE).
- **RÃ©seau de stockage** : trafic vers systÃ¨mes de fichiers parallÃ¨les (Lustre, BeeGFS, etc.).

---

## 3. Ordonnancement des jobs : rÃ´le du scheduler

### 3.1 Pourquoi un scheduler ?

- **Partage Ã©quitable** des ressources entre utilisateurs et projets
- **File dâ€™attente** : les jobs en attente sont ordonnancÃ©s selon politiques (prioritÃ©, QoS, fair-share)
- **Ã‰viter la surcharge** : pas dâ€™exÃ©cution directe sur les nÅ“uds de calcul par les utilisateurs

### 3.2 Schedulers courants

| Scheduler | Usage typique | Points forts |
|-----------|----------------|--------------|
| **Slurm** | TrÃ¨s rÃ©pandu (acadÃ©mique, labs, cloud HPC) | Open source, riche, communautÃ© active |
| **PBS Pro / OpenPBS** | Enterprise, certains centres nationaux | Politiques avancÃ©es, support commercial |
| **SGE / Univa Grid Engine** | Historique, certains clusters | CompatibilitÃ© legacy |
| **LSF** | Enterprise (IBM) | IntÃ©gration Ã©cosystÃ¨me IBM |

Dans ce projet, le scheduler utilisÃ© est **Slurm**. Voir [Guide-SLURM-Complet](Guide-SLURM-Complet).

### 3.3 Cycle de vie dâ€™un job

1. **Soumission** : `sbatch script.sh` ou `srun` (interactif)
2. **File dâ€™attente** : le job est **PENDING** jusquâ€™Ã  attribution de ressources
3. **Allocation** : le scheduler alloue nÅ“uds/CPU/GPU/mÃ©moire
4. **ExÃ©cution** : le job passe en **RUNNING**
5. **Fin** : **COMPLETED**, **FAILED**, **CANCELLED**, **TIMEOUT**, etc.

---

## 4. Programmation parallÃ¨le : MPI et OpenMP

### 4.1 MPI (Message Passing Interface)

- **ModÃ¨le** : mÃ©moire distribuÃ©e, communication par messages entre processus.
- **Utilisation** : applications qui sâ€™exÃ©cutent sur **plusieurs nÅ“uds** (plusieurs processus, souvent 1 par cÅ“ur ou par nÅ“ud).
- **ImplÃ©mentations** : OpenMPI, Intel MPI, MPICH, MVAPICH (InfiniBand).

**Concepts** : communicateur, rang, envoi/rÃ©ception (point-Ã -point), collectives (broadcast, reduce, scatter, gather), types dÃ©rivÃ©s.

### 4.2 OpenMP

- **ModÃ¨le** : mÃ©moire partagÃ©e, parallÃ©lisme de boucles et de rÃ©gions sur **un seul nÅ“ud** (multi-threads).
- **Utilisation** : parallÃ©lisme Ã  lâ€™intÃ©rieur dâ€™un nÅ“ud ; souvent combinÃ© avec MPI (MPI entre nÅ“uds, OpenMP dans le nÅ“ud = hybride MPI+OpenMP).

### 4.3 ModÃ¨le hybride MPI + OpenMP

- **MPI** : un processus par nÅ“ud (ou par socket) pour la communication inter-nÅ“uds.
- **OpenMP** : plusieurs threads par processus pour utiliser tous les cÅ“urs du nÅ“ud.
- RÃ©duit le volume de messages MPI et peut amÃ©liorer les performances sur des nÅ“uds multi-cÅ“urs.

---

## 5. Stockage et systÃ¨mes de fichiers

### 5.1 HiÃ©rarchie typique

| Espace | Usage | CaractÃ©ristiques |
|--------|--------|-------------------|
| **Home** | RÃ©pertoire personnel, petits fichiers, sauvegardes | NFS, quotas, sauvegardes |
| **Scratch / work** | DonnÃ©es temporaires de calcul, gros I/O | Fichier parallÃ¨le, haute bande passante, purge |
| **Project / shared** | DonnÃ©es de projet partagÃ©es | NFS ou parallÃ¨le selon taille |
| **Local** | Disque local nÅ“ud (si prÃ©sent) | TrÃ¨s rapide, non partagÃ©, Ã©phÃ©mÃ¨re |

### 5.2 SystÃ¨mes de fichiers parallÃ¨les

| SystÃ¨me | ModÃ¨le | Points forts |
|---------|--------|--------------|
| **Lustre** | ParallÃ¨le, metadata servers + object storage servers | TrÃ¨s gros dÃ©bits, trÃ¨s rÃ©pandu en HPC |
| **BeeGFS** | ParallÃ¨le, metadata + storage servers | Installation plus simple, bon pour clusters moyens |
| **GlusterFS** | DistribuÃ©, pas de metadata central | Scalable, rÃ©plication |
| **Ceph** | Objet (object storage) + interfaces bloc/fichier | UnifiÃ© bloc/fichier/objet, rÃ©plication |
| **NFS** | CentralisÃ© | Simple, pour home et petits partages |

### 5.3 Bonnes pratiques I/O

- PrivilÃ©gier les **gros transferts sÃ©quentiels** plutÃ´t que beaucoup de petits fichiers
- Utiliser **scratch** pour les gros I/O et **ne pas** y garder les seules copies
- Ã‰viter les accÃ¨s trÃ¨s petits et alÃ©atoires sur un systÃ¨me parallÃ¨le partagÃ©
- Utiliser les **API parallÃ¨les** (MPI-IO, HDF5 parallÃ¨le, NetCDF) quand câ€™est possible

---

## 6. GPU et accÃ©lÃ©ration

### 6.1 RÃ´le des GPU en HPC

- **Calcul vectoriel/matrice** : algÃ¨bre linÃ©aire, deep learning, simulations ciblÃ©es
- **ModÃ¨le** : beaucoup de cÅ“urs lÃ©gers, mÃ©moire dÃ©diÃ©e (VRAM), transferts CPUâ€“GPU Ã  minimiser

### 6.2 Environnements courants

- **CUDA** (NVIDIA) : langage et librairies (cuBLAS, cuDNN, etc.)
- **ROCm** (AMD) : Ã©quivalent pour GPU AMD
- **oneAPI / SYCL** : approche plus portable (Intel, NVIDIA, AMD)

### 6.3 IntÃ©gration avec le scheduler

- Slurm gÃ¨re les **GPU** comme ressource (Generic Resource, `--gres=gpu:n` ou `--gres=gpu:type:n`).
- Variables dâ€™environnement typiques : `CUDA_VISIBLE_DEVICES`, `GPU_DEVICE_ORDINAL` (selon configuration).

---

## 7. Conteneurs et HPC

### 7.1 IntÃ©rÃªt des conteneurs en HPC

- **ReproductibilitÃ©** : mÃªme environnement (libs, versions) sur tout le cluster
- **PortabilitÃ©** : image unique pour dev, test et production
- **Isolation** : pas de conflit entre versions dâ€™outils ou de librairies

### 7.2 Outils

- **Docker** : build et partage dâ€™images (souvent utilisÃ© en dehors des nÅ“uds de calcul)
- **Singularity / Apptainer** : conÃ§u pour HPC (pas de dÃ©mon root, support MPI, GPU)
- **Podman** : alternative Ã  Docker, rootless

Sur un cluster Slurm, les jobs GPU ou MPI utilisent en gÃ©nÃ©ral **Singularity/Apptainer** pour lancer lâ€™image sur les nÅ“uds allouÃ©s.

---

## 8. SÃ©curitÃ© et bonnes pratiques

- **Authentification centralisÃ©e** : LDAP, Kerberos ou FreeIPA
- **Quotas** : CPU-heures, nombre de jobs, taille stockage
- **Politiques** : limites par partition, par QoS, fair-share
- **Audit** : logs de soumission, dâ€™exÃ©cution et dâ€™accÃ¨s aux donnÃ©es sensibles
- **Mise Ã  jour** : correctifs sÃ©curitÃ© sur OS, scheduler et logiciels critiques

---

## 9. Monitoring et observabilitÃ©

- **MÃ©triques** : utilisation CPU/RAM/GPU, files dâ€™attente, taux de succÃ¨s des jobs
- **Outils** : Prometheus (mÃ©triques), Grafana (tableaux de bord), Ã©ventuellement InfluxDB, Loki pour les logs
- **Alertes** : nÅ“uds down, files dâ€™attente anormalement longues, panne de stockage

Voir [Monitoring](Monitoring) et [Commandes-Utiles](Commandes-Utiles).

---

## 10. SynthÃ¨se et suite

Ce cours pose les bases **conceptuelles** et **architecturales** du HPC. Pour aller plus loin dans ce projet :

- **[Guide-SLURM-Complet](Guide-SLURM-Complet)** : utilisation avancÃ©e de Slurm (partitions, QoS, script sbatch, bonnes pratiques)
- **[Glossaire-et-Acronymes](Glossaire-et-Acronymes)** : dÃ©finitions et acronymes (HPC, MPI, Slurm, Lustre, etc.)
- **[Configuration-de-Base](Configuration-de-Base)** : dÃ©ploiement concret du cluster
- **[Commandes-Utiles](Commandes-Utiles)** : commandes de rÃ©fÃ©rence (Slurm, monitoring, dÃ©pannage)

---

**Niveau** : Master / Doctorat / DevOps Senior  
**DerniÃ¨re mise Ã  jour** : 2024
