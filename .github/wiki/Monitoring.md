# ğŸ“Š Monitoring - Guide Complet

> **Guide complet du monitoring - Niveau DevOps Senior**

---

## ğŸ¯ Vue d'Ensemble

Le cluster HPC utilise une stack de monitoring complÃ¨te :
- **Prometheus** : Collecte et stockage des mÃ©triques
- **Grafana** : Visualisation et dashboards
- **InfluxDB** : Base de donnÃ©es temporelle
- **Loki** : Logs agrÃ©gÃ©s
- **Alertmanager** : Gestion des alertes

---

## ğŸ“ˆ Prometheus

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prometheus  â”‚
â”‚  (Collecte) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â–º Node Exporter (MÃ©triques systÃ¨me)
       â”œâ”€â”€â–º Slurm Exporter (MÃ©triques Slurm)
       â”œâ”€â”€â–º cAdvisor (MÃ©triques conteneurs)
       â””â”€â”€â–º Exporters personnalisÃ©s
```

### Configuration

**Fichier** : `/etc/prometheus/prometheus.yml`

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'hpc-cluster'

scrape_configs:
  - job_name: 'node'
    static_configs:
      - targets: ['localhost:9100']
        labels:
          instance: 'frontend01'
          role: 'frontend'
```

### MÃ©triques ClÃ©s

**SystÃ¨me** :
- `node_cpu_seconds_total` : Utilisation CPU
- `node_memory_MemTotal_bytes` : MÃ©moire totale
- `node_filesystem_size_bytes` : Taille du systÃ¨me de fichiers
- `node_network_receive_bytes_total` : RÃ©seau entrant

**Slurm** :
- `slurm_jobs_total` : Nombre total de jobs
- `slurm_jobs_running` : Jobs en cours
- `slurm_nodes_total` : Nombre de nÅ“uds
- `slurm_nodes_idle` : NÅ“uds inactifs

---

## ğŸ“Š Grafana

### Dashboards Disponibles

1. **System Overview** : Vue d'ensemble systÃ¨me
2. **Slurm Cluster** : Ã‰tat du cluster Slurm
3. **Node Metrics** : MÃ©triques par nÅ“ud
4. **Network** : Statistiques rÃ©seau
5. **Storage** : Utilisation du stockage

### CrÃ©er un Dashboard

**Via l'interface web** :
1. Aller dans **Dashboards** > **New Dashboard**
2. Ajouter un **Panel**
3. Configurer la requÃªte PromQL
4. Personnaliser la visualisation

**Exemple de requÃªte** :
```promql
100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
```

### Variables de Template

```json
{
  "templating": {
    "list": [
      {
        "name": "instance",
        "type": "query",
        "query": "label_values(node_cpu_seconds_total, instance)"
      }
    ]
  }
}
```

---

## ğŸ’¾ InfluxDB

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Telegraf â”‚â”€â”€â–º Collecte des mÃ©triques
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ InfluxDB â”‚â”€â”€â–º Stockage temporel
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grafana  â”‚â”€â”€â–º Visualisation
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configuration Telegraf

**Fichier** : `/etc/telegraf/telegraf.conf`

```toml
[[outputs.influxdb_v2]]
  urls = ["http://localhost:8086"]
  token = "YOUR_TOKEN"
  organization = "hpc-cluster"
  bucket = "metrics"

[[inputs.cpu]]
  percpu = true
  totalcpu = true

[[inputs.mem]]
[[inputs.disk]]
[[inputs.net]]
```

### RequÃªtes Flux

```flux
from(bucket: "metrics")
  |> range(start: -1h)
  |> filter(fn: (r) => r._measurement == "cpu")
  |> aggregateWindow(every: 1m, fn: mean)
```

---

## ğŸš¨ Alertes

### Configuration Alertmanager

**Fichier** : `/etc/alertmanager/alertmanager.yml`

```yaml
global:
  resolve_timeout: 5m

route:
  group_by: ['alertname', 'cluster']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'

receivers:
  - name: 'default'
    webhook_configs:
      - url: 'http://localhost:5001/webhook'
```

### RÃ¨gles d'Alerte

**Fichier** : `/etc/prometheus/alerts.yml`

```yaml
groups:
  - name: system_alerts
    rules:
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "CPU usage is high on {{ $labels.instance }}"
```

---

## ğŸ“ Logs avec Loki

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Promtail â”‚â”€â”€â–º Collecte des logs
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Loki   â”‚â”€â”€â–º Stockage des logs
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grafana  â”‚â”€â”€â–º Visualisation
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Configuration Promtail

**Fichier** : `/etc/promtail/config.yml`

```yaml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://localhost:3100/loki/api/v1/push

scrape_configs:
  - job_name: system
    static_configs:
      - targets:
          - localhost
        labels:
          job: varlogs
          __path__: /var/log/*.log
```

---

## ğŸ” MÃ©triques PersonnalisÃ©es

### Exporter PersonnalisÃ©

**Exemple Python** :
```python
from prometheus_client import start_http_server, Gauge
import time

custom_metric = Gauge('custom_metric', 'Description')

def update_metric():
    while True:
        custom_metric.set(42)
        time.sleep(10)

if __name__ == '__main__':
    start_http_server(8000)
    update_metric()
```

**Configuration Prometheus** :
```yaml
scrape_configs:
  - job_name: 'custom'
    static_configs:
      - targets: ['localhost:8000']
```

---

## ğŸ“Š Dashboards RecommandÃ©s

### 1. System Overview

**MÃ©triques** :
- CPU usage par nÅ“ud
- MÃ©moire utilisÃ©e
- Disque utilisÃ©
- RÃ©seau entrant/sortant

**RequÃªtes** :
```promql
100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
```

### 2. Slurm Cluster

**MÃ©triques** :
- Jobs en cours
- Jobs en attente
- NÅ“uds actifs/inactifs
- Utilisation des partitions

**RequÃªtes** :
```promql
slurm_jobs_running
slurm_jobs_pending
slurm_nodes_idle
```

### 3. Storage

**MÃ©triques** :
- Espace disque utilisÃ©
- IOPS
- Latence

**RequÃªtes** :
```promql
(node_filesystem_size_bytes - node_filesystem_avail_bytes) / node_filesystem_size_bytes * 100
```

---

## ğŸ¯ Bonnes Pratiques

### 1. RÃ©tention Optimale

```yaml
# Prometheus
retention: 15d  # Pour les mÃ©triques dÃ©taillÃ©es

# InfluxDB
retention: 30d  # Pour les mÃ©triques agrÃ©gÃ©es
```

### 2. AgrÃ©gation

```yaml
# RÃ¨gles d'enregistrement
- record: job:node_cpu_usage:avg
  expr: avg(rate(node_cpu_seconds_total[5m])) by (job)
```

### 3. Labels Efficaces

```yaml
# âœ… Bon : Labels spÃ©cifiques
node_cpu_seconds_total{instance="compute01", mode="idle"}

# âŒ Mauvais : Trop de labels
node_cpu_seconds_total{instance="compute01", mode="idle", job="node", cluster="hpc"}
```

---

## ğŸ“š Ressources

- **ğŸ“– [Configuration de Base](Configuration-de-Base)**
- **ğŸ’¡ [Astuces](Astuces)**
- **ğŸ› [DÃ©pannage](Depannage)**

---

**DerniÃ¨re mise Ã  jour** : 2024  
**Niveau** : DevOps Senior
