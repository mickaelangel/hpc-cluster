# üìö Manuel d'architecture et d'ing√©nierie HPC

**Volume 2 : R√©seaux datacenter, interconnexions et s√©curit√©**

> **Niveau** : DevOps Senior / Architecte HPC ‚Äî **Public** : Master, Doctorat, ing√©nieurs syst√®me

---

## Vue d'ensemble du volume

Un cluster HPC n'est pas qu'une simple collection de serveurs : c'est **l'interconnexion** de ces n≈ìuds qui permet le calcul massivement parall√®le et l'acc√®s fulgurant aux donn√©es. Ce volume couvre le **syst√®me nerveux** du supercalculateur : r√©seaux de management et de stockage (Ethernet, Spine-Leaf, MLAG, Jumbo Frames), interconnexions √† faible latence (InfiniBand, RoCE v2), et fondations de la s√©curit√© (IAM, bastion, durcissement). Un [lab pratique](#-lab-2--d√©ploiement-dun-annuaire-central-et-int√©gration) et un [examen de fin de volume](#-examen-de-fin-de-volume-2) permettent de valider les acquis.

**Pr√©requis :**
- Mod√®le OSI, routage IP, VLANs, LACP (Ch. 5)
- Notions de DMA, files d'attente (Ch. 6)
- Linux PAM, SSH, bases de cryptographie (Ch. 7)

---

## Chapitre 5 : R√©seaux de management et de stockage (Ethernet)

### Objectifs d'apprentissage

- Concevoir une **topologie r√©seau datacenter** moderne (Spine-Leaf)
- Comprendre et configurer les **agr√©gats de liens (MLAG)** et les **trames g√©antes (Jumbo Frames)**
- Diff√©rencier les flux **Nord-Sud** (vers l'ext√©rieur) des flux **Est-Ouest** (inter-n≈ìuds)

---

### 5.1 La fin du mod√®le 3-Tiers : vive le Spine-Leaf

Historiquement, les r√©seaux √©taient organis√©s en **Core, Aggregation et Access** (3-Tiers). En HPC, le trafic est **massivement Est-Ouest** (entre n≈ìuds de calcul ou vers le stockage). Le mod√®le 3-Tiers cr√©e des **goulots d'√©tranglement** (oversubscription).

La topologie **Spine-Leaf** (ou **architecture de Clos**) garantit que n'importe quel n≈ìud A est √† **exactement un saut** (hop) de n'importe quel n≈ìud B, assurant une **latence pr√©dictible**.

**Sch√©ma : Topologie Spine-Leaf**

```
       +-------------+        +-------------+
       |   Spine 1   |        |   Spine 2   |   <-- Couche de routage (L3)
       | (100/400GbE)|        | (100/400GbE)|       (Aucun serveur connect√© ici)
       +--+-------+--+        +--+-------+--+
          |       |              |       |
 +--------+       +-------+------+       +--------+
 |                        |                       |
 |  +------------------+  |  +------------------+ |
 +--+      Leaf 1      +--+--+      Leaf 2      +-+ <-- Couche d'acc√®s (L2/L3)
    |    (Top of Rack) |     |    (Top of Rack) |
    +---+---+---+---+--+     +---+---+---+---+--+
        |   |   |   |            |   |   |   |
      +---+---+---+---+        +---+---+---+---+
      |   N≈ìuds de    |        |  N≈ìuds de     |
      |   Calcul 1-40 |        |  Stockage     |
      +---------------+        +---------------+
```

---

### 5.2 MLAG et redondance

Pour √©viter les boucles, le protocole **Spanning Tree (STP)** bloque des ports et **divise la bande passante**. En production, on utilise le **MLAG** (Multi-Chassis Link Aggregation) ou le **VPC** (Virtual PortChannel) : un serveur est connect√© √† **deux switches Leaf** distincts via **LACP** (802.3ad), offrant **redondance ET** cumul de bande passante (Active-Active).

---

### 5.3 Jumbo Frames (MTU 9000)

Le **MTU** (Maximum Transmission Unit) standard Ethernet est **1500** octets. Pour le trafic de stockage (NFS, Ceph, [Lustre](Glossaire-et-Acronymes#l) via LNet TCP), un **MTU de 9000** octets r√©duit la charge CPU (moins d'interruptions, moins d'en-t√™tes) et augmente le **throughput**.

**Exemple : Configuration persistante du MTU (NetworkManager)**

```bash
# Configuration Jumbo Frame via nmcli
nmcli connection modify "System eth1" ethernet.mtu 9000
nmcli connection up "System eth1"

# V√©rification
ip link show eth1 | grep mtu
```

---

### Pi√®ge : ¬´ Le MTU Mismatch silencieux ¬ª

Si un serveur a un **MTU 9000** et le switch reste √† **1500**, les petits pings (ICMP de base) passent, mais les **gros transferts** (SSH avec grosses cl√©s, SCP, montages NFS) peuvent **bloquer** (hang) sans message d'erreur clair.

> **Sympt√¥me courant** : la connexion SSH s'√©tablit, puis bloque apr√®s l'affichage du mot de passe.

---

### Check-list production (Chapitre 5)

- [ ] V√©rifier le **MTU de bout en bout** avec : `ping -M do -s 8972 <IP_CIBLE>`
- [ ] Activer le **LACP "fast rate"** pour une d√©tection de panne en ~1 s au lieu de 30 s

---

## Chapitre 6 : Interconnexions √† faible latence (InfiniBand & RoCE v2)

### Objectifs d'apprentissage

- Comprendre le **RDMA** (Remote Direct Memory Access) et l'**OS Bypass**
- Configurer et diagnostiquer un r√©seau **InfiniBand** (Subnet Manager, routage)
- Appr√©hender les d√©fis de **RoCE v2** (Lossless Ethernet, PFC, ECN)

---

### 6.1 RDMA et OS Bypass

En r√©seau **TCP/IP** classique, envoyer un message implique des copies utilisateur ‚Üí noyau, calcul des checksums, encapsulation : latence typique **~10 √† 50 ¬µs**.

Le **RDMA** permet √† une carte r√©seau (**HCA** ‚Äî Host Channel Adapter) de **lire/√©crire directement** dans la RAM d'un autre serveur, en **contournant l'OS** (OS Bypass) et le CPU. **Latence typique : ~1 √† 2 ¬µs.**

---

### 6.2 InfiniBand et le Subnet Manager

InfiniBand n'utilise pas d'adresses MAC ni d'ARP. Les cartes ont des **LID** (Local Identifiers) et des **GUID**. Le r√©seau IB n√©cessite un ¬´ cerveau ¬ª : le **Subnet Manager (SM)** (souvent **opensm**). Il d√©couvre la topologie, assigne les LIDs et calcule les **tables de routage** (ex. MinHop, UpDown pour Fat-Tree, Nue pour Dragonfly).

**Commandes de diagnostic (Mellanox/NVIDIA OFED) :**

```bash
# √âtat du lien local (doit √™tre "Active" et "LinkUp")
ibstat

# D√©couvrir tous les n≈ìuds de la fabric IB
ibnetdiscover

# DANGER EN PROD : Reset du port IB (coupe les jobs MPI en cours !)
ibportstate 1 1 reset

# Test bande passante RDMA point √† point
# Serveur :
ib_write_bw
# Client :
ib_write_bw <IP_IB_DU_SERVEUR>
```

---

### 6.3 RoCE v2 (RDMA over Converged Ethernet)

**RoCE v2** encapsule le trafic RDMA dans des trames **UDP/IP**. Avantage : switches Ethernet standards. Inconv√©nient : Ethernet est **lossy** (paquets perdus en congestion). RoCE v2 exige un r√©seau Ethernet **Lossless**, avec r√©glage sur les switches :

| Protocole | R√¥le |
|-----------|------|
| **PFC** (Priority Flow Control) | Met le trafic en pause, classe par classe |
| **ECN** (Explicit Congestion Notification) | Marque les paquets pour demander aux exp√©diteurs de ralentir |

---

### Pi√®ge : ¬´ Le SM Split-Brain ¬ª

Avoir **plusieurs instances OpenSM** avec la **m√™me priorit√©** sur diff√©rents n≈ìuds, sans configuration ma√Ætre/esclave : elles se battent pour assigner les LIDs, causant des **micro-coupures** d√©vastatrices pour les jobs MPI.

---

## Chapitre 7 : Fondations de s√©curit√© HPC

### Objectifs d'apprentissage

- Impl√©menter une **gestion des identit√©s centralis√©e** (IAM)
- Concevoir un acc√®s s√©curis√© via **Bastion**
- **Durcir** le syst√®me d'exploitation et isoler les environnements (multi-tenancy)

---

### 7.1 IAM et FreeIPA

Dans un cluster, l'**UID/GID** d'un utilisateur doit √™tre **strictement identique** sur le n≈ìud de login, le manager, tous les n≈ìuds de calcul et le stockage. Aujourd'hui : **LDAP** ou **FreeIPA** (LDAP + Kerberos + PKI).

**Exemple : Cr√©ation d'un utilisateur HPC via FreeIPA**

```bash
# Authentification admin
kinit admin

# Cr√©ation utilisateur + assignation √† un projet Slurm
ipa user-add jdupont --first="Jean" --last="Dupont" --shell=/bin/bash --uid=1050
ipa group-add-member grp_projet_astro --users=jdupont
```

---

### 7.2 Segmentation et Bastion (Jump Host)

- **Bastion (Login Node)** : seul point d'entr√©e SSH.
- **MFA** : obligatoire sur le Bastion (ex. TOTP via PAM, ou YubiKey).
- **Segmentation** : les n≈ìuds de calcul n'ont souvent **pas d'acc√®s direct √† Internet** ; les t√©l√©chargements passent par un **proxy HTTP** (ex. Squid).

---

### 7.3 Hardening OS (durcissement)

En HPC, **SELinux** est parfois en *Permissive* (incompatibilit√©s avec certains FS parall√®les ou toolchains). D'autres couches compensent :

| Mesure | R√¥le |
|--------|------|
| **Root Squash** | Sur NFS/Lustre : root client est mapp√© √† `nobody` pour ne pas lire les fichiers des autres |
| **Cgroups** | Limite RAM/PIDs par utilisateur, prot√®ge le d√©mon [Slurm](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Guide-SLURM-Complet.md) (slurmd) |

**Snippet : S√©curit√© SSH sur les n≈ìuds de calcul** (`/etc/ssh/sshd_config`)

```
# Les utilisateurs ne se connectent aux n≈ìuds de calcul
# QUE via Slurm (PAM slurm), pas par SSH direct.
PermitRootLogin no
PasswordAuthentication no
AllowTcpForwarding no
X11Forwarding no
```

---

## üß™ Lab 2 : D√©ploiement d'un annuaire central et int√©gration

### √ânonc√©

Vous g√©rez un cluster (1 Master, 2 Computes).

1. Installez **SSSD** (System Security Services Daemon) sur les n≈ìuds de calcul.
2. Configurez-les pour s'authentifier contre un annuaire **LDAP** (ou un serveur **FreeIPA** sur le Master).
3. Configurez **NFSv4** sur le Master pour exporter `/home`.
4. Montez `/home` sur les n≈ìuds de calcul.

### Crit√®res de r√©ussite

- La commande `id jean_hpc` renvoie le **m√™me UID** (ex. 2001) sur Master, Node01 et Node02.
- L'utilisateur `jean_hpc` peut **cr√©er** un fichier dans `/home/jean_hpc` depuis Node01 et le **lire** depuis Node02, avec les permissions correctes.

### Corrig√© (grandes lignes avec Ansible)

```yaml
# 1. Installation SSSD sur les clients
- hosts: computes
  tasks:
    - dnf: name=sssd state=present
    - copy:
        src: sssd.conf   # URI ldap://master, search_base
        dest: /etc/sssd/sssd.conf
        mode: '0600'
    - service: name=sssd state=restarted
    # 2. PAM pour SSSD
    - command: authselect select sssd --force

    # 3. Montage NFS
    - mount:
        path: /home
        src: master:/home
        fstype: nfs4
        opts: rw,soft,intr,rsize=8192,wsize=8192
        state: mounted
```

---

## üìù Examen de fin de volume 2

### QCM (1 point chaque)

**1.** Quelle est l'utilit√© principale de l'algorithme de routage **Fat-Tree** configur√© dans OpenSM ?  
- A) Assigner des adresses IP aux interfaces InfiniBand  
- B) **Calculer des chemins pour √©viter l'engorgement et exploiter les liens redondants de la topologie de Clos**  
- C) Traduire les trames Ethernet en paquets RDMA  

**2.** Pourquoi RoCE v2 est-il qualifi√© de ¬´ Lossless Ethernet ¬ª ?  
- A) Parce qu'il utilise TCP pour garantir la livraison  
- B) **Parce qu'il n√©cessite PFC et ECN au niveau des switches pour √©viter toute chute de paquet due √† la congestion**  
- C) Parce qu'il utilise des c√¢bles optiques qui ne perdent pas de signal  

---

### Question ouverte (Troubleshooting r√©seau)

Vous constatez qu'un job MPI **all-to-all** prend **10 fois plus de temps** que la normale. Les `ping` montrent une latence de 0,1 ms. `ibstat` indique que tous les liens sont ¬´ Active ¬ª √† 200 Gbps. **Quel outil** utilisez-vous pour diagnostiquer le r√©seau IB en profondeur et **que cherchez-vous** ?

**R√©ponse attendue** : Le ping ne teste pas le r√©seau IB (il teste l'IP, souvent sur le r√©seau OOB/Admin). Il faut v√©rifier **congestion**, **compteurs d'erreurs** et **routage**. Outils : **ibdiagnet** pour la sant√© de la fabric, **ibqueryerrors** pour des ¬´ Symbol Errors ¬ª ou ¬´ LinkDowned ¬ª (c√¢ble optique d√©fectueux ou sale ‚Üí retransmissions).

---

### √âtude de cas : ¬´ Le trou de s√©curit√© du Root Squash ¬ª

Un utilisateur malveillant a obtenu les **droits root** sur le n≈ìud de calcul `node045` (vuln√©rabilit√© kernel). Le `/home` est mont√© en **NFSv4**.

1. **Expliquez** comment le param√®tre **root_squash** sur le serveur NFS emp√™che (th√©oriquement) cet utilisateur de lire les cl√©s SSH des autres dans `/home`.
2. **D√©montrez** par quelle m√©thode simple (sans utiliser le r√©seau) cet utilisateur root local peut **contourner** root_squash et usurper l'identit√© d'un autre utilisateur pour lire ses fichiers.

**R√©ponses attendues :**

1. **root_squash** mappe l'UID 0 (root) du client vers l'UID `nobody` (ex. 65534) sur le serveur. L'acc√®s aux fichiers de `jdupont` (UID 1050) est donc refus√© par le serveur NFS.
2. **Contournement** : En √©tant root localement, il peut ex√©cuter `su - jdupont`. Les requ√™tes NFS partent alors avec l'UID 1050 et sont accept√©es par le serveur. **Mitigation** : chiffrement des donn√©es, ou **NFSv4 + Kerberos** (krb5p) o√π le ticket Kerberos est requis.

---

## Solutions des QCM

- **Q1** : **B** ‚Äî Fat-Tree permet d'exploiter la redondance de la topologie Clos pour un routage optimal.
- **Q2** : **B** ‚Äî RoCE v2 repose sur PFC/ECN pour un Ethernet sans perte.

---

## üìã Relecture qualit√© du volume 2

- [x] Couverture : Ethernet datacenter (Spine-Leaf, MTU), InfiniBand/RoCE (RDMA, OpenSM), S√©curit√© (IAM, Bastion, Root Squash)
- [x] Rigueur technique : MTU mismatch, r√¥le du Subnet Manager, faille d'usurpation locale
- [x] Format : Markdown, sch√©mas Spine-Leaf
- [x] P√©dagogie : TP Ansible LDAP/NFS, cas de troubleshooting (ibdiagnet, faille root)

---

## Liens utiles

- **[Sommaire complet du Manuel HPC](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Manuel-HPC-Sommaire-Complet.md)** : plan des 8 volumes, chapitres, labs
- **[Manuel Architecture HPC ‚Äî Volume 1](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Manuel-Architecture-HPC-Volume1.md)** : fondations, provisioning, Ansible
- **[Cours HPC Complet](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Cours-HPC-Complet.md)** : concepts, parall√©lisme, stockage
- **[Guide SLURM Complet](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Guide-SLURM-Complet.md)** : ordonnancement et jobs
- **[Glossaire et Acronymes](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Glossaire-et-Acronymes.md)** : RDMA, RoCE, LACP, MTU, FreeIPA, etc.
- **[Home](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Home.md)** : page d'accueil du wiki

---

**Volume 2** ‚Äî R√©seaux datacenter, interconnexions et s√©curit√©  
**Derni√®re mise √† jour** : 2024
