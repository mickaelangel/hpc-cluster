# üìö Manuel d'architecture et d'ing√©nierie HPC

**Volume 6 : Ing√©nierie des performances et benchmarking**

> **Niveau** : DevOps Senior / Architecte HPC ‚Äî **Public** : Master, Doctorat, ing√©nieurs syst√®me

---

## Vue d'ensemble du volume

L'infrastructure est stable, l'environnement logiciel est d√©ploy√©. Ce volume couvre la phase la plus **scientifique** de l'ing√©nierie HPC : traquer la latence et saturer la bande passante mat√©rielle. On y traite l'**architecture m√©moire** (NUMA, pinning, Hugepages), la **m√©thodologie de profiling** (mod√®le Roofline, `perf`), puis le **benchmarking en production** (HPL, HPCG, IOR, mdtest). Les [Labs 8 & 9](#-lab-8--9--numa-stream-et-profiling-perf) et l'[examen de fin de volume](#-examen-de-fin-de-volume-6) permettent de valider les acquis.

**Pr√©requis :**
- Caches L1/L2/L3, RAM, TLB, gestion des pages (Ch. 19)
- Compilation C/Fortran avec symboles de d√©bogage `-g` (Ch. 20)
- MPI, [Slurm](Guide-SLURM-Complet), [Lustre](Glossaire-et-Acronymes#l) (Ch. 21)

---

## Chapitre 19 : Architecture m√©moire et optimisation (NUMA & Hugepages)

### Objectifs d'apprentissage

- Comprendre la **topologie NUMA** et l'impact des interconnexions inter-sockets (UPI, Infinity Fabric)
- Ma√Ætriser le **pinning** (affinit√©) des processus et de la m√©moire
- Optimiser via les **Hugepages**

---

### 19.1 La r√©alit√© de l'architecture NUMA

La plupart des serveurs HPC sont **bi-socket** ou √† **chiplets** (ex. AMD EPYC). Chaque processeur/chiplet a son **contr√¥leur m√©moire** et sa **RAM locale** ‚Üí un **n≈ìud NUMA** (Non-Uniform Memory Access).

Si le **CPU 0** lit une donn√©e dans la **RAM du CPU 1**, la requ√™te passe par le bus d'interconnexion (Intel **UPI** ou AMD **Infinity Fabric**).

| Cons√©quence | Impact |
|-------------|--------|
| **Latence** | Augmentation d'environ **30 √† 50 %** |
| **Bande passante** | Divis√©e : le bus inter-socket est souvent plus lent que la bande passante m√©moire locale agr√©g√©e |

---

### 19.2 Pinning et outil numactl

Pour des **performances maximales**, un processus MPI (ou thread OpenMP) doit √™tre **√©pingl√©** (pinned) sur un c≈ìur donn√©, et sa **m√©moire** allou√©e sur le **n≈ìud NUMA local**. Linux utilise la politique **¬´ First Touch ¬ª** : la page physique est allou√©e sur le n≈ìud NUMA du **premier thread** qui √©crit dedans.

**Exemple avec numactl :**

```bash
# Topologie NUMA et distances entre n≈ìuds
numactl --hardware

# Forcer le code sur les c≈ìurs du n≈ìud NUMA 0 et la m√©moire sur la RAM du n≈ìud 0
numactl --cpunodebind=0 --membind=0 ./mon_code_intensif
```

---

### 19.3 L'optimisation par les Hugepages

Le **TLB** (Translation Lookaside Buffer) traduit les adresses virtuelles en physiques. Par d√©faut, Linux utilise des **pages de 4 Ko**. Une application avec **64 Go** de RAM ‚Üí **16 millions de pages** ‚Üí le TLB (quelques milliers d'entr√©es) est **satur√©** ‚Üí **TLB misses** co√ªteux.

Les **Hugepages** (2 Mo ou 1 Go) r√©duisent le **nombre de pages**, limitant fortement les TLB misses pour les grands tableaux.

---

### Pi√®ge : ¬´ Le First Touch fatal ¬ª

En **OpenMP**, le **thread ma√Ætre (0)** initialise souvent un grand tableau en **s√©quentiel** avant le calcul parall√®le. **Cons√©quence** : toute la m√©moire est allou√©e sur le **n≈ìud NUMA du thread 0**. Pendant le calcul, les autres threads **saturent le bus inter-socket**. **Solution** : **parall√©liser l'initialisation** aussi.

---

### Check-list production (Chapitre 19)

- [ ] Surveiller **numad** ou **numastat**
- [ ] Pour clusters MPI purs : configurer Slurm avec **TaskPlugin=task/affinity,task/cgroup**

---

## Chapitre 20 : M√©thodologie de profiling et mod√®le Roofline

### Objectifs d'apprentissage

- Diagnostiquer les **goulots d'√©tranglement** (CPU-bound vs Memory-bound)
- Utiliser les **compteurs de performance** via **perf**
- Interpr√©ter le **mod√®le Roofline**

---

### 20.1 Le mod√®le Roofline

Le **mod√®le Roofline** relie les **performances flottantes** (GFLOPS), la **bande passante m√©moire** (Go/s) et l'**intensit√© arithm√©tique** d'un algorithme.

**Intensit√© arithm√©tique (I)** = nombre d'**op√©rations flottantes** par **octet** transf√©r√© depuis la RAM (FLOP/byte).

**Performance th√©orique maximale P :**

```
P = min(P_peak, I √ó b_peak)
```

*(P_peak = puissance cr√™te du CPU, b_peak = bande passante m√©moire cr√™te.)*

| Zone | Signification | Optimisation typique |
|------|----------------|----------------------|
| **Memory-bound** (pente) | Intensit√© faible (ex. addition de vecteurs). Le code attend la RAM. | Cache blocking, tuning NUMA. |
| **Compute-bound** (plafond plat) | Intensit√© √©lev√©e (ex. multiplication matricielle dense). | Vectorisation (AVX-512), multithreading. |

---

### 20.2 Profiling l√©ger avec perf

**perf** lit les registres **PMU** (Performance Monitoring Unit) du processeur avec un **overhead** quasi nul.

**Commandes de base :**

```bash
# Statistiques globales (IPC, cache misses, branch misses)
perf stat ./mon_code_calcul

# Enregistrer l'arbre d'appels (call graph)
perf record --call-graph fp ./mon_code_calcul

# Analyser le rapport
perf report
```

---

## Chapitre 21 : Benchmarking en production et acceptation

### Objectifs d'apprentissage

- D√©ployer et tuner le benchmark **HPL** (Top500)
- Ex√©cuter la suite **OSU Micro-Benchmarks** pour valider l'interconnexion
- Valider les performances **I/O** avec **IOR** et **mdtest**

---

### 21.1 HPL (High Performance Linpack)

**HPL** r√©sout un syst√®me d'√©quations lin√©aires denses **Ax = b**. Tr√®s **compute-bound**, bas√© sur **BLAS** (DGEMM). C'est le **m√®tre √©talon** du classement **Top500**.

**M√©thodologie de tuning :**
- La taille du probl√®me **N** doit remplir environ **80‚Äì90 %** de la RAM totale du cluster.
- La grille MPI **(P√óQ)** doit √™tre la plus **carr√©e** possible pour limiter les communications de bordure.

---

### 21.2 HPCG (High Performance Conjugate Gradients)

**HPL** ne refl√®te plus les charges r√©elles (souvent **memory-bound**). **HPCG** mesure les acc√®s m√©moire irr√©guliers, la bande passante et les collectives. Un syst√®me √† **80 %** du pic sur HPL peut n'atteindre que **2‚Äì3 %** sur HPCG.

---

### 21.3 Benchmarking I/O : IOR et mdtest

| Outil | Mesure | Usage |
|-------|--------|--------|
| **IOR** | Bande passante brute (Go/s) | Gros fichiers (MPI-IO ou fichier par processus). |
| **mdtest** | M√©tadonn√©es (IOPS) | Cr√©ation/lecture/suppression de millions de fichiers ‚Üí stress du **MDS** Lustre. |

**Exemple IOR (File-Per-Process, write puis read) :**

```bash
# -F : fichier par processus, -w : write, -r : read, -b : block size, -t : transfer size
srun mpirun -np 128 ior -F -w -r -b 10G -t 1M -o /scratch/test_ior/data
```

---

### DANGER en prod : ¬´ Le benchmark destructeur ¬ª

Lancer **mdtest** avec des **millions de fichiers** √† la racine de `/scratch` en production peut **saturer le MDT** (100 %) et bloquer tous les jobs. **Isoler** les benchmarks I/O, id√©alement en **fen√™tre de maintenance**.

---

## üß™ Lab 8 & 9 : NUMA, STREAM et profiling perf

### √ânonc√©

**Lab 8 (NUMA/STREAM)** : Compilez le benchmark **STREAM** (bande passante m√©moire). Ex√©cutez-le **sans contrainte**, puis en le **for√ßant sur un seul n≈ìud NUMA** via `numactl`. Observez la diff√©rence en **Mo/s**.

**Lab 9 (Perf)** : √âcrivez un petit programme C qui it√®re sur un tableau 2D **10000√ó10000** : une version **par lignes** (row-major, cache friendly) et une **par colonnes** (column-major, cache hostile). Utilisez **perf stat -e cache-misses** sur les deux et analysez.

### Crit√®res de r√©ussite

- STREAM avec **numactl** montre une bande passante **locale maximale et stable** par rapport √† une ex√©cution inter-socket.
- **perf stat** montre un nombre de **cache-misses** nettement plus √©lev√© (souvent √ó10 √† √ó20) sur la version **par colonnes** en C.

### Corrig√© (Lab 9 ‚Äî pi√®ge column-major en C)

```c
// Le CPU charge des lignes de cache (64 octets). Parcourir par colonne
// d√©truit l'efficacit√© du Hardware Prefetcher.
int matrix[10000][10000];

// CACHE FRIENDLY (C)
for (int i = 0; i < 10000; i++)
    for (int j = 0; j < 10000; j++)
        matrix[i][j] = 1;

// CACHE HOSTILE (√† proscrire en C ; en Fortran c'est l'inverse)
for (int j = 0; j < 10000; j++)
    for (int i = 0; i < 10000; i++)
        matrix[i][j] = 1;
```

**Ex√©cution :** `perf stat -e L1-dcache-load-misses ./bad_code`

---

## üìù Examen de fin de volume 6

### QCM (1 point chaque)

**1.** Pourquoi recommande-t-on souvent d'activer les **Hugepages** pour un code MPI ?  
- A) Pour augmenter la fr√©quence du CPU  
- B) **Pour r√©duire les TLB misses lors de l'acc√®s √† de grandes zones m√©moire contigu√´s**  
- C) Pour forcer l'utilisation du GPU  

**2.** Dans le mod√®le Roofline, que repr√©sente la zone **¬´ plafond ¬ª** horizontale du graphique ?  
- A) La limite de bande passante du r√©seau InfiniBand  
- B) La limite caus√©e par les acc√®s m√©moire (Memory-bound)  
- C) **La limite de performance de calcul maximale du CPU (Compute-bound)**  

**3.** Quel benchmark est utilis√© historiquement pour le **classement Top500** (r√©solution de syst√®mes lin√©aires) ?  
- A) IOR  
- B) **HPL**  
- C) HPCG  

---

### Question ouverte (Analyse de m√©triques)

Vous analysez un job avec **perf stat**. **IPC = 0,2** (Instructions Per Cycle) et **Branch miss rate = 15 %**. Le processeur peut ex√©cuter jusqu'√† **4 instructions par cycle**.

**Expliquez** ce que signifie un IPC de 0,2 et **comment** le fort taux de branch misses contribue √† ce r√©sultat au niveau du **pipeline**.

**R√©ponse attendue** : Un IPC de **0,2** signifie que le CPU **stalle** ~80 % du temps (attend la RAM ou vide son pipeline). Le **branch predictor** devine les branches (if/else). √Ä **15 %** de miss, le processeur **annule** les instructions sp√©culatives, jette les calculs et recharge le bon chemin ‚Üí **p√©nalit√©** importante en cycles perdus.

---

### √âtude de cas : ¬´ La recette magique HPL qui ne marche pas ¬ª

Vous validez l'**acceptation** d'un cluster **100 n≈ìuds** (AMD EPYC, 128 c≈ìurs, **512 Go RAM** par n≈ìud). L'int√©grateur fournit **HPL.dat** avec la taille **N** r√©gl√©e pour **50 Go** de RAM par n≈ìud. Le r√©sultat est d'√† peine **30 %** des FLOPS th√©oriques.

1. **Pourquoi** sous-dimensionner N d√©t√©riore le score HPL ?  
2. **Donnez** un calcul rapide pour un **N id√©al** (‚âà 80 % des 512 Go).  
3. **Pourquoi** la configuration **topology.conf** de Slurm est-elle critique pour ce test ?

**R√©ponses attendues :**

1. HPL fait des calculs en **O(N¬≥)** et des communications en **O(N¬≤)**. Plus **N** est grand, plus le ratio **Calculs/Communications** augmente (intensit√© arithm√©tique) ‚Üí meilleure saturation du CPU et masquage de la latence r√©seau.

2. **80 % √ó 512 Go ‚âà 410 Go** par n≈ìud. Matrice **A** : N¬≤√ó8 octets (double). Total cluster : 410 Go √ó 100 = 41 000 Go. N¬≤ √ó 8 = 41√ó10¬π¬≤ octets ‚Üí **N ‚âà ‚àö(41√ó10¬π¬≤/8) ‚âà 2,26√ó10‚Å∂**.

3. HPL envoie des donn√©es en continu. Si les rangs MPI ne sont pas **cartographi√©s** (topology.conf) pour minimiser les **sauts** sur les switches, la **congestion r√©seau** ralentit tout le syst√®me.

---

## Solutions des QCM

- **Q1** : **B** ‚Äî Hugepages r√©duisent les TLB misses.  
- **Q2** : **C** ‚Äî Plafond = Compute-bound (pic CPU).  
- **Q3** : **B** ‚Äî HPL (Top500).

---

## üìã Relecture qualit√© du volume 6

- [x] Couverture : NUMA (numactl), Hugepages/TLB, Roofline, perf, HPL, HPCG, IOR/mdtest
- [x] Rigueur technique : formules Roofline et HPL, distinction Memory-bound / Compute-bound
- [x] Format : Markdown, structure claire
- [x] P√©dagogie : Labs (STREAM, pi√®ge column-major), √©tude de cas Top500

---

## Liens utiles

- **[Sommaire complet du Manuel HPC](Manuel-HPC-Sommaire-Complet)** : plan des 8 volumes, chapitres, labs
- **[Manuel Architecture HPC ‚Äî Vol. 1 √† 5](Manuel-Architecture-HPC-Volume1)** : fondations, r√©seaux, stockage, Slurm, toolchains
- **[Guide SLURM Complet](Guide-SLURM-Complet)** : partitions, topology, task/affinity
- **[Glossaire et Acronymes](Glossaire-et-Acronymes)** : NUMA, TLB, HPL, HPCG, etc.
- **[Home](Home)** : page d'accueil du wiki

---

**Volume 6** ‚Äî Ing√©nierie des performances et benchmarking  
**Derni√®re mise √† jour** : 2024
