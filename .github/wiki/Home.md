# Wiki - Cluster HPC Enterprise

> **Documentation collaborative professionnelle - Niveau DevOps Senior**

---

## ğŸ¯ Bienvenue

Ce Wiki contient la documentation collaborative pour le **Cluster HPC Enterprise**, une infrastructure HPC complÃ¨te, 100% open-source, prÃªte pour la production.

**Niveau** : DevOps Senior / Architecte  
**Public** : Administrateurs systÃ¨me, DevOps, Architectes HPC, **Ã©tudiants Master Data Science / Doctorat**

---

## ğŸ“š Formation HPC â€” Cours complet & Glossaire

### ğŸ“ Parcours formation (niveau Master / Doctorat)

- **[Cours HPC Complet](Cours-HPC-Complet)** â€” Formation exhaustive : concepts HPC, architecture des clusters, parallÃ©lisme (MPI, OpenMP), stockage (Lustre, BeeGFS, Ceph), GPU, conteneurs, sÃ©curitÃ© et monitoring. IdÃ©al pour tout savoir sur les technologies et le fonctionnement des clusters.
- **[Sommaire complet du Manuel HPC](Manuel-HPC-Sommaire-Complet)** â€” Plan des 9 volumes (~620â€“720 p.), tous les chapitres, labs, Ã©tudes de cas, annexes.
- **[Manuel Architecture & IngÃ©nierie HPC â€” Vol. 1](Manuel-Architecture-HPC-Volume1)** â€” Fondations, co-design, 3 architectures types (CPU-only, GPU, Data-Intensive), OOB (BMC, IPMI, Redfish), provisioning bare-metal (PXE, Warewulf v4), Ansible, lab et examen.
- **[Manuel Architecture & IngÃ©nierie HPC â€” Vol. 2](Manuel-Architecture-HPC-Volume2)** â€” RÃ©seaux datacenter (Spine-Leaf, MLAG, Jumbo Frames), InfiniBand & RoCE v2 (RDMA, OpenSM), sÃ©curitÃ© (IAM/FreeIPA, Bastion, hardening), lab SSSD/NFS et examen.
- **[Manuel Architecture & IngÃ©nierie HPC â€” Vol. 3](Manuel-Architecture-HPC-Volume3)** â€” Stockage parallÃ¨le : POSIX, tiering (scratch/project/archive), Lustre (MGS, MDS/MDT, OSS/OST, LNet, DNE), striping, BeeGFS/GPFS/CephFS, lab mini-Lustre et examen.
- **[Manuel Architecture & IngÃ©nierie HPC â€” Vol. 4](Manuel-Architecture-HPC-Volume4)** â€” Ordonnancement : Backfill, Fairshare, architecture Slurm (slurmctld, slurmd, slurmdbd), MUNGE, cgroups, GRES GPU, troubleshooting, labs Slurm/Fairshare/cgroups et examen.
- **[Manuel Architecture & IngÃ©nierie HPC â€” Vol. 5](Manuel-Architecture-HPC-Volume5)** â€” Toolchains : Lmod, Spack, Apptainer ; MPI (Eager/Rendezvous, binding NUMA, UCX) ; GPU et IA (NCCL, GPUDirect RDMA, Slurm) ; labs Spack/OMB et Apptainer/PyTorch, examen.
- **[Manuel Architecture & IngÃ©nierie HPC â€” Vol. 6](Manuel-Architecture-HPC-Volume6)** â€” Performances : NUMA, numactl, Hugepages/TLB, modÃ¨le Roofline, perf, HPL/HPCG, IOR/mdtest, labs STREAM et profiling cache, examen.
- **[Manuel Architecture & IngÃ©nierie HPC â€” Vol. 7](Manuel-Architecture-HPC-Volume7)** â€” ObservabilitÃ© & SRE : Pull vs Push, exporters (Slurm/Lustre/GPU), capacity planning, SLO/SLA, runbooks, post-mortems blameless, MTTR, lab Prometheus+Slurm, examen.
- **[Manuel Architecture & IngÃ©nierie HPC â€” Vol. 8](Manuel-Architecture-HPC-Volume8)** â€” Fil rouge : 5 phases (design â†’ bare-metal â†’ stockage â†’ Slurm â†’ observabilitÃ©), Go-Live, Exascale/DLC, convergence IA-HPC, Cloud Bursting, Data Gravity, lab architecture WRF, examen final.
- **[Manuel Architecture & IngÃ©nierie HPC â€” Vol. 9](Manuel-Architecture-HPC-Volume9)** â€” Data Science & ML sur cluster : data locality, I/O/formats, PyTorch DDP multi-nÅ“uds (NCCL), job arrays/Submitit/Optuna, reproductibilitÃ© et MLOps on-prem, labs 12 & 13.
- **[Guide SLURM Complet](Guide-SLURM-Complet)** â€” Scheduler en dÃ©tail : partitions, QoS, soumission (sbatch, srun), file dâ€™attente, bonnes pratiques, intÃ©gration MPI/GPU.
- **[Glossaire et Acronymes](Glossaire-et-Acronymes)** â€” Dictionnaire des acronymes (HPC, MPI, SLURM, PBS, Lustre, etc.) et dÃ©finitions des termes (cluster, allocation, fair-share, walltime, etc.). RÃ©fÃ©rence pour Ã©tudiants et professionnels.
- **[Dictionnaire encyclopÃ©dique HPC](Dictionnaire-Encyclopedique-HPC)** â€” EntrÃ©es encyclopÃ©diques (Backfill, GPUDirect RDMA, NUMA, Fairshare, RDMA, Striping, DLM, Hugepages, OOM-Killer) : dÃ©finition rigoureuse, internals, bonnes pratiques, tuning, troubleshooting, rÃ©fÃ©rences. Niveau Doctorat/Architecte.
- **[Annexes SRE & Cheatsheets HPC](hpc_annexes)** â€” Annexe A : Slurm (scontrol, sacct, sdiag) ; B : Lustre (lfs, lctl, OST) ; C : perf, numactl, htop, iostat ; D : Post-Mortem Blameless (RCA).

---

## ğŸ“‹ Navigation Rapide

### ğŸš€ Pour DÃ©marrer

- **[Installation Rapide](Installation-Rapide)** : Installation en 5 minutes
- **[Configuration de Base](Configuration-de-Base)** : Configuration minimale fonctionnelle
- **[Premiers Pas](Premiers-Pas)** : Guide pour commencer

### ğŸ“š Documentation par RÃ´le

#### ğŸ‘¨â€ğŸ’¼ Administrateur SystÃ¨me
- **[Guide Administrateur](Guide-Administrateur)** : Administration complÃ¨te
- **[Maintenance](Maintenance)** : Maintenance et opÃ©rations
- **[SÃ©curitÃ©](Securite)** : SÃ©curitÃ© avancÃ©e

#### ğŸ”§ DevOps
- **[CI/CD](CI-CD)** : Pipelines et automatisation
- **[Infrastructure as Code](Infrastructure-as-Code)** : Terraform, Ansible
- **[Monitoring](Monitoring)** : ObservabilitÃ© complÃ¨te

#### ğŸ‘¥ Utilisateur
- **[Guide Utilisateur](Guide-Utilisateur)** : Utilisation du cluster
- **[Lancement de Jobs](Lancement-de-Jobs)** : Comment lancer des jobs
- **[Applications Scientifiques](Applications-Scientifiques)** : Utilisation des applications

### ğŸ” RÃ©fÃ©rence

- **[Glossaire et Acronymes](Glossaire-et-Acronymes)** : Dictionnaire HPC et acronymes (SLURM, MPI, Lustre, etc.)
- **[FAQ](FAQ)** : Questions frÃ©quentes
- **[DÃ©pannage](Depannage)** : Solutions aux problÃ¨mes courants
- **[Astuces](Astuces)** : Trucs et optimisations
- **[Commandes Utiles](Commandes-Utiles)** : RÃ©fÃ©rence rapide

### ğŸ“Š Cas d'Usage

- **[Cas d'Usage](Cas-d-Usage)** : Exemples d'utilisation
- **[Configurations RecommandÃ©es](Configurations-Recommandees)** : Configurations par scÃ©nario
- **[Retours d'ExpÃ©rience](Retours-d-Experience)** : Partage d'expÃ©riences

---

## ğŸ—ï¸ Architecture

### Vue d'Ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Cluster HPC Enterprise                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚  Frontal-01  â”‚  â”‚  Frontal-02  â”‚  (HA Master/Backup)     â”‚
â”‚  â”‚  172.20.0.101â”‚  â”‚  172.20.0.102â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚Comp-01â”‚ â”‚Comp-02â”‚ â”‚Comp-03â”‚ â”‚Comp-04â”‚ â”‚Comp-05â”‚ â”‚Comp-06â”‚ â”‚
â”‚  â”‚.201  â”‚ â”‚.202  â”‚ â”‚.203  â”‚ â”‚.204  â”‚ â”‚.205  â”‚ â”‚.206  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Stack Monitoring & Observability              â”‚   â”‚
â”‚  â”‚  Prometheus â”‚ Grafana â”‚ InfluxDB â”‚ Loki â”‚ Promtail   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Services Enterprise                            â”‚   â”‚
â”‚  â”‚  Slurm â”‚ FreeIPA â”‚ JupyterHub â”‚ GitLab â”‚ Kubernetes   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Composants Principaux

- **2 NÅ“uds Frontaux** : Haute disponibilitÃ©, authentification, accÃ¨s utilisateurs
- **6 NÅ“uds de Calcul** : ExÃ©cution des jobs HPC
- **Monitoring** : Prometheus, Grafana, InfluxDB, Loki
- **Scheduler** : Slurm avec partitions, QoS, prioritÃ©s
- **Stockage** : BeeGFS, Lustre, GlusterFS, Ceph
- **Authentification** : LDAP/Kerberos ou FreeIPA
- **Applications** : 27+ applications scientifiques

---

## ğŸš€ Quick Start

### Installation en 3 Commandes

```bash
git clone https://github.com/mickaelangel/hpc-cluster.git
cd hpc-cluster
sudo ./install-all.sh
```

### AccÃ¨s aux Services

- **Grafana** : http://localhost:3000 (admin/admin123 - âš ï¸ Ã€ changer)
- **Prometheus** : http://localhost:9090
- **InfluxDB** : http://localhost:8086
- **JupyterHub** : http://localhost:8000

---

## ğŸ“š Documentation ComplÃ¨te

### Documentation Principale

- **ğŸ“– [Index Documentation](https://github.com/mickaelangel/hpc-cluster/blob/main/docs/INDEX_DOCUMENTATION.md)** : 80+ guides
- **ğŸ“š [Documentation ComplÃ¨te](https://github.com/mickaelangel/hpc-cluster/tree/main/docs)** : Tous les guides

### Guides par ThÃ¨me

- **Installation** : [GUIDE_INSTALLATION_COMPLETE.md](https://github.com/mickaelangel/hpc-cluster/blob/main/docs/GUIDE_INSTALLATION_COMPLETE.md)
- **SÃ©curitÃ©** : [GUIDE_SECURITE_AVANCEE.md](https://github.com/mickaelangel/hpc-cluster/blob/main/docs/GUIDE_SECURITE_AVANCEE.md)
- **Monitoring** : [GUIDE_MONITORING_COMPLET.md](https://github.com/mickaelangel/hpc-cluster/blob/main/docs/GUIDE_MONITORING_COMPLET.md)
- **CI/CD** : [GUIDE_CI_CD_COMPLET.md](https://github.com/mickaelangel/hpc-cluster/blob/main/docs/GUIDE_CI_CD_COMPLET.md)

---

## ğŸ”— Liens Utiles

### Support

- **ğŸ’¬ [Discussions GitHub](https://github.com/mickaelangel/hpc-cluster/discussions)** : Poser des questions
- **ğŸ› [Signaler un Bug](https://github.com/mickaelangel/hpc-cluster/issues/new?template=bug_report.md)** : CrÃ©er une issue
- **âœ¨ [Demander une FonctionnalitÃ©](https://github.com/mickaelangel/hpc-cluster/issues/new?template=feature_request.md)** : Proposer une fonctionnalitÃ©

### Contribution

- **ğŸ“ [Guide Contribution](https://github.com/mickaelangel/hpc-cluster/blob/main/CONTRIBUTING.md)** : Comment contribuer
- **ğŸ”’ [Politique de SÃ©curitÃ©](https://github.com/mickaelangel/hpc-cluster/blob/main/SECURITY.md)** : Signaler une vulnÃ©rabilitÃ©

---

## ğŸ“Š Statistiques

- **ğŸ“¦ 579 fichiers** de code et configuration
- **ğŸ“š 93 guides** de documentation
- **ğŸ”§ 258 scripts** d'installation/configuration
- **ğŸ“Š 54 dashboards** Grafana
- **ğŸ’» 89,452+ lignes** de code

---

## ğŸ¯ Objectifs du Wiki

Ce Wiki est maintenu par la communautÃ© pour :
- âœ… **Formation** : cours HPC complet (Master/Doctorat), guide SLURM, glossaire et acronymes
- âœ… Partager des guides rapides
- âœ… Documenter des cas d'usage spÃ©cifiques
- âœ… Maintenir une FAQ Ã  jour
- âœ… Partager des astuces et optimisations
- âœ… Faciliter l'onboarding (Ã©tudiants, chercheurs, DevOps)

---

## ğŸ“ Comment Contribuer au Wiki

1. Cliquer sur **"Edit"** en haut de la page
2. Modifier le contenu en Markdown
3. Sauvegarder avec un message descriptif
4. Respecter le formatage existant

**Guide complet** : [docs/GUIDE_COMMUNAUTE.md](https://github.com/mickaelangel/hpc-cluster/blob/main/docs/GUIDE_COMMUNAUTE.md)

---

**DerniÃ¨re mise Ã  jour** : 2024  
**Maintenu par** : La communautÃ© HPC
