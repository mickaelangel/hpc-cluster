# üìö Manuel d'architecture et d'ing√©nierie HPC

**Volume 1 : Fondations, architecture de base et provisioning DevOps**

> **Niveau** : DevOps Senior / Architecte HPC ‚Äî **Public** : Master, Doctorat, ing√©nieurs syst√®me

---

## Vue d'ensemble du volume

Ce manuel couvre les **fondations** d'un cluster HPC en production : co-design, architectures types, gestion hors-bande (OOB), provisioning bare-metal (PXE, Warewulf) et configuration management (Ansible). Un [lab pratique](#-lab-1--provisioning-dun-mini-cluster-from-scratch) et un [examen de fin de volume](#-examen-de-fin-de-volume-1) permettent de valider les acquis.

**Pr√©requis g√©n√©raux :**
- Notions d'architecture des ordinateurs (CPU, RAM, I/O)
- Bases du calcul parall√®le
- Connaissances r√©seaux (IP, MAC, VLAN) pour les chapitres 2 et 3

---

## Chapitre 1 : Introduction au HPC moderne et co-design

### Objectifs d'apprentissage

- Comprendre la philosophie du **co-design** (mat√©riel / logiciel)
- Identifier les **3 grandes architectures types** de clusters HPC
- Cartographier les **composants logiques** d'un supercalculateur

---

### 1.1 Le paradigme du co-design

En HPC, le mat√©riel et le logiciel ne sont **pas choisis de mani√®re isol√©e**. Le **co-design** est l'art d'architecturer un syst√®me en fonction des caract√©ristiques exactes de la **charge de travail** (workload).

> On ne construit pas un cluster pour ¬´ faire du calcul ¬ª ; on le construit pour **¬´ r√©soudre des √©quations de Navier‚ÄìStokes massivement parall√®les ¬ª** ou **¬´ entra√Æner des LLM de 70B param√®tres ¬ª**.

Chaque famille de workload impose des choix d‚Äôarchitecture (CPU vs GPU, latence vs d√©bit r√©seau, stockage parall√®le vs local).

---

### 1.2 Les trois architectures types

| Architecture | Cible typique | Design principal |
|--------------|----------------|-------------------|
| **CPU-only** (capacit√© & scalabilit√©) | CFD, chimie quantique, Monte-Carlo | N≈ìuds denses en c≈ìurs (AMD EPYC, Intel Xeon). Bande passante m√©moire (DDR5, HBM) et **r√©seau √† tr√®s faible latence** (InfiniBand NDR). Fort usage [MPI](Glossaire-et-Acronymes#m). |
| **GPU / Accelerated** (IA & calcul vectoriel) | Deep Learning (training), dynamique mol√©culaire (GROMACS, NAMD) | N≈ìuds ¬´ fat ¬ª (ex. 8√ó NVIDIA H100 ou AMD MI300X). **Interconnexion interne (NVLink)** cruciale. R√©seau externe √† d√©bit massif (RoCE v2 ou InfiniBand) avec **GPUDirect RDMA** pour bypasser le CPU. |
| **Data-Intensive** (I/O heavy) | G√©nomique, bio-informatique, physique des particules | N≈ìuds avec **beaucoup de RAM locale**. R√©seau dimensionn√© pour le **throughput** vers le stockage. **Stockage parall√®le** ([Lustre](Glossaire-et-Acronymes#l), GPFS) optimis√© pour millions de petits fichiers (IOPS) ou flux s√©quentiels massifs. |

---

### 1.3 Architecture macroscopique d‚Äôun cluster

Les composants logiques s‚Äôarticulent ainsi en production :

```
+-------------------+       +-------------------+       +-------------------+
|  Utilisateurs     |       |   R√©seau d'Admin  |       | R√©seau Haute Perf |
|  (SSH / Portail)  |       |   (1GbE / 10GbE)  |       | (InfiniBand/RoCE) |
+--------+----------+       +---------+---------+       +---------+---------+
         |                            |                           |
         v                            |                           |
+-------------------+                 |                           |
|   Login Nodes     +-----------------+                           |
| (Bastion/Compil)  |                 |                           |
+-------------------+                 |                           |
                                      |                           |
+-------------------+                 |                           |
| Management Nodes  +-----------------+                           |
| (Slurmctld, DNS,  |                 |                           |
|  LDAP, Observab.) |                 |                           |
+-------------------+                 |                           |
                                      |                           |
+-------------------+                 |                           |
| Compute Nodes     |                 |                           |
| (CPU/GPU/Fat RAM) +-----------------+---------------------------+
+-------------------+                 |                           |
                                      |                           |
+-------------------+                 |                           |
| Storage Nodes     |                 |                           |
| (Lustre MGS/OSS)  +-----------------+---------------------------+
+-------------------+
```

---

### Pi√®ges et anti-patterns

| Pi√®ge | Description |
|-------|-------------|
| **Login node fourre-tout** | Laisser les utilisateurs lancer de gros calculs ou compilations lourdes sur le n≈ìud de connexion. **Sympt√¥me** : le cluster semble plant√© car le point d‚Äôentr√©e est satur√©. |
| **N√©gliger le r√©seau d‚Äôadministration (OOB)** | Penser que le r√©seau 1GbE / IPMI n‚Äôa pas besoin d‚Äô√™tre redondant. En cas de freeze du cluster, c‚Äôest souvent la **seule porte de secours**. |

---

### Check-list production (Chapitre 1)

- [ ] S√©paration physique ou **VLAN strict** entre : r√©seau d‚Äôadmin, r√©seau IPMI, r√©seau haute performance (Data/MPI)
- [ ] **Quotas stricts** sur les n≈ìuds de login (ex. `/home` limit√©)
- [ ] Politique claire : pas de calcul lourd sur les login nodes (utilisation de [Slurm](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Guide-SLURM-Complet.md) pour toute ex√©cution)

---

## Chapitre 2 : Bases mat√©rielles et gestion Out-of-Band (OOB)

### Objectifs d'apprentissage

- Ma√Ætriser l‚Äô**administration bare-metal** sans d√©pendre de l‚ÄôOS
- Utiliser **IPMI** et l‚Äô**API Redfish** pour la gestion de flotte

---

### 2.1 Le BMC (Baseboard Management Controller)

Le **BMC** est un **micro-ordinateur ind√©pendant** (souvent bas√© sur un SoC ARM) soud√© √† la carte m√®re du serveur. Il dispose de :

- Sa propre **RAM** et son propre **OS** (souvent [OpenBMC](https://github.com/openbmc/openbmc))
- Sa propre **interface r√©seau** (port d√©di√© ou partag√©)
- Il reste **allum√©** m√™me si le serveur est √©lectriquement √©teint (mais branch√©)

Il permet : console s√©rie/KVM √† distance, power on/off/reset, lecture capteurs (temp√©rature, tension), logs mat√©riels (SEL), mise √† jour firmware.

---

### 2.2 IPMI vs Redfish

| Crit√®re | IPMI | Redfish |
|--------|------|---------|
| **Standard** | Historique (port UDP 623) | Moderne DMTF |
| **Interface** | Binaire, protocole propri√©taire | **API REST** (HTTPS), r√©ponses **JSON** |
| **S√©curit√©** | Failles connues si expos√© sur le r√©seau | HTTPS, authentification, mieux adapt√© √† l‚Äôautomatisation |
| **Usage** | Outils type `ipmitool` | `curl`, scripts, orchestration (Ansible, Terraform) |

**En pratique** : Redfish est l‚Äô**avenir** du provisionnement et de la supervision mat√©rielle automatis√©e.

---

### Exemple : Red√©marrage forc√© d‚Äôun n≈ìud bloqu√© (IPMI)

```bash
# DANGER : Coupe brutalement l‚Äôalimentation (√©quivalent bouton power maintenu)
ipmitool -I lanplus -H 10.0.1.50 -U admin -P secret_pass power reset
```

---

### Exemple : Temp√©rature CPU via Redfish (REST/JSON)

```bash
curl -u admin:secret_pass -k https://10.0.1.50/redfish/v1/Chassis/1/Thermal | \
  jq '.Temperatures[] | select(.Name=="CPU1 Temp") | .ReadingCelsius'
```

---

### ‚ö†Ô∏è S√©curit√© OOB en production

> **DANGER** : L‚Äôexposition du r√©seau BMC/IPMI sur Internet ou sur le r√©seau de recherche est une **faille critique** (CVSS 10.0). Ces interfaces doivent √™tre **strictement isol√©es** sur un VLAN d‚Äôadministration, id√©alement **non rout√©** vers Internet.

- R√©seau **Out-of-Band** d√©di√©
- Acc√®s restreint (VPN, bastion)
- Changer les mots de passe par d√©faut et les stocker dans un coffre (vault)

---

## Chapitre 3 : Provisioning bare-metal et gestion du cycle de vie

### Objectifs d'apprentissage

- Comprendre la **cha√Æne de boot PXE**
- D√©ployer et configurer **Warewulf v4** pour provisionner des n≈ìuds **stateless**

**Pr√©requis** : DHCP, TFTP, notions de conteneurs OCI.

---

### 3.1 La cha√Æne PXE (Preboot eXecution Environment)

Dans beaucoup de clusters HPC, les n≈ìuds de calcul **n‚Äôont pas de disque OS local** (ou seulement cache/scratch). Ils d√©marrent **par le r√©seau** (stateless).

| √âtape | R√¥le |
|-------|------|
| 1 | Le n≈ìud s‚Äôallume ; la carte r√©seau envoie un **DHCP Discover** |
| 2 | Le serveur Management (DHCP) r√©pond avec une **IP** et l‚Äô**adresse du serveur TFTP** |
| 3 | Le n≈ìud t√©l√©charge **iPXE** (ou PXELinux) via **TFTP** |
| 4 | iPXE t√©l√©charge le **noyau Linux** (`vmlinuz`) et l‚Äô**initramfs** (souvent en HTTP, plus rapide que TFTP) |
| 5 | Le n≈ìud charge l‚Äô**OS en RAM** (tmpfs) ; au red√©marrage, tout est re-t√©l√©charg√© |

---

### 3.2 Provisioning avec Warewulf v4

Warewulf v4 utilise des **conteneurs OCI** (compatibles Docker / Apptainer) comme **images syst√®me** pour les n≈ìuds, au lieu d‚Äôimages chroot traditionnelles.

**Workflow typique (sur le n≈ìud Management) :**

```bash
# 1. Importer une image de base depuis un registre
wwctl container import docker://rockylinux:9 rockylinux-9-hpc

# 2. Entrer dans le conteneur pour installer des paquets (client Slurm, Mellanox OFED, etc.)
wwctl container exec rockylinux-9-hpc /bin/bash
# > dnf install epel-release -y
# > dnf install slurm-slurmd munge -y
# > exit

# 3. Enregistrer un n≈ìud (node01) avec MAC et IP
wwctl node add node01 --macaddr=00:11:22:33:44:55 --ipaddr=10.10.0.1 --container=rockylinux-9-hpc

# 4. Configurer le r√©seau PXE et red√©marrer les services
wwctl configure --all
```

---

### √Ä retenir (stateless)

> Dans une architecture **stateless**, toute modification faite **directement sur un n≈ìud** (via SSH) sera **perdue au prochain red√©marrage**. Les changements doivent √™tre faits :
> 1. Dans l‚Äô**image conteneur Warewulf** (`wwctl container exec`), ou  
> 2. Via **Ansible** (ou autre Configuration Management) appliqu√© apr√®s chaque boot.

---

## Chapitre 4 : Configuration Management & GitOps en HPC

### Objectifs d'apprentissage

- Automatiser la **configuration post-d√©ploiement** avec **Ansible**
- √âviter l‚Äôeffet **¬´ Snowflake ¬ª** (n≈ìuds ayant d√©riv√© de la configuration initiale)

**Pr√©requis** : Bases YAML, SSH.

---

### 4.1 Ansible √† l‚Äô√©chelle HPC

Ansible utilise un mod√®le **push** via SSH. Sur 10 n≈ìuds, c‚Äôest rapide ; sur **2000 n≈ìuds**, la configuration par d√©faut peut √™tre trop lente.

**Tuning recommand√©** dans `ansible.cfg` :

```ini
[defaults]
forks = 100          # Parall√©liser sur 100 n≈ìuds simultan√©ment
pipelining = True    # R√©duit le nombre de connexions SSH
strategy = free      # Les n≈ìuds n‚Äôattendent pas les autres pour la t√¢che suivante
```

---

### 4.2 Exemple : durcissement et tuning OS pour HPC

Extrait de playbook typique : d√©sactiver services inutiles (r√©duction du **OS Jitter**, important pour la latence MPI), d√©sactiver le swap, fixer le governor CPU en **performance**.

```yaml
---
- name: HPC Compute Node Hardening & Tuning
  hosts: compute_nodes
  tasks:
    - name: D√©sactiver firewalld (g√©r√© au niveau r√©seau en HPC)
      ansible.builtin.systemd:
        name: firewalld
        state: stopped
        enabled: no

    - name: D√©sactiver le swap (les jobs HPC ne doivent pas swapper ; OOM si manque de RAM)
      ansible.posix.mount:
        name: swap
        fstype: swap
        state: absent

    - name: Governor CPU en performance
      ansible.builtin.command: cpupower frequency-set -g performance
      changed_when: false
```

---

## üß™ Lab 1 : Provisioning d‚Äôun mini-cluster ¬´ from scratch ¬ª

### √ânonc√©

Vous disposez d‚Äôun hyperviseur **KVM**. Vous devez :

1. D√©ployer une **VM Master** (Rocky Linux 9) avec deux interfaces : WAN et LAN `10.0.0.1/24`
2. Installer **Warewulf v4** sur le Master
3. Importer une image Rocky 9, y installer **htop** et **munge**
4. Cr√©er **2 VMs** (node01, node02) d√©marrant en **PXE** sur le r√©seau LAN
5. Les faire **booter** avec succ√®s sur votre image

### Crit√®res de r√©ussite

- `ping node01` et `ping node02` r√©pondent depuis le Master
- **SSH** (sans mot de passe, cl√©s g√©r√©es par Warewulf) fonctionne vers les n≈ìuds
- La commande **htop** est disponible sur les n≈ìuds

### Corrig√© (grandes lignes)

- Sur le Master : `dnf install epel-release` ; installer Warewulf selon la doc officielle
- √âditer `/etc/warewulf/warewulf.conf` : interface LAN (ex. `eth1`), plage DHCP (ex. `10.0.0.50`‚Äì`10.0.0.100`)
- Lancer `wwctl configure --all` (configure DHCP, TFTP, NFS si utilis√©)
- `wwctl container import docker://rockylinux:9 base_image`
- `wwctl container exec base_image dnf install htop munge -y`
- Ajouter les n≈ìuds : `wwctl node add node01 --netdev eth0 --hwaddr <MAC_VM1>` (idem pour node02)
- D√©marrer les VMs clientes en **boot r√©seau (PXE)** et v√©rifier le chargement de l‚Äôimage

---

## üìù Examen de fin de volume 1

### QCM (1 point chaque)

**1.** Quelle architecture est la plus pertinente pour de la **dynamique des fluides (CFD)** fortement coupl√©e via MPI ?  
- A) Data-Intensive (grosse RAM)  
- B) **CPU-only avec r√©seau InfiniBand**  
- C) GPU-heavy avec Ethernet 1 Gbps  

**2.** Pourquoi pr√©f√®re-t-on **Redfish** √† IPMI aujourd‚Äôhui ?  
- A) Redfish est plus rapide pour booter l‚ÄôOS  
- B) **Redfish utilise des standards web modernes (REST/JSON), plus s√ªrs et automatisables**  
- C) IPMI ne supporte pas les CPU AMD  

---

### Question ouverte (sc√©nario d‚Äôexploitation)

Vous √™tes d‚Äôastreinte. Le cluster de 500 n≈ìuds est **injoignable par SSH** (timeout). L‚Äôalimentation de la salle est normale. Quelle est votre **premi√®re action technique** pour diagnostiquer sans vous d√©placer physiquement ?

**R√©ponse attendue** : Se connecter via le **r√©seau OOB/Management** (IPMI ou Redfish) pour ouvrir une **console KVM distante** (Virtual Console) ou consulter les **logs mat√©riels (SEL ‚Äì System Event Log)** et v√©rifier l‚Äô√©tat des n≈ìuds (power, panne, etc.).

---

### √âtude de cas : ¬´ Le n≈ìud amn√©sique ¬ª

Un administrateur junior modifie **directement** le fichier `/etc/security/limits.conf` sur le n≈ìud `compute-045` via SSH pour r√©gler un probl√®me de limites de fichiers ouverts. Il red√©marre le n≈ìud pour appliquer la modification. **Au red√©marrage, la modification a disparu.**

- **Expliquez** techniquement pourquoi.  
- **D√©taillez** la proc√©dure correcte pour rendre cette modification **persistante** dans une architecture HPC moderne (stateless).

**R√©ponse attendue :**

- **Pourquoi** : Le cluster utilise un provisioning **stateless** (ex. Warewulf) ; l‚ÄôOS tourne en RAM (tmpfs). Au reboot, l‚Äôimage est **re-t√©l√©charg√©e** depuis le Master : toute modification locale est perdue.
- **Proc√©dure correcte** :  
  1) Faire la modification dans le **conteneur source** via `wwctl container exec <image>`, puis reconstruire/d√©ployer l‚Äôimage ; **ou**  
  2) Ajouter un **playbook Ansible** qui d√©ploie ce fichier (ou ce param√®tre), et l‚Äôappliquer apr√®s chaque d√©ploiement/red√©marrage.

---

## Solutions des QCM

- **Q1** : **B** ‚Äî En CFD fortement coupl√©e, le ratio communication/calcul est √©lev√© ; le r√©seau √† **faible latence** (InfiniBand) est d√©terminant.
- **Q2** : **B** ‚Äî Redfish est une API REST/JSON, adapt√©e √† l‚Äôautomatisation et √† une meilleure s√©curit√© (HTTPS).

---

## Liens utiles

- **[Sommaire complet du Manuel HPC](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Manuel-HPC-Sommaire-Complet.md)** : plan des 8 volumes, chapitres, labs, annexes
- **[Cours HPC Complet](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Cours-HPC-Complet.md)** : concepts g√©n√©raux, parall√©lisme, stockage, GPU
- **[Guide SLURM Complet](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Guide-SLURM-Complet.md)** : ordonnancement et soumission de jobs
- **[Glossaire et Acronymes](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Glossaire-et-Acronymes.md)** : d√©finitions (BMC, PXE, Redfish, Lustre, etc.)
- **[Home](https://github.com/mickaelangel/hpc-cluster/blob/main/.github/wiki/Home.md)** : page d‚Äôaccueil du wiki

---

**Volume 1** ‚Äî Fondations, architecture de base et provisioning DevOps  
**Derni√®re mise √† jour** : 2024
