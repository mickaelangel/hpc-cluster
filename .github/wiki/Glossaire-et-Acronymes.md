# üìñ Glossaire et dictionnaire des acronymes HPC

> **R√©f√©rence Master Data Science / Doctorat & DevOps Senior**

---

## Comment utiliser ce glossaire

- **Acronymes** : tri alphab√©tique, avec expansion et courte d√©finition (tableaux par lettre).
- **Termes** : notions importantes du domaine HPC, clusters, stockage, r√©seau, logiciels.
- **Glossaire technique d√©taill√© (A‚ÜíZ)** : en bas de page, d√©finitions longues avec exemples et termes li√©s pour les notions les plus critiques (architecture, Slurm, Lustre, IA, r√©seaux).
- **Dictionnaire encyclop√©dique HPC** : entr√©es encyclop√©diques compl√®tes (d√©finition rigoureuse, internals, bonnes/mauvaises pratiques, commandes, tuning, troubleshooting, r√©f√©rences) ‚Äî voir [Dictionnaire-Encyclopedique-HPC](Dictionnaire-Encyclopedique-HPC).

---

## A

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **ACL** | Access Control List ‚Äî liste de contr√¥le d‚Äôacc√®s (fichiers, r√©seaux). |
| **API** | Application Programming Interface ‚Äî interface de programmation. |
| **AWX** | Projet open source fournissant une interface web et API pour Ansible (Automation Controller). |
| **ANSSI** | Agence nationale de la s√©curit√© des syst√®mes d‚Äôinformation (r√©f√©rentiels s√©curit√© France). |
| **Apptainer** | Nouveau nom du projet Singularity (conteneurs HPC). |
| **AVX** | Advanced Vector Extensions ‚Äî jeux d‚Äôinstructions SIMD x86 (AVX, AVX2, AVX-512). |

---

## B

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **BeeGFS** | Syst√®me de fichiers parall√®le (ex-Parallel Virtual File System, puis Fraunhofer). Tr√®s utilis√© en HPC. |
| **BLAS** | Basic Linear Algebra Subprograms ‚Äî biblioth√®que standard d‚Äôalg√®bre lin√©aire (performance CPU). |
| **Burst buffer** | Couche de stockage tampon (souvent SSD/NVMe) entre calcul et stockage parall√®le pour absorber les pics I/O. |

---

## C

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **Ceph** | Syst√®me de stockage distribu√© (objet, bloc, fichier) open source. |
| **CI/CD** | Continuous Integration / Continuous Delivery (or Deployment) ‚Äî int√©gration et d√©ploiement continus. |
| **CIS** | Center for Internet Security ‚Äî benchmarks de durcissement (ex. CIS Level 2). |
| **Cloud** | Informatique en nuage (IaaS, PaaS, SaaS) ; parfois utilis√© pour offres HPC (cloud HPC). |
| **CPU** | Central Processing Unit ‚Äî processeur. |
| **CUDA** | Compute Unified Device Architecture ‚Äî plateforme NVIDIA pour calcul sur GPU. |
| **cgroups** | Control groups ‚Äî m√©canisme Linux pour limiter et mesurer l‚Äôusage des ressources (CPU, m√©moire, I/O). |

---

## D

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **Daemon** | Programme qui tourne en arri√®re-plan (ex. slurmd, slurmctld). |
| **Data parallelism** | Parall√©lisme par les donn√©es (m√™me code, donn√©es diff√©rentes). |
| **DISCO** | Terme parfois utilis√© pour stockage ou donn√©es distribu√©es (selon contexte). |
| **DISA STIG** | Defense Information Systems Agency Security Technical Implementation Guide ‚Äî durcissement s√©curit√© (US). |
| **DNS** | Domain Name System ‚Äî r√©solution noms de domaine. |
| **DPU** | Data Processing Unit ‚Äî processeur d√©di√© au r√©seau/stockage (ex. NVIDIA BlueField). |
| **DRAM** | Dynamic Random Access Memory ‚Äî m√©moire vive classique. |

---

## E

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **Efficacit√©** | En parall√©lisme : E(n) = Speedup(n) / n. Mesure l‚Äôutilisation des c≈ìurs. |
| **Ethernet** | Technologie r√©seau (1G, 10G, 25G, 100G) ; souvent utilis√©e pour interconnexion HPC (avec ou sans RoCE). |
| **Exascale** | Ordre de grandeur : 10^18 op√©rations/s (1 exaflops). |
| **Exporters** | Programmes qui exposent des m√©triques pour Prometheus (ex. node_exporter, slurm_exporter). |

---

## F

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **Fair-share** | Politique d‚Äôordonnancement qui favorise les utilisateurs/projets qui ont moins consomm√© r√©cemment. |
| **FLOPS** | Floating Point Operations Per Second ‚Äî op√©rations en virgule flottante par seconde (K/M/G/T/P FLOPS). |
| **FreeIPA** | Suite d‚Äôauthentification (LDAP, Kerberos, DNS, certificats) pour environnements Linux. |
| **Frontal** | N≈ìud d‚Äôacc√®s utilisateur (login, soumission de jobs), par opposition aux n≈ìuds de calcul. |
| **FS** | File System ‚Äî syst√®me de fichiers. |

---

## G

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **GFLOPS** | Giga FLOPS ‚Äî 10^9 op√©rations/s. |
| **GitOps** | Gestion de l‚Äôinfrastructure et des d√©ploiements via Git (d√©claratif, ex. ArgoCD, Flux). |
| **GlusterFS** | Syst√®me de fichiers distribu√© (scale-out) open source. |
| **GPU** | Graphics Processing Unit ‚Äî processeur graphique, utilis√© pour calcul (CUDA, ROCm, etc.). |
| **Grafana** | Outil de visualisation de m√©triques et de logs (dashboards). |
| **GRES** | Generic Resource ‚Äî ressource g√©n√©rique dans Slurm (ex. GPU, licences). |

---

## H

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **HA** | High Availability ‚Äî haute disponibilit√©. |
| **HDF5** | Hierarchical Data Format version 5 ‚Äî format et biblioth√®ques pour donn√©es scientifiques (I/O parall√®le possible). |
| **HPC** | High Performance Computing ‚Äî calcul haute performance. |
| **HTC** | High Throughput Computing ‚Äî grand nombre de t√¢ches ind√©pendantes (vs. gros jobs parall√®les). |

---

## I

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **IaaS** | Infrastructure as a Service. |
| **IaC** | Infrastructure as Code ‚Äî infrastructure d√©finie en code (Terraform, Ansible, etc.). |
| **IAM** | Identity and Access Management ‚Äî gestion des identit√©s et des acc√®s. |
| **IB** | InfiniBand ‚Äî r√©seau √† haute performance (faible latence, haut d√©bit). |
| **InfluxDB** | Base de donn√©es temporelle (time-series) pour m√©triques et √©v√©nements. |
| **IOPS** | Input/Output Operations Per Second ‚Äî op√©rations d‚ÄôI/O par seconde. |
| **I/O** | Input/Output ‚Äî entr√©es/sorties (disque, r√©seau). |

---

## J

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **JupyterHub** | Serveur multi-utilisateurs pour Jupyter (notebooks). |
| **Job** | Unit√© de travail soumise au scheduler (ex. un script sbatch, une session srun). |
| **JWT** | JSON Web Token ‚Äî jeton d‚Äôauthentification. |

---

## K

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **Kerberos** | Protocole d‚Äôauthentification r√©seau (tickets, SSO). |
| **Kubernetes (K8s)** | Orchestrateur de conteneurs (pods, services, d√©ploiements). |

---

## L

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **LAN** | Local Area Network. |
| **LDAP** | Lightweight Directory Access Protocol ‚Äî annuaire (utilisateurs, groupes). |
| **Lustre** | Syst√®me de fichiers parall√®le tr√®s r√©pandu en HPC (metadata servers + object storage servers). |
| **LSF** | Load Sharing Facility ‚Äî scheduler commercial (IBM). |
| **Loki** | Syst√®me de stockage et requ√™tes de logs (int√©gr√© √† Grafana). |

---

## M

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **MDS** | Metadata Server ‚Äî serveur de m√©tadonn√©es (Lustre). |
| **MFA** | Multi-Factor Authentication ‚Äî authentification multi-facteurs. |
| **MPI** | Message Passing Interface ‚Äî standard de programmation parall√®le √† m√©moire distribu√©e (multi-n≈ìuds). |
| **MPICH** | Impl√©mentation open source de MPI. |
| **MVAPICH** | Impl√©mentation MPI optimis√©e pour InfiniBand. |

---

## N

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **NAS** | Network Attached Storage ‚Äî stockage en r√©seau. |
| **NetCDF** | Network Common Data Form ‚Äî format et API pour donn√©es scientifiques (I/O parall√®le possible). |
| **NFS** | Network File System ‚Äî syst√®me de fichiers r√©seau (partage de r√©pertoires). |
| **NVMe** | Non-Volatile Memory Express ‚Äî interface pour SSD rapides. |

---

## O

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **OOM** | Out Of Memory ‚Äî manque de m√©moire (processus tu√© par le noyau). |
| **OpenMP** | API de parall√©lisme √† m√©moire partag√©e (threads sur un n≈ìud). |
| **OpenMPI** | Impl√©mentation open source de MPI. |
| **OSS** | Object Storage Server ‚Äî serveur de stockage d‚Äôobjets (Lustre). |

---

## P

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **PaaS** | Platform as a Service. |
| **PBS** | Portable Batch System ‚Äî famille de schedulers (PBS Pro, OpenPBS). |
| **PDU** | Power Distribution Unit ‚Äî unit√© de distribution √©lectrique (racks). |
| **PFLOPS** | Peta FLOPS ‚Äî 10^15 op√©rations/s. |
| **Prometheus** | Syst√®me de collecte et de requ√™tes de m√©triques (PromQL). |
| **PromQL** | Langage de requ√™te des m√©triques Prometheus. |
| **PV** | Persistent Volume ‚Äî volume persistant (Kubernetes). |
| **PXE** | Preboot eXecution Environment ‚Äî d√©marrage r√©seau. |

---

## Q

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **QoS** | Quality of Service ‚Äî dans Slurm : ensemble de limites (temps, n≈ìuds, etc.) et priorit√©s associ√©es √† un type de job. |
| **Quota** | Limite d‚Äôusage (CPU-heures, espace disque, nombre de jobs). |

---

## R

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **RBAC** | Role-Based Access Control ‚Äî contr√¥le d‚Äôacc√®s par r√¥les. |
| **RDMA** | Remote Direct Memory Access ‚Äî acc√®s m√©moire distant (sans CPU), utilis√© en InfiniBand et RoCE. |
| **RoCE** | RDMA over Converged Ethernet ‚Äî RDMA sur Ethernet. |
| **ROCm** | Plateforme AMD pour calcul sur GPU. |
| **RPC** | Remote Procedure Call ‚Äî appel de proc√©dure √† distance. |

---

## S

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **SaaS** | Software as a Service. |
| **Scheduler** | Ordonnanceur de jobs (Slurm, PBS, LSF, etc.). |
| **SIMD** | Single Instruction Multiple Data ‚Äî une instruction, plusieurs donn√©es (vectorisation). |
| **Singularity** | Conteneurs pour HPC (projet renomm√© Apptainer). |
| **Slurm** | Simple Linux Utility for Resource Management ‚Äî scheduler et gestionnaire de ressources. |
| **slurmctld** | D√©mon contr√¥leur Slurm. |
| **slurmd** | D√©mon Slurm sur chaque n≈ìud de calcul. |
| **slurmdbd** | D√©mon base de donn√©es Slurm (comptabilit√©, multi-cluster). |
| **Speedup** | S(n) = T(1)/T(n) ‚Äî acc√©l√©ration obtenue avec n processeurs. |
| **SSD** | Solid State Drive ‚Äî disque √† semi-conducteurs. |
| **SSH** | Secure Shell ‚Äî acc√®s √† distance s√©curis√©. |
| **SSL/TLS** | Protocoles de s√©curisation des communications. |

---

## T

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **TFLOPS** | Tera FLOPS ‚Äî 10^12 op√©rations/s. |
| **TLB** | Translation Lookaside Buffer ‚Äî cache mat√©riel pour la traduction d‚Äôadresses virtuelles en physiques. |
| **TOTP** | Time-based One-Time Password ‚Äî mot de passe √† usage unique (ex. Google Authenticator). |
| **Torque** | Scheduler d√©riv√© de PBS (open source). |

---

## U

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **UCX** | Unified Communication X ‚Äî couche de communication unifi√©e pour MPI et RPC (support InfiniBand, RoCE, GPU). |
| **UID** | User Identifier ‚Äî identifiant num√©rique d‚Äôun utilisateur (Linux). |

---

## V

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **VRAM** | Video RAM ‚Äî m√©moire d√©di√©e au GPU. |
| **VPN** | Virtual Private Network. |

---

## W

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **Walltime** | Temps r√©el (horloge) allou√© √† un job (vs. temps CPU). |
| **Workload** | Charge de travail ‚Äî ensemble des jobs ou t√¢ches √† ex√©cuter. |

---

## X

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **x86** | Architecture processeur (Intel, AMD). |
| **x86_64** | Architecture 64 bits (AMD64 / Intel 64). |

---

## Z

| Acronyme / Terme | Expansion / D√©finition |
|------------------|------------------------|
| **Zero Trust** | Mod√®le de s√©curit√© o√π rien n‚Äôest consid√©r√© comme fiable par d√©faut (v√©rification continue). |

---

## Termes m√©tier (d√©finitions courtes)

| Terme | D√©finition |
|-------|------------|
| **Allocation** | Ensemble de ressources (n≈ìuds, CPU, m√©moire, GPU) attribu√©es √† un job par le scheduler. |
| **Cluster** | Ensemble de n≈ìuds (frontaux + calcul + √©ventuellement stockage) g√©r√©s comme une ressource commune. |
| **Compute node** | N≈ìud d√©di√© √† l‚Äôex√©cution des jobs (pas de login utilisateur direct en g√©n√©ral). |
| **Interconnect** | R√©seau reliant les n≈ìuds de calcul (InfiniBand, Ethernet, RoCE). |
| **Login node** | N≈ìud frontal pour connexion SSH et soumission de jobs. |
| **Metadata** | Donn√©es d√©crivant les fichiers (nom, taille, permissions) ‚Äî s√©par√©es des donn√©es en Lustre/BeeGFS. |
| **Partition** | Dans Slurm : file d‚Äôattente associ√©e √† un ensemble de n≈ìuds et des r√®gles (temps max, etc.). |
| **Step** | Sous-partie d‚Äôun job Slurm (ex. un appel √† srun dans un script). |
| **Supercomputer** | Tr√®s gros syst√®me HPC (souvent class√© Top500). |
| **Throughput** | D√©bit ‚Äî volume de travail ou de donn√©es trait√© par unit√© de temps. |

---

## Glossaire technique d√©taill√© (A‚ÜíZ)

D√©finitions longues avec exemples et termes li√©s pour les notions les plus critiques (architecture, Slurm, Lustre, IA, r√©seaux).

---

### A

**Affinity (Affinit√©)**  
- **Courte** : Fait de forcer l‚Äôex√©cution d‚Äôun processus ou l‚Äôallocation de la m√©moire sur un composant mat√©riel sp√©cifique (c≈ìur CPU ou n≈ìud NUMA).  
- **Longue** : En architectures multi-sockets ou multi-chiplets, l‚ÄôOS d√©place les processus pour √©quilibrer la charge ; en HPC cela d√©truit la localit√© du cache (cache misses). L‚Äôaffinit√© (pinning / binding) verrouille le processus MPI sur un c≈ìur pour des perfs maximales et pr√©dictibles.  
- **Exemple** : `numactl --physcpubind=0-7` ou l‚Äôoption Slurm `--bind-to core`.  
- **Termes li√©s** : NUMA, Cgroups, Slurm GRES, First Touch.

**Apptainer (ex-Singularity)**  
- **Courte** : Moteur de conteneurs con√ßu pour le calcul intensif (HPC).  
- **Longue** : Pas de d√©mon root comme Docker ; l‚Äôutilisateur garde son UID/GID dans le conteneur. Monte nativement les FS parall√®les (Lustre) et lie les pilotes GPU (`--nv`) dans un fichier image unique (.sif).  
- **Exemple** : `apptainer exec --nv mon_image.sif python script.py`  
- **Termes li√©s** : Docker, OCI, SIF, environnements utilisateurs.

---

### B

**Backfill (Ordonnancement par remplissage)**  
- **Courte** : Algorithme de Slurm qui lance des petits jobs sur des n≈ìuds inactifs en attendant qu‚Äôun grand job d√©marre.  
- **Longue** : Le scheduler calcule l‚Äôheure de d√©marrage du prochain job prioritaire ; si un petit job peut finir avant, il est lanc√©. N√©cessite un **walltime** pr√©cis.  
- **Exemple** : Un job de 2 h est lanc√© sur un n≈ìud qui doit rester libre 3 h pour une grosse simulation.  
- **Termes li√©s** : Slurm, Walltime, Fairshare, Scheduler.

**BeeGFS**  
- **Courte** : Syst√®me de fichiers parall√®le open-source (ThinkParQ), alternative √† Lustre pour le HPC.  
- **Longue** : S√©pare m√©tadonn√©es (MDS) et stockage (storage targets) ; client en espace utilisateur ou noyau, protocole propri√©taire sur TCP ou RDMA. Pas de DLM centralis√© comme Lustre ; d√©ploiement plus simple, scaling horizontal.  
- **Exemple** : `beegfs-ctl`, `beegfs-df` pour le diagnostic.  
- **Termes li√©s** : Lustre, Striping, POSIX, MPI-IO.

**Burst Buffer**  
- **Courte** : Couche de stockage interm√©diaire ultra-rapide (souvent NVMe) entre les n≈ìuds de calcul et le FS parall√®le (Lustre/GPFS).  
- **Longue** : Absorbe les pics d‚Äô√©critures synchrones (ex. checkpointing MPI). Les donn√©es sont √©crites tr√®s vite sur le burst buffer puis drain√©es vers le stockage capacitif en arri√®re-plan.  
- **Exemple** : Cray DataWarp, allocations NVMe locales via Slurm.  
- **Termes li√©s** : Checkpoint/Restart, Lustre OST, Tiering.

---

### C

**Cgroups (Control Groups) / Cgroups v2**  
- **Courte** : M√©canisme du noyau Linux pour limiter, isoler et mesurer l‚Äôusage des ressources (CPU, RAM, I/O) par un groupe de processus.  
- **Longue** : En HPC, Slurm utilise les **Cgroups v2** (task/cgroup) : un cgroup par job avec limite m√©moire (ConstrainRAMSpace, AllowedRAMSpace). Si le job d√©passe, l‚ÄôOOM-Killer n‚Äôagit que dans ce cgroup ‚Üí seul le job est tu√©, pas slurmd.  
- **Exemple** : `ConstrainRAMSpace=yes`, `AllowedRAMSpace=98` dans cgroup.conf.  
- **Termes li√©s** : Namespaces, Slurm, OOM-Killer, Multi-tenancy.

**Co-design**  
- **Courte** : Conception conjointe et it√©rative du mat√©riel et du logiciel pour maximiser les performances.  
- **Longue** : On ne choisit pas des serveurs au hasard : on analyse le profil du code (memory-bound, compute-bound, I/O-bound) pour dimensionner r√©seau, CPU/GPU et stockage et √©liminer le goulot.  
- **Exemple** : Choisir des processeurs avec HBM pour un code de dynamique des fluides (memory-bound).  
- **Termes li√©s** : Roofline Model, Profiling, Benchmarking.

---

### D

**DLM (Distributed Lock Manager)**  
- **Courte** : Sous-syst√®me Lustre (LDLM) qui garantit la coh√©rence POSIX des donn√©es et m√©tadonn√©es entre clients concurrents.  
- **Longue** : G√®re les verrous d‚Äô√©tendue (extent locks) : un client qui √©crit re√ßoit un verrou exclusif ; un autre qui lit d√©clenche une AST pour flush et r√©trogradation. Le trafic de verrous (lock traffic) peut d√©grader les perfs si les applications font du false sharing.  
- **Exemple** : `lctl get_param ldlm.namespaces.*.lock_count` pour le nombre de verrous actifs.  
- **Termes li√©s** : Lustre, POSIX, Striping, MPI-IO, False Sharing.

**DNE (Distributed Namespace)**  
- **Courte** : Fonctionnalit√© Lustre qui r√©partit la charge des m√©tadonn√©es sur plusieurs MDT.  
- **Longue** : Un seul MDT cr√©ait un goulot (millions de fichiers). DNE permet d‚Äôaffecter des r√©pertoires (Phase 1) ou des fichiers d‚Äôun m√™me r√©pertoire (Phase 2) √† plusieurs MDTs pour scaler les m√©tadonn√©es.  
- **Exemple** : `/scratch/projetA` sur MDT0, `/scratch/projetB` sur MDT1.  
- **Termes li√©s** : Lustre, MDS/MDT, M√©tadonn√©es, IOPS.

---

### E

**Eager Protocol**  
- **Courte** : Protocole MPI pour l‚Äôenvoi de **petits** messages.  
- **Longue** : L‚Äôexp√©diteur envoie directement la donn√©e en supposant des buffers de r√©ception pr√©-allou√©s ; pas de handshake, donc tr√®s faible latence, mais inadapt√© aux gros transferts (saturation RAM).  
- **Exemple** : Un `MPI_Send` de 10 Ko utilise typiquement Eager.  
- **Termes li√©s** : Rendezvous Protocol, MPI, Latence, RDMA.

---

### F

**Fairshare**  
- **Courte** : Algorithme qui r√©partit √©quitablement le temps de calcul entre utilisateurs sur une p√©riode.  
- **Longue** : Ajuste la priorit√© des jobs : si un labo a consomm√© plus que sa part, la priorit√© de ses prochains jobs baisse au profit des sous-consommateurs. L‚Äôhistorique s‚Äôestompe (demi-vie).  
- **Exemple** : `PriorityWeightFairshare=100000` dans Slurm donne un poids fort √† l‚Äô√©quit√©.  
- **Termes li√©s** : Slurm, Priority, Accounting, Chargeback.

**Fat-Tree (Topologie de Clos)**  
- **Courte** : Architecture r√©seau o√π tous les n≈ìuds ont la m√™me bande passante pour communiquer.  
- **Longue** : En remontant vers le core, les liens sont multipli√©s (Spine-Leaf). Un Fat-Tree non-blocking (1:1) garantit qu‚Äôaucun lien ne sature quand la moiti√© du cluster parle √† l‚Äôautre.  
- **Exemple** : Chaque n≈ìud √† 200 Gbps sur le Leaf, assez de liens montants vers les Spines pour √©couler tout le trafic.  
- **Termes li√©s** : Spine-Leaf, InfiniBand, Oversubscription.

---

### G

**GPUDirect RDMA**  
- **Courte** : Technologie NVIDIA permettant √† une carte r√©seau de lire/√©crire **directement** dans la VRAM d‚Äôun GPU.  
- **Longue** : Sans cela, GPU‚ÜíRAM CPU‚Üícarte r√©seau‚Üír√©seau‚Üí‚Ä¶ ; avec GPUDirect, la carte r√©seau acc√®de √† la VRAM via PCIe, sans CPU, divisant la latence et lib√©rant le processeur.  
- **Exemple** : Indispensable pour l‚Äôentra√Ænement distribu√© de LLM via NCCL.  
- **Termes li√©s** : RDMA, NCCL, PCIe, VRAM, OS Bypass.

**GPU Tensor Cores**  
- **Courte** : Unit√©s de calcul matriciel (GEMM) dans les GPU NVIDIA (Volta et suivants), optimis√©es FP16/BF16/INT8.  
- **Longue** : Acc√©l√®rent massivement les multiplications matricielles des r√©seaux de neurones (convolutions, attention). Un n≈ìud avec Tensor Cores atteint des TFLOP/s bien sup√©rieurs aux c≈ìurs CUDA classiques pour ces op√©rations.  
- **Exemple** : PyTorch avec `torch.autocast` ou TF32 sur Ampere.  
- **Termes li√©s** : CUDA, cuBLAS, cuDNN, NVLink, Mixed Precision.

---

### H

**HPL (High Performance Linpack)**  
- **Courte** : Benchmark standard du classement **Top500**.  
- **Longue** : R√©sout un grand syst√®me lin√©aire dense ; tr√®s compute-bound, stresse les FPU et la consommation. Critiqu√© car il ne refl√®te pas les acc√®s m√©moire irr√©guliers de la science actuelle.  
- **Exemple** : Frontier a atteint 1,194 ExaFLOPS sur HPL.  
- **Termes li√©s** : HPCG, FLOPS, Top500, Benchmarking.

**Hugepages / TLB**  
- **Courte** : Pages m√©moire de 2 Mo ou 1 Go (vs 4 Ko standard) pour r√©duire les TLB misses et acc√©l√©rer la traduction d‚Äôadresses.  
- **Longue** : Un code avec 128 Go de RAM = 33 M de pages 4 Ko ‚Üí le TLB (cache d‚Äôadresses) sature. Avec des Hugepages 1 Go, 128 entr√©es suffisent. En HPC : activer les Hugepages explicites pour RDMA (memory registration) ; d√©sactiver les Transparent Hugepages (THP) sur les n≈ìuds de calcul (jitter).  
- **Exemple** : `cat /proc/meminfo | grep Huge` ; GRUB `hugepagesz=1G hugepages=64`.  
- **Termes li√©s** : NUMA, RDMA, OOM-Killer, perf.

---

### I

**InfiniBand (IB)**  
- **Courte** : R√©seau √† tr√®s haut d√©bit et tr√®s faible latence, dominant en HPC.  
- **Longue** : Con√ßu pour le calcul parall√®le : RDMA natif, fabric lossless, Subnet Manager pour les routes optimales (pas de BGP/OSPF).  
- **Exemple** : G√©n√©rations FDR (56G), EDR (100G), HDR (200G), NDR (400G), XDR (800G).  
- **Termes li√©s** : RoCE, Subnet Manager, RDMA, HCA.

---

### L

**LNet (Lustre Network)**  
- **Courte** : Couche d‚Äôabstraction r√©seau de Lustre.  
- **Longue** : Permet √† Lustre de tourner sur plusieurs protocoles (TCP/Ethernet, o2ib/InfiniBand) et g√®re le routage (LNet Routers), ex. clients 10G vers stockage IB 200G.  
- **Exemple** : `10.0.0.5@tcp` ou `192.168.1.10@o2ib`.  
- **Termes li√©s** : Lustre, RDMA, OS Bypass.

**Lustre**  
- **Courte** : Syst√®me de fichiers parall√®le open-source le plus utilis√© dans les supercalculateurs.  
- **Longue** : S√©pare les m√©tadonn√©es (MDS) des donn√©es (OSS). En strippant les fichiers sur de nombreux serveurs, il permet des √©critures simultan√©es √† des d√©bits d√©passant le To/s.  
- **Exemple** : Espace `/scratch` temporaire d‚Äôun cluster.  
- **Termes li√©s** : MDS/MDT, OSS/OST, MGS, Striping, POSIX.

---

### M

**MPI (Message Passing Interface)**  
- **Courte** : Standard de programmation pour la communication entre processus en environnement parall√®le distribu√©.  
- **Longue** : Les processus sur des milliers de n≈ìuds n‚Äôont pas la m√™me RAM ; ils s‚Äôenvoient des messages via le r√©seau. MPI d√©finit les appels (point-√†-point : MPI_Send ; collectifs : MPI_Allreduce).  
- **Exemple** : OpenMPI, MPICH, MVAPICH2.  
- **Termes li√©s** : OpenMP, RDMA, Eager, Rendezvous, Rank.

**MPI Collectives / AllReduce**  
- **Courte** : Op√©rations de groupe o√π tous les rangs participent (r√©duction + diffusion du r√©sultat).  
- **Longue** : MPI_Allreduce combine les donn√©es locales (somme, max, etc.) et renvoie le r√©sultat √† tous. En deep learning, la synchro des gradients utilise des AllReduce (NCCL ou MPI). Algorithmes typiques : ring, recursive halving ; la bande passante r√©seau et le choix d‚Äôalgorithme d√©terminent la scalabilit√©.  
- **Exemple** : OSU Micro-Benchmarks `osu_allreduce` ; PyTorch DDP avec NCCL.  
- **Termes li√©s** : MPI, NCCL, RDMA, Ring AllReduce.

**MUNGE (MUNGE Uid 'N' Gid Emporium)**  
- **Courte** : Service d‚Äôauthentification pour s√©curiser les communications intra-cluster.  
- **Longue** : Chiffre et signe les identit√©s (UID/GID) sans mot de passe. Slurm l‚Äôutilise. Tous les n≈ìuds doivent avoir la m√™me cl√© (munge.key) et √™tre synchronis√©s (NTP).  
- **Exemple** : L‚Äôerreur ¬´ Munge decode failed ¬ª indique souvent une d√©synchronisation d‚Äôhorloge (> 5 min).  
- **Termes li√©s** : Slurm, NTP, S√©curit√© intra-n≈ìud.

---

### N

**NCCL (NVIDIA Collective Communication Library)**  
- **Courte** : Biblioth√®que optimis√©e pour les communications multi-GPU et multi-n≈ìuds.  
- **Longue** : √âquivalent de MPI pour les topologies GPU (NVLink, PCIe, GPUDirect RDMA). D√©tecte la topologie pour cr√©er anneaux ou arbres de transfert optimaux (entra√Ænement distribu√©).  
- **Exemple** : PyTorch DDP utilise NCCL par d√©faut.  
- **Termes li√©s** : GPUDirect RDMA, MPI, NVLink.

**NUMA (Non-Uniform Memory Access)**  
- **Courte** : Architecture o√π le temps d‚Äôacc√®s √† la m√©moire d√©pend de sa position par rapport au processeur.  
- **Longue** : En bi-processeur, chaque CPU a sa RAM locale ; acc√©der √† la RAM de l‚Äôautre CPU passe par le bus inter-socket (latence et bande passante d√©grad√©es). Le tuning HPC vise √† garder calcul et donn√©es sur la m√©moire locale.  
- **Exemple** : `numactl --hardware` pour voir les distances entre sockets.  
- **Termes li√©s** : Affinity, Binding, Hugepages.

**NVLink**  
- **Courte** : Interconnexion NVIDIA √† haut d√©bit entre GPU (et optionnellement CPU) au sein d‚Äôun n≈ìud.  
- **Longue** : Bande passante bien sup√©rieure au PCIe ; permet aux GPU d‚Äô√©changer tenseurs et gradients sans saturer le bus. NVSwitch (n≈ìuds type DGX) connecte tous les GPU en full bisection.  
- **Exemple** : `nvidia-smi topo -m` pour la matrice de connectivit√© GPU.  
- **Termes li√©s** : GPUDirect RDMA, NCCL, Tensor Cores, Multi-GPU.

---

### O

**OOM-Killer (Out-Of-Memory Killer)**  
- **Courte** : M√©canisme du noyau Linux qui tue un processus quand la RAM est satur√©e.  
- **Longue** : Sans cgroups, un job qui d√©passe sa part peut d√©clencher l‚ÄôOOM-Killer ; celui-ci peut tuer sshd ou slurmd au lieu du job, rendant le n≈ìud DOWN.  
- **Exemple** : `dmesg` : ¬´ Out of memory: Kill process 1234 (python) score 950 or sacrifice child ¬ª.  
- **Termes li√©s** : Cgroups, Slurm, Swap (proscrit en HPC).

---

### R

**RDMA (Remote Direct Memory Access)**  
- **Courte** : Technologie permettant √† une carte r√©seau de lire/√©crire directement dans la RAM d‚Äôun autre ordinateur sans solliciter l‚ÄôOS ni le CPU.  
- **Longue** : Cl√© de la faible latence (~1 ¬µs). Contourne le noyau (OS Bypass) et la pile TCP/IP, supprimant copies de buffers et interruptions.  
- **Exemple** : InfiniBand et RoCE v2 sont bas√©s sur RDMA.  
- **Termes li√©s** : InfiniBand, RoCE, GPUDirect RDMA, OS Bypass.

**RoCE v2 (RDMA over Converged Ethernet)**  
- **Courte** : RDMA sur Ethernet (UDP/IP), alternative √† InfiniBand pour datacenters Ethernet.  
- **Longue** : Permet une latence proche de l‚ÄôIB sur 25/100 GbE √† condition d‚Äôactiver PFC (Priority Flow Control) et √©ventuellement ECN pour √©viter les pertes. Sensible aux pertes de paquets.  
- **Exemple** : `rdma link` ; UCX avec RoCE.  
- **Termes li√©s** : RDMA, InfiniBand, PFC, Spine-Leaf.

**Root Squash**  
- **Courte** : Mesure de s√©curit√© sur NFS/Lustre : les requ√™tes de l‚ÄôUID 0 (root) sont r√©trograd√©es en ¬´ nobody ¬ª.  
- **Longue** : Emp√™che qu‚Äôun root local (ex. via faille ou sudo) lise/√©crive les fichiers des autres sur le stockage central.  
- **Exemple** : Param√®tre d‚Äôexport NFS `root_squash`.  
- **Termes li√©s** : NFS, S√©curit√©, IAM, Multi-tenancy.

---

### S

**Slurm (Simple Linux Utility for Resource Management)**  
- **Courte** : Ordonnanceur (scheduler / workload manager) open-source le plus utilis√© en HPC.  
- **Longue** : G√®re la file d‚Äôattente, l‚Äôallocation des ressources (n≈ìuds, CPU, GPU, m√©moire) et fournit l‚Äôenvironnement (srun) pour lancer les t√¢ches parall√®les de fa√ßon synchronis√©e.  
- **Exemple** : `sbatch` (soumettre), `squeue` (file), `sinfo` (n≈ìuds).  
- **Termes li√©s** : Backfill, Fairshare, Cgroups, Partition, QOS.

**Spine-Leaf**  
- **Courte** : Architecture r√©seau datacenter √† deux couches optimisant le trafic Est-Ouest.  
- **Longue** : Serveurs sur des switches Leaf (ToR) ; chaque Leaf est reli√© √† tous les Spine. Le chemin entre deux serveurs a toujours le m√™me nombre de sauts ‚Üí latence constante, vitale pour MPI.  
- **Exemple** : Remplace le mod√®le Core/Aggregation/Access en cascade.  
- **Termes li√©s** : Fat-Tree, Oversubscription.

**Stripe / Striping (Entrelacement)**  
- **Courte** : D√©couper un grand fichier en morceaux r√©partis sur plusieurs disques ou serveurs.  
- **Longue** : En Lustre, le striping permet √† des centaines de processus MPI de lire/√©crire un m√™me fichier en parall√®le. Fichier sur 4 OSTs ‚Üí bande passante √ó4.  
- **Exemple** : `lfs setstripe -c 8 /scratch/mon_dossier`  
- **Termes li√©s** : Lustre, OSS/OST, MPI-IO, Bande passante.

---

### W

**Walltime**  
- **Courte** : Temps maximum d‚Äôex√©cution autoris√© (et d√©clar√©) pour un job.  
- **Longue** : Si le job d√©passe cette limite (temps ¬´ mur ¬ª), le scheduler le tue (SIGKILL). Indispensable pour que le Backfill soit d√©terministe.  
- **Exemple** : `#SBATCH --time=24:00:00`  
- **Termes li√©s** : Slurm, Backfill, Scheduler.

**Warewulf**  
- **Courte** : Outil de provisionnement con√ßu pour le HPC.  
- **Longue** : Gestion de cluster ¬´ stateless ¬ª : les n≈ìuds d√©marrent en PXE et chargent l‚ÄôOS en RAM (tmpfs) depuis une image. Mise √† jour de 1000 n≈ìuds = red√©marrage, sans d√©rive de config.  
- **Exemple** : Warewulf v4 importe des images OCI (type Docker) pour l‚ÄôOS des n≈ìuds.  
- **Termes li√©s** : PXE, Stateless, Provisioning.

---

## Liens utiles dans le wiki

- **[Dictionnaire encyclop√©dique HPC](Dictionnaire-Encyclopedique-HPC)** : 17 entr√©es (Backfill, BeeGFS, Burst Buffers, Cgroups v2, DLM, Fairshare, Slurm Fairshare, GPUDirect RDMA, GPU Tensor Cores, Hugepages, MPI Collectives, NUMA, NVLink, OOM-Killer, RDMA, RoCE v2, Striping) ‚Äî format Doctorat/Architecte
- **[Sommaire du Manuel HPC](Manuel-HPC-Sommaire-Complet)** : 8 volumes (architecture, Slurm, Lustre, toolchains, performances, observabilit√©, fil rouge)
- **[Cours-HPC-Complet](Cours-HPC-Complet)** : cours complet HPC (concepts, architecture, MPI, stockage)
- **[Guide-SLURM-Complet](Guide-SLURM-Complet)** : Slurm en d√©tail (commandes, partitions, QoS)
- **[Home](Home)** : page d‚Äôaccueil du wiki

---

**Public** : Master Data Science, Doctorat, DevOps Senior  
**Derni√®re mise √† jour** : 2024
