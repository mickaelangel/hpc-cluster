# üéØ Guide SLURM Complet ‚Äî Scheduler HPC

> **R√©f√©rence professionnelle Slurm ‚Äî Niveau DevOps Senior & Utilisateurs avanc√©s**

---

## 1. Pr√©sentation de Slurm

**Slurm** (Simple Linux Utility for Resource Management) est un **ordonnanceur de jobs** (job scheduler) et un **gestionnaire de ressources** pour clusters Linux. Il est utilis√© dans une grande partie des clusters acad√©miques et des supercalculateurs.

### 1.1 R√¥les principaux

- **Ordonnancement** : d√©cider quand et sur quels n≈ìuds un job s‚Äôex√©cute
- **Allocation** : r√©server CPU, m√©moire, GPU, temps pour chaque job
- **Gestion de la file d‚Äôattente** : priorit√©s, partitions, QoS (Quality of Service)
- **Comptabilit√©** : enregistrement de l‚Äôusage (comptes, projets) pour facturation ou quotas

### 1.2 Architecture Slurm (r√©sum√©)

| Composant | R√¥le |
|-----------|------|
| **slurmctld** | D√©mon contr√¥leur (un primaire, √©ventuellement un backup) ‚Äî d√©cisions de scheduling |
| **slurmd** | D√©mon sur chaque n≈ìud de calcul ‚Äî ex√©cute les √©tapes (steps) des jobs |
| **slurmdbd** | D√©mon base de donn√©es (optionnel) ‚Äî historique, comptabilit√©, multi-cluster |
| **sbatch** | Soumettre un job batch (script) |
| **srun** | Lancer une t√¢che (souvent depuis un job allou√©, ou en interactif) |
| **squeue** | Afficher la file d‚Äôattente |
| **scancel** | Annuler un job |
| **sinfo** | √âtat des n≈ìuds et partitions |

---

## 2. Concepts cl√©s

### 2.1 Job, step, allocation

- **Job** : unit√© de travail soumise par l‚Äôutilisateur (un ou plusieurs steps).
- **Step** : sous-partie d‚Äôun job (ex. un `srun` dans un script sbatch). Les ressources peuvent √™tre partag√©es entre steps ou r√©serv√©es par step.
- **Allocation** : ensemble de n≈ìuds/CPU/m√©moire/GPU attribu√©s √† un job (ou √† un step).

### 2.2 Partitions (queues)

Une **partition** est une **file d‚Äôattente** associ√©e √† un sous-ensemble de n≈ìuds et √† des limites (temps max, nombre de jobs, etc.).

Exemples :

- `normal` : usage standard
- `high` : priorit√© plus √©lev√©e ou n≈ìuds d√©di√©s
- `gpu` : n≈ìuds avec GPU
- `short` : jobs courts (temps max faible)
- `long` : jobs longs

Commandes utiles :

```bash
sinfo -s                    # R√©sum√© des partitions
sinfo -p normal,gpu          # D√©tail des partitions normal et gpu
scontrol show partition normal
```

### 2.3 QoS (Quality of Service)

Les **QoS** d√©finissent des **contraintes et limites** (temps max, n≈ìuds max, priorit√©) appliqu√©es aux jobs. Un job est soumis dans une partition et peut √™tre associ√© √† une QoS.

```bash
sacctmgr show qos            # Lister les QoS (si slurmdbd configur√©)
scontrol show qos            # D√©tails des QoS
```

### 2.4 Priorit√© et fair-share

- La **priorit√©** d‚Äôun job d√©pend souvent de : QoS, partition, **fair-share** (usage pass√© de l‚Äôutilisateur/projet), √¢ge du job, taille de l‚Äôallocation.
- **Fair-share** : les utilisateurs (ou comptes) qui ont moins consomm√© r√©cemment obtiennent une priorit√© plus √©lev√©e pour √©quilibrer l‚Äôusage.

---

## 3. Soumission de jobs

### 3.1 Job batch avec sbatch

Fichier `mon_job.sh` :

```bash
#!/bin/bash
#SBATCH --job-name=mon_job
#SBATCH --partition=normal
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=4
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=2G
#SBATCH --time=01:00:00
#SBATCH --output=slurm-%j.out
#SBATCH --error=slurm-%j.err

# Optionnel : r√©pertoire de travail
# #SBATCH --chdir=/scratch/user/mon_projet

# Charger un module (si Environment Modules utilis√©)
# module load openmpi/4.1

# Lancer l‚Äôapplication (ex. MPI)
srun ./mon_executable_mpi
```

Soumission :

```bash
sbatch mon_job.sh
```

### 3.2 Principales options sbatch / srun

| Option | Description | Exemple |
|--------|-------------|--------|
| `--job-name` | Nom du job | `--job-name=simu_a` |
| `--partition` | Partition (queue) | `--partition=gpu` |
| `--nodes` | Nombre de n≈ìuds | `--nodes=4` |
| `--ntasks` | Nombre de t√¢ches (processus) | `--ntasks=64` |
| `--ntasks-per-node` | T√¢ches par n≈ìud | `--ntasks-per-node=16` |
| `--cpus-per-task` | CPU par t√¢che | `--cpus-per-task=2` |
| `--mem` | M√©moire totale pour le job | `--mem=32G` |
| `--mem-per-cpu` | M√©moire par CPU | `--mem-per-cpu=4G` |
| `--time` | Dur√©e max (walltime) | `--time=02:30:00` |
| `--output` | Fichier stdout | `--output=out_%j.txt` |
| `--error` | Fichier stderr | `--error=err_%j.txt` |
| `--gres` | Ressources g√©n√©riques (ex. GPU) | `--gres=gpu:2` |
| `--exclusive` | N≈ìuds d√©di√©s au job | `--exclusive` |
| `--mail-type` | Mail (BEGIN, END, FAIL, etc.) | `--mail-type=END` |
| `--mail-user` | Adresse email | `--mail-user=user@domain` |

Format du temps : `JOURS-HOURS:MINUTES:SECONDS` ou `MINUTES` ou `HOURS:MINUTES:SECONDS`.  
`%j` dans output/error = Job ID.

### 3.3 Job interactif avec srun

R√©server des ressources et ouvrir un shell sur un n≈ìud de calcul :

```bash
srun --partition=normal --nodes=1 --ntasks=1 --cpus-per-task=4 --mem=8G --time=01:00:00 --pty bash
```

Ou avec GPU :

```bash
srun --partition=gpu --gres=gpu:1 --cpus-per-task=4 --mem=16G --time=01:00:00 --pty bash
```

---

## 4. Consultation de l‚Äô√©tat des jobs et des n≈ìuds

### 4.1 File d‚Äôattente et jobs

```bash
squeue                    # Tous les jobs
squeue -u $USER           # Mes jobs
squeue -p normal          # Jobs dans la partition normal
squeue -j 12345           # D√©tail du job 12345
squeue --start            # Estimation du d√©marrage (si configur√©)
```

√âtats courants : `PENDING` (PD), `RUNNING` (R), `COMPLETING` (CG), `COMPLETED`, `FAILED`, `CANCELLED`, `TIMEOUT`.

### 4.2 D√©tails d‚Äôun job

```bash
scontrol show job 12345
scontrol show job 12345 -d   # Plus de d√©tails
```

### 4.3 Historique des jobs (comptabilit√©)

```bash
sacct                      # Jobs r√©cents (par d√©faut aujourd‚Äôhui)
sacct -j 12345             # Job 12345
sacct -u $USER --starttime 2024-01-01 --endtime 2024-01-31
sacct -l                   # Format long (nombreux champs)
sacct -o JobID,JobName,Partition,State,Elapsed,MaxRSS,ExitCode
```

### 4.4 N≈ìuds et partitions

```bash
sinfo                      # Vue compacte n≈ìuds/partitions
sinfo -N -l                # Un ligne par n≈ìud, d√©taill√©
sinfo -p normal,gpu        # Partitions normal et gpu
scontrol show nodes        # D√©tail de tous les n≈ìuds
scontrol show node compute01
```

√âtats de n≈ìud courants : `idle`, `allocated`, `mix`, `drain`, `down`, `reserved`.

---

## 5. Annulation et modification

### 5.1 Annuler un job

```bash
scancel 12345              # Un job
scancel -u $USER           # Tous mes jobs
scancel -p normal          # Tous les jobs dans la partition normal
scancel --state=PENDING -u $USER   # Tous mes jobs en attente
```

### 5.2 Modifier un job en attente

```bash
scontrol update jobid=12345 TimeLimit=02:00:00
scontrol update jobid=12345 Partition=high
```

Seuls les jobs **PENDING** peuvent √™tre modifi√©s (selon la configuration du cluster).

---

## 6. Bonnes pratiques (r√©sum√©)

- **Toujours** demander un **temps r√©aliste** (`--time`) pour √©viter de tuer le job ou de gaspiller des ressources.
- **Demander la m√©moire** n√©cessaire (`--mem` ou `--mem-per-cpu`) pour √©viter les OOM.
- Utiliser **partitions et QoS** adapt√©s (court/long, GPU, debug).
- Pr√©f√©rer **sbatch** pour les calculs longs et **srun** pour le debug court ou interactif.
- Dans les scripts, utiliser **srun** (et non mpirun) pour lancer des applications MPI dans un job Slurm.
- V√©rifier **squeue** et **sacct** pour comprendre refus ou √©checs (limites, n≈ìuds down, etc.).

---

## 7. Int√©gration avec l‚Äôenvironnement

### 7.1 Variables d‚Äôenvironnement fournies par Slurm

En cours d‚Äôex√©cution d‚Äôun job, Slurm d√©finit notamment :

- `SLURM_JOB_ID`, `SLURM_JOB_NAME`
- `SLURM_NODELIST`, `SLURM_NNODES`, `SLURM_NTASKS`, `SLURM_CPUS_PER_TASK`
- `SLURM_SUBMIT_DIR`, `SLURM_JOB_PARTITION`
- `SLURM_GPUS_ON_NODE`, `CUDA_VISIBLE_DEVICES` (si GPU configur√©s)

Utile pour des logs ou des chemins d√©pendant du job.

### 7.2 Modules (Environment Modules)

Beaucoup de sites utilisent **Environment Modules** pour charger compilateurs, MPI, librairies :

```bash
module avail
module load openmpi/4.1
module load gcc/11
module list
```

√Ä appeler dans le script **sbatch** (ou en interactif) avant `srun` ou l‚Äôex√©cutable.

---

## 8. R√©f√©rence rapide des commandes

| Commande | Usage |
|----------|--------|
| `sbatch script.sh` | Soumettre un job batch |
| `srun [options] cmd` | Lancer une t√¢che (souvent dans une allocation) |
| `squeue` | File d‚Äôattente |
| `scontrol show job JOBID` | D√©tail d‚Äôun job |
| `scontrol show node NODE` | D√©tail d‚Äôun n≈ìud |
| `scancel JOBID` | Annuler un job |
| `sinfo` | √âtat n≈ìuds/partitions |
| `sacct` | Historique des jobs |

Pour plus de commandes syst√®me (Prometheus, Grafana, Slurm, Docker, etc.) : [Commandes-Utiles](Commandes-Utiles.md).

---

## 9. Aller plus loin

- **[Cours-HPC-Complet](Cours-HPC-Complet.md)** : architecture HPC, MPI, stockage, GPU
- **[Glossaire-et-Acronymes](Glossaire-et-Acronymes.md)** : acronymes et d√©finitions (SLURM, PBS, MPI, etc.)
- **[Depannage](Depannage.md)** : probl√®mes courants et solutions
- **[FAQ](FAQ.md)** : questions fr√©quentes sur le cluster

---

**Niveau** : DevOps Senior / Utilisateurs avanc√©s  
**Derni√®re mise √† jour** : 2024
