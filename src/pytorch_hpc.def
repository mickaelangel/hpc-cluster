# =============================================================================
# pytorch_hpc.def — Définition Apptainer pour PyTorch HPC (GPU, Slurm)
# Build : apptainer build pytorch_hpc.sif pytorch_hpc.def
# Run   : apptainer run --nv pytorch_hpc.sif   (ou exec pour une commande)
# Job   : srun --gres=gpu:1 apptainer run --nv pytorch_hpc.sif python train.py
# Référence : Manuel HPC Vol. 5, Lab 7 ; Dictionnaire GPUDirect RDMA, GPU Tensor Cores
# =============================================================================

Bootstrap: docker
From: nvcr.io/nvidia/pytorch:24.01-py3
# Alternatives : nvcr.io/nvidia/pytorch:23.10-py3  ou  pytorch/pytorch:2.1.0-cuda12.1-cudnn8-runtime

# -----------------------------------------------------------------------------
# Métadonnées (optionnel mais utile pour traçabilité)
# -----------------------------------------------------------------------------
%labels
    Author HPC Team
    Version 1.0
    Description PyTorch + CUDA pour jobs Slurm GPU (A100/V100)

# -----------------------------------------------------------------------------
# Variables d'environnement dans le conteneur
# Optimisations HPC : threads, libs CUDA, comportement Python
# -----------------------------------------------------------------------------
%environment
    export LANG=C.UTF-8
    # Chemins CUDA (souvent déjà dans l'image ; au cas où)
    export LD_LIBRARY_PATH=/usr/local/cuda/lib64:${LD_LIBRARY_PATH}
    # Éviter de saturer les cœurs avec des threads MKL/OpenMP non contrôlés
    export OMP_NUM_THREADS=1
    # Optionnel : pour du multi-GPU / distribué (NCCL)
    # export NCCL_DEBUG=WARN
    # export NCCL_IB_DISABLE=0

# -----------------------------------------------------------------------------
# Script par défaut si on lance le conteneur sans commande
# -----------------------------------------------------------------------------
%runscript
    exec python3 "$@"

# -----------------------------------------------------------------------------
# Installation post-bootstrap (optionnel)
# Adapter selon les besoins : pip install horovod, transformers, etc.
# Garder l'image légère pour des builds rapides et des pulls reproductibles
# -----------------------------------------------------------------------------
%post
    apt-get update && apt-get install -y --no-install-recommends \
        git \
        && rm -rf /var/lib/apt/lists/*
    # Exemple : pip install --no-cache-dir horovod[pytorch]
    pip install --no-cache-dir --upgrade pip
    # pip install --no-cache-dir transformers datasets  # décommenter si besoin

# -----------------------------------------------------------------------------
# Tests rapides après build :
#   apptainer run --nv pytorch_hpc.sif python3 -c "import torch; print(torch.cuda.is_available()); print(torch.cuda.get_device_name(0))"
# Sur un nœud Slurm avec GPU :
#   srun --partition=gpu --gres=gpu:1 --pty apptainer run --nv /path/to/pytorch_hpc.sif bash
# =============================================================================
