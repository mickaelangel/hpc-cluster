# =============================================================================
# gres.conf — Generic Resources (GRES) : GPUs A100 avec binding NUMA/PCIe
# Référence de production — À adapter selon la topologie réelle (numactl, nvidia-smi topo -m)
# Documentation : Slurm gres.conf, resource_binding ; Dictionnaire : NVLink, NUMA
# =============================================================================
#
# Objectif : associer chaque GPU à un ensemble de cœurs CPU (socket/NUMA) proche
# du bus PCIe de ce GPU, pour réduire la latence et le cross-NUMA.
# Vérifier la topologie avec : nvidia-smi topo -m  et  numactl --hardware
#
# Exemple typique (nœud 2 sockets × 32 cœurs, 4× A100) :
#   GPU0 (PCIe socket 0) -> Cores 0-15
#   GPU1 (PCIe socket 0) -> Cores 16-31
#   GPU2 (PCIe socket 1) -> Cores 32-47
#   GPU3 (PCIe socket 1) -> Cores 48-63
# =============================================================================

# Définition du type de GRES (optionnel si tout est défini par nœud)
Name=gpu Type=a100 File=/dev/nvidia[0-3]

# -----------------------------------------------------------------------------
# Nœud gpu01 — 4× A100, binding explicite par device
# Cores= : plage de cœurs à laquelle ce GPU est « associé » (pour --gpu-bind=closest)
# Adapter les plages à votre topologie (lscpu, nvidia-smi topo -m)
# -----------------------------------------------------------------------------
NodeName=gpu01 Name=gpu Type=a100 File=/dev/nvidia0 Cores=0-15
NodeName=gpu01 Name=gpu Type=a100 File=/dev/nvidia1 Cores=16-31
NodeName=gpu01 Name=gpu Type=a100 File=/dev/nvidia2 Cores=32-47
NodeName=gpu01 Name=gpu Type=a100 File=/dev/nvidia3 Cores=48-63

# -----------------------------------------------------------------------------
# Nœud gpu02 — même schéma (4× A100)
# -----------------------------------------------------------------------------
NodeName=gpu02 Name=gpu Type=a100 File=/dev/nvidia0 Cores=0-15
NodeName=gpu02 Name=gpu Type=a100 File=/dev/nvidia1 Cores=16-31
NodeName=gpu02 Name=gpu Type=a100 File=/dev/nvidia2 Cores=32-47
NodeName=gpu02 Name=gpu Type=a100 File=/dev/nvidia3 Cores=48-63

# -----------------------------------------------------------------------------
# Utilisation côté job :
#   --gres=gpu:a100:2 --cpus-per-gpu=16  (2 GPU, 32 CPU liés)
#   --gpu-bind=closest  (lier chaque tâche au GPU/CPU les plus proches)
# Voir : srun --help ; Dictionnaire NVLink, NUMA
# -----------------------------------------------------------------------------
